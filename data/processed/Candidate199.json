{
  "filename": "Candidate199.docx",
  "filepath": "D:\\Iqrar Workspace\\AI-Resume-Ranker\\data\\raw\\Candidate199.docx",
  "text": "Candidate199\nSr. Java/ Hadoop Developer\n@:candidate19909@gmail.com                                                                                   \u00a9: (646) 801-5340\nResponsibility:\n9+ years of experience in SDLC with key emphasis on the trending Big Data and Java Technologies - Spark, Scala, Spark Mlib, Hadoop, Tableau, Cassandra, Java, J2EE.\nArchitect, design & develop Big Data Solutions practice including set up Big Data roadmap, build supporting infrastructure and team to provide Big Data.\nArchitecting, Solutioning and Modeling DI (Data Integrity) Platforms using sqoop, flume, kafka, Spark Streaming, Spark Mllib, Cassandra.\nStrong experience in migrating data warehouses and databases into Hadoop/NoSQL platforms.\nStrong expertise on Amazon AWS EC2, Dynamo DB, S3, Kinesis and other services\u00a0\nExpertise in data analysis, design and modeling using tools like ErWin.\nExpertise in Big Data architecture like hadoop (Azure, Hortonworks, Cloudera) distributed system, MongoDB, NoSQL.\nExpertise in Service Oriented Architectures (SOA- Web Services) using Apache Axis, WebLogic, JBoss and\u00a0EJB\u00a0Web service framework.\u00a0\nUsed Mule ESB in designing the application as a middleware between the third-party system and the customer side system.\nHands on experience on Hadoop /Big Data related technology experience in Storage, Querying, Processing and analysis of data. \nExpertise in archtiecting Big data solutions using Data ingestion, Data Storage\nStrong Experience in Front End Technologies like JSP, HTML5, JQuery, JavaScript, CSS3. \nWorked on windows server AD configuration and Kerberos protocol.\u00a0\nExperienced with Perl, Shell scripting and test automation tools like Selenium RC, Web Driver and Selenium Grid.\u00a0 \nDeveloped Python Mapper and Reducer scripts and implemented them using Hadoop streaming. \nExperienced in customizing Selenium API to suit in testing environment.\u00a0 \nIntegration of Mule ESB system while utilizing MQ Series, Http, File system and SFTP transports.\nSolid Knowledge of My SQL and Oracle databases and writing SQL Queries.\u00a0\nProficient in developing the application using JSF, Hibernate, Core\u00a0Java, JDBC and Groovy and Grails presentation layer components using JSPs, Java script, XML and HTML Cassandra , Cucumber, OLE and Continuous deployment,  API , Angular JS along with Web service , REST , GemFire , Rabbit MQ , Spring Boot..\u00a0\nExperience in Back End Development including Web services, Data service layers\u00a0with service desk experience.\nDesigned and coded Hibernate, struts for mapping, configurations and HQL for enhancement and new module development of Transport Optimization, Planning and Scheduling Web app.\u00a0\nUsed Groovy and Grails with spring, Java, J2EE for user interface.\nInitiated the Automation framework using\u00a0Selenium Web Driver to run test cases in multiple browsers and platforms.\nHighly motivated software engineer and experience in developing in web applications using Java script, Backbone.js and Coffee script technologies.\u00a0\u00a0 \nUtilized integration Patterns, integration tools, EAI, Transformations, XML Schemas, and XSLT. \nUsed Quartz connector to schedule the batch jobs. \nArchitected Integrations using Mule Soft ESB environments for both on premise and Cloud hub environments. \n\u2022 Experience in developing interfaces between Sales force and Oracle ERP using Informatica Cloud/Mule ESB technologies. \n\u2022 Implemented flows for sales force outbound / inbound calls and business process. \n\u2022 Experience in Mulesoft Any point API platform on designing and implementing Mule APIs.\nGood knowledge on Soap UI tool to unit testing SOA based applications.\u00a0\nAbility to understand and use design patterns in application development. \nVery good knowledge in different development methodologies like SDLC and Agile.\u00a0\nExperienced in developing applications using HIBERNATE (Object/Relational mapping framework) and involves in working on service desk client.\nExperienced in developing Web Services using JAX-RPC, JAXP, SOAP and WSDL. Also knowledgeable in using WSIF (Web Services Invocation Framework) API. \nExperience in writing database objects like Stored Procedures, Triggers, SQL, PL/SQL packages and Cursors for Oracle, SQL Server, DB2 and Sybase. \nJava 8, J2EE, spring (MVC, Data-JPA, Security),Hibernate, Jenkins or Bamboo, HTML 5, JSP, JavaScript, JQuery, Ajax, Angular JS.\nTechnical Skills\n\nBig Data: Hive, Hadoop, oozie, sqoop, Storm, Kafka, Elastic Search, HDFS, Zoo Keeper, Map Reduce, hive, pig, spark, flume.\nJ2EE Technologies: Servlets, JSP, JDBC, JNDI, OSGI, EJB, RMI, ASP.\nProgramming Languages:  Java 8, C, C++, Pig Latin, HQL, R, Python, XPath, Spark.\nFrameworks: Jakarta Struts, Spring, Spring MVC, JSF (Java Server Faces), Hibernate, Tiles, I Batis, Validator, Cucumber, OLE and Continuous deployment, micro services, Groovy.\nWeb Technologies: HTML, DHTML, Cassandra , API , Angular JS along with Web service, REST, Gem Fire, Rabbit MQ , Java script with J query, Python, Ext JS, AJAX, CSS,CMS, Yahoo UI, ice faces API\u00a0, Angular, Node.js, Backbone.js.\nXML Technologies: XML 1.0, XSLT, XSL, HTML5, DHTML, J query,, XSL / XSLT /XSL-FO, JNDI, LDAP, SOAP, AXIS 2\u00a0\nApplication/Web Servers: IBM Web Sphere 5.X/6.0/7.0/8.0, IBM HTTP server 8.x, Web Logic 7.x/8.x/9.0, Web Logic Portal 5.x, J Boss 4.0, j BPM, Apache Tomcat, OC4J, Docker.\nNO SQL Data Base: Cassandra, mongo DB\nDatabases: Oracle 12c /10g/11g, SQL Server, My SQL, DB2. \nMessaging Systems: JMS, IBM MQ-Series\u00a0\nIDE Tools:\u00a0IBM Web Sphere Studio Application Developer (WSAD) RSA, RAD, Eclipse /RCP, J developer, Net Beans .\nProfessional Experience\nCigna, Hartford, CT\t\t\t                                                                              Aug\u201915 \u2013 Till Date\nSr. Java/Hadoop Developer\n      Roles & Responsibilities:\nImplementation of Big Data ecosystem (Hive, Impala, Sqoop, Flume, Spark, Lambda) with Cloud Architecture\u00a0\nUsed Talend for Big data Integration using Spark and Hadoop\nUsed Microsoft Windows server and authenticated client server relationship via Kerbros protocol.\nExperience on BI reporting with At Scale OLAP for Big Data.\nImplemented solutions for ingesting data from various sources and processing the Data-at-Rest utilizing Big Data technologies such as\u00a0Hadoop, Map Reduce Frameworks, HBase, Hive\nLoaded and transformed large sets of structured, semi structured and unstructured data using Hadoop/Big Data concepts.\u00a0\nI have Working experience in Middleware Integration product Mulesoft \nDesigned and Developed Real time Stream processing Application using Spark, Kafka, Scala and Hive to perform Streaming ETL and apply Machine Learning.\nIdentify query duplication, complexity and dependency to minimize migration efforts\u00a0\nTechnology stack: Oracle, Hortonworks HDP cluster, Attunity Visibility, Cloudera Navigator Optimizer, AWS Cloud and Dynamo DB.\nExperience in AWS, implementing solutions using services like (EC2, S3, RDS, Redshift, VPC)\nWorked on Talend Magic Quadrant for performing fast integration tasks.\nWorked as a Hadoop consultant on (Map Reduce/Pig/HIVE/Sqoop).\nWorked with Spark and Python.\nWorked using Apache Hadoop ecosystem components like HDFS, Hive, Sqoop, Pig, and Map Reduce.\nLead architecture and design of data processing, warehousing and analytics initiatives.\nWorked with AWS to implement the client-side encryption as Dynamo DB does not support at rest encryption at this time.\u00a0\nExploring with the Spark for improving the performance and optimization of the existing algorithms in Hadoop using Spark Context, Spark-SQL, Data Frame, Pair RDD's, Spark YARN. \nUsed Data Frame API in Scala for converting the distributed collection of data organized into named columns. \nPerformed data profiling and transformation on the raw data using Pig, Python, and Java.\nExperienced with batch processing of data sources using Apache Spark. \nDeveloping predictive analytic using Apache Spark Scala APIs. \nInvolved in working of big data analysis using Pig and User defined functions (UDF).\nCreated Hive External tables and loaded the data into tables and query data using HQL.\nUsed Sqoop to efficiently transfer data between databases and HDFS and used Flume to stream the log data from servers.\u00a0\nInvolved in developing and configuration of enterprise components using Mulesoft ESB. \nImplement enterprise grade platform (mark logic) for ETL from mainframe to NO SQL (cassandra).\nExperience on BI reporting with At Scale OLAP for Big Data.\u00a0\nResponsible for importing log files from various sources into HDFS using Flume\u00a0\nWorked on tools Flume, Storm and Spark.\u00a0\nExpert in performing business analytical scripts using Hive SQL.\u00a0\nBest practices for designing integration modules using ESB& Data Integrator modules    \nImplemented continuous integration & deployment (CICD) through Jenkins for Hadoop jobs.\u00a0\nWorked in writing Hadoop Jobs for analyzing data using Hive, Pig accessing Text format files, sequence files, Parquet files.\nExperience in different Hadoop distributions like Cloudera (CDH3 & CDH4) and Horton Works Distributions (HDP) and MapR.\nExperience in integrating oozie logs to kibana dashboard.\u00a0\nExtracted the data from MySQL, AWS RedShift into HDFS using Sqoop.\u00a0\nDeveloped Spark code using Scala and Spark-SQL for faster testing and data processing. \nImported millions of structured data from relational databases using Sqoop import to process using Spark and stored the data into HDFS in CSV format. \nDeveloped Spark streaming application to pull data from cloud to Hive table.\nUsed Spark SQL to process the huge amount of structured data.\nAssigned name to each of the columns using case class option in Scala. \nImplemented Spark GraphX application to analyze guest behavior for data science segments.\nEnhancements to traditional data warehouse based on STAR schema, update data models, perform Data Analytics and Reporting using Tableau.\nEnvironment: Big Data, SparkSpark, YARN, HIVE, Pig, Scala, Python, Hadoop, AWS, Dynamo DB, Kibana, Cloudera, EMR, JDBC, Redshift, NOSQL, Sqoop, MYSQL.\nBNFS,Fort Worth, TX\t\t\t\t\t\t\t                                 Apr\u201913 \u2013 Jul\u201915\nSr. Java/ Hadoop Developer\nRoles & Responsibilities:\nInvolved in Big\u00a0Data\u00a0Project Implementation and Support.\u00a0\nInvolved in the coding and integration of several business critical modules of CARE application using spring, Hibernate and REST web services on Web Sphere application server.\nImplemented Installation and configuration of multi-node cluster on Cloud using AWS on EC2.\nDesigned and developed Enterprise Eligibility business objects and domain objects with Object Relational Mapping framework such as Hibernate.\nUsed\u00a0Hive\u00a0to analyze data ingested into\u00a0HBase\u00a0by using\u00a0Hive-HBase\u00a0integration and compute various metrics for reporting on the dashboard\nDeveloped the Web Based Rich Internet Application (RIA) using JAVA/J2EE (spring framework).\nUsed the light weight container of the Spring Frame work to provide architectural flexibility for inversion of controller (IOC).\nUtilized Oozie workflow to run Pig and Hive Jobs Extracted files from Mongo DB through Sqoop and placed in HDFS and processed.\nUsed Flume to collect, aggregate, and store the web log data from different sources like web servers, mobile and network devices and pushed to HDFS.\nInvolved in end to end implementation of\u00a0Big\u00a0data\u00a0design.\nDeveloped and Implemented new UI's using Angular JS and HTML.\nDeveloped Spring Configuration for dependency injection by using Spring IOC, Spring Controllers.\nAll the data was loaded from our relational DBs to HIVE using Sqoop. We were getting four flat files from different vendors. These were all in different formats e.g. text, EDI and XML formats\nObjective of this project is to build a data lake as a cloud based solution in AWS using Apache Spark and provide visualization of the ETL orchestration using CDAP tool.\nProof-of-concept to determine feasibility and product evaluation of Big Data products \nWriting Hive join query to fetch info from multiple tables, writing multiple Map Reduce jobs to collect output from Hive\nUsed Hive to analyze the partitioned and bucketed data and compute various metrics for reporting on the dashboard.\nAWS Cloud and On-Premise environments with Infrastructure Provisioning / Configuration.\nWorked on writing Perl scripts covering data feed handling, implementingmark logic, communicating with web-services through SOAP Lite module and WSDL.\u00a0\nInvolved in developing Map-reduce framework, writing queries scheduling map-reduce\nDeveloped the code for Importing and exporting data into HDFS and Hive using Sqoop\nInstalled and configured Hadoop and responsible for maintaining cluster and managing and reviewing Hadoop log files.\nDeveloped Shell, Perl and Python scripts to automate and provide Control flow to Pig scripts.\nDesign of Redshift Data model, Redshift Performance improvements/analysis\nContinuous monitoring and managing the Hadoop cluster through Cloudera Manager.\nWorked on configuring and managing disaster recovery and backup on Cassandra Data.\nPerformed File system management and monitoring on Hadoop log files.\nImplemented partitioning, dynamic partitions and buckets in HIVE.\nDeveloped customized classes for serialization and Deserialization in Hadoop\nAnalyzed large amounts of data sets to determine optimal way to aggregate and report on it.\nImplemented a proof of concept deploying this product in Amazon Web Services AWS.\u00a0\nInvolved in migration of data from existing RDBMS (oracle and SQL server) to Hadoop using Sqoop for processing data.\nEnvironment: Pig, Sqoop, Kafka, Apache Cassandra, Oozie, Impala, Cloudera, AWS, AWS EMR, Redshift, Flume, Apache Hadoop, HDFS, Hive, Map Reduce, Cassandra, Zookeeper, MySQL, Eclipse, Dynamo DB, PL/SQL and Python.\nWW Norton, NYC, NY                                                                                                                Nov\u201911 \u2013 Mar\u201913\nJAVA/J2EE DEVELOPER\nRoles & Responsibilities:\nUsed Web Logic to build and deploy the application.\u00a0\nCreated stubs to consume Web services.\nAutomated tests were coded in Java Script with Frog logic\u2019s Squish or Smart Bear\u2019s Test Complete for client applications and coded in Java with Selenium for web application testing.\nDeveloped automation testing process using Selenium\u00a0and QTP which involves study of client testing requirements, analyzing the feasible testing strategies and development of automated test scripts which also includes testing and finally deployment of the test scripts.\u00a0\nUsed spring framework to achieve loose coupling between the layers thus moving towards Service Oriented Architecture (SOA) exposed through\u00a0RESTful.\u00a0\u00a0\nInvolved in performing Unit and Integration testing (Junit)\u00a0\nInvolved in building EJB Session/Entity beans to maintain Transaction Management across the application.\nBuilt Web pages that is more user-interactive using Java script and Angular JS.\nGroovy allows using the primitive\u2019s types as a short form for the variable declaration and the compiler translates this into the object.\nExtensively used Spring\u00a0JDBC\u00a0in data access layer to access and update information in the database.\nDeveloped Web Services to create reports module and send it to different agencies and premium calculation for manual classes using\u00a0SOAP\u00a0and Restful web services and rich faces components.\nInvolved in writing Spring\u00a0MVC\u00a0controllers and writing custom validations.\u00a0\nWorking on Struts Framework for developing the front-end application and extensively. Spring as middle tier for entire application.\u00a0\nUsed JAX-WS (SOAP) and JAX-RS (REST) to produce web services and involved in writing programs to consume the web services.\nJava 8, J2EE, spring (MVC, Data-JPA, Security),Hibernate, Jenkins or Bamboo, HTML 5, JSP, JavaScript, JQuery, Ajax, Angular JS\nInvolved in working with Struts Tiles for the common look and feel for a web application.\u00a0\nWorking on Web Services using\u00a0Java\u00a0API for XML Services (JAX-WS) and supporting, building, deploying Web APIs Services.\u00a0\nWorking as a part of team from business transfer, development, testing, code review, build implementation and support.\u00a0\nWrote PL/SQL statements according to the need using Oracle 10g database.\u00a0\nWorking on an internal web-based client server application built with Struts 2 Framework using Oracle backend Database, working on establishing the relation for the different beans using the Hibernate 3.1.\u00a0\nInvolved in writing various components using Spring AOP and IOC framework.\nInvolved in writing JSP and JSF components. Used JSTL Tag library (Core, Logic, Nested, Beans and Html tag lib's) to create standard dynamic web pages.\nDeveloped connection to the backend using\u00a0JDBC\u00a0after building the Entity Beans as Bean Managed Persistence Entity Beans.\nDesigned and Developed the UI Framework using Spring\u00a0MVC\u00a0and AngularJS.\nCreation of REST Web Services for the management of data using Apache CXF and Docker.\nImplementation of EJB as entry point for web services. Effectively prepared for and organized technical inspections to review code and design models with peers and software architects.\u00a0\nIdentified the defects through Selenium and ensured that business processes deliver the expected results and remain reliable throughout the production release.\nSpring 3.x is used as framework to write the application code and\u00a0RESTful\u00a0web services for external clients. \nDesigned and developed backend application servers using\u00a0Python.\nManaged application deployment using Python.\nUpgraded Python 2.3 to Python 2.5, this required recompiling mode Python to use Python 2.5.\nEnhanced user experience by designing new web features using MVC Framework like Backbone.js, and node.js.\nUsed\u00a0JDBC\u00a0connectivity for connecting to the Oracle 8.0 database.\nDeveloped major websites and services by including\u00a0Mongo\u00a0DB\u00a0as backend software.\nGood experience in creating and consuming Restful and SOAP Web Services.\u00a0\u00a0\nDeveloping ability to move and consolidate critical information for the businesses and financial account data Using EJB 2.1 and Hibernate for performing the Database Transactions.\u00a0\nEnvironment:\u00a0Java\u00a0and\u00a0, Struts Framework ,J query, Oracle , HTML, Mark logic, micro services, Python, Groovy,  PL/SQL, JDBC, Mark logic, Talend, Hibernate, Ant, WSDL, EJB .\nInfinite Computer Solutions                                                                              Jun\u201908 \u2013 Oct\u201911\t                                                                                         \nJAVA DEVELOPER\nRoles & Responsibilities:\nInvolved in various phases of Software Development Life Cycle (SDLC) of the application like Requirement gathering, Design, Analysis and Code development.\nDeveloped\u00a0hibernate\u00a0mapping using db model.\u00a0\nInvolved in designing and developing Customized tags using JSP tag lib\u00a0\nImplemented Model View Control (MVC) architecture using Struts Framework and Spring framework\u00a0\nDeveloped browser-based\u00a0Java\u00a0Server Faces front-end to an AS/400 system\u00a0\nUsed Ajax to provide dynamic features where applicable.\nImplemented\u00a0RESTful\u00a0web services to communicate with components of other Sourcing systems within the firm and to provide data to the reporting team.\u00a0\nUsed MVC pattern for GUI development in JSF and worked closely with JSF lifecycle, Servlets and JSPs are used for real-time reporting which is too complex to be handled by the Business Objects\u00a0\nUsed Jira for bug tracking and project management.\nPrepared user documentation with screenshots for UAT (User Acceptance testing).\u00a0\nImplemented Struts Validation Framework for Server side validation.\u00a0\nDeveloped JSP's with Custom Tag Libraries for control of the business processes in the middle-tier and was involved in their integration.\u00a0\nDeveloped Web services (SOAP) through WSDL in Apache Axis to interact with other components.\u00a0\nImplemented EJBs Session beans for business logic.\u00a0\nUsed parsers like SAX and DOM for parsing xml documents and used XML transformations using XSLT.\u00a0\nWrote stored procedures, triggers, and cursors using Oracle PL/SQL.\u00a0\nUsed Rational Clear Case as Version control.\u00a0\nImplemented\u00a0Java/J2EE Design patterns like Business Delegate and Data Transfer Object (DTO), Data Access Object and Service Locator.\u00a0\nInteract with clients to understand their needs and propose design to the team to implement the requirement.\nBuilt an online system using XML, Java script, AJAX, Strut 2.0, JDBC\u00a0\nInvolved in technical Documentation for the module\u00a0\nDesigned and created SQL Server Database, Stored Procedures\u00a0\n\u00a0\nEnvironment: Java, JSP, JDBC, Cassandra , API , Python, J query, Angular JS along with Web service , REST , Spring Core, Struts, Hibernate, Design Patterns, XML, Oracle, Apache Axis, ANT, Junit, UML, Web services, SOAP, XSLT, Jira.",
  "size_bytes": 47256,
  "num_pages": null,
  "extracted_at": "2025-09-10T18:15:39.534539",
  "clean_text": "sr hadoop developer responsibility year experience sdlc key emphasis trending big data java technologies spark scala spark mlib hadoop tableau cassandra java architect design develop big data solutions practice include set big data roadmap build support infrastructure team provide big data architecte solutioning modeling di data integrity platform sqoop flume kafka spark streaming spark mllib cassandra strong experience migrate data warehouse database hadoop nosql platform strong expertise amazon aws dynamo db kinesis service expertise datum analysis design modeling tool like erwin expertise big data architecture like hadoop azure hortonworks cloudera distribute system mongodb nosql expertise service orient architectures web services apache axis weblogic jboss ejb web service framework mule esb design application middleware party system customer system hand experience hadoop data relate technology experience storage querying processing analysis datum expertise archtiecte big data solution data ingestion data storage strong experience end technologies like jsp jquery javascript work window server ad configuration kerberos protocol experience perl shell scripting test automation tool like selenium rc web driver selenium grid develop python mapper reducer script implement hadoop streaming experience customize selenium api suit testing environment integration mule esb system utilize mq series http file system sftp transport solid knowledge sql oracle database write sql queries proficient develop application jsf hibernate core java jdbc groovy grail presentation layer component jsp java script xml html cassandra cucumber ole continuous deployment api angular js web service rest gemfire rabbit mq spring boot experience end development include web service data service layer service desk experience design code hibernate strut mapping configuration hql enhancement new module development transport optimization planning scheduling web app groovy grails spring java user interface initiate automation framework selenium web driver run test case multiple browser platform highly motivated software engineer experience develop web application java script coffee script technology utilize integration patterns integration tool eai transformations xml schemas xslt quartz connector schedule batch job architected integrations mule soft esb environment premise cloud hub environment experience develop interface sale force oracle erp informatica cloud mule esb technology implement flow sale force outbound inbound call business process experience mulesoft point api platform design implement mule api good knowledge soap ui tool unit testing soa base application ability understand use design pattern application development good knowledge different development methodology like sdlc agile experience develop application hibernate object relational mapping framework involve work service desk client experience develop web services jax rpc jaxp soap wsdl knowledgeable wsif web services invocation framework api experience write database object like stored procedures triggers sql pl sql package cursors oracle sql server sybase java spring mvc data jpa jenkins bamboo html jsp javascript jquery ajax angular js technical skills big data hive hadoop oozie sqoop storm kafka elastic search hdfs zoo keeper map reduce hive pig spark flume technologies servlets jsp jdbc jndi osgi ejb rmi asp programming languages java c pig latin hql r python xpath spark framework jakarta struts spring spring mvc jsf java server faces hibernate tiles batis validator cucumber ole continuous deployment micro service groovy web technologies html dhtml cassandra api angular js web service rest gem fire rabbit mq java script j query python ext js ajax css cms yahoo ui ice face api angular xml technologies xml xslt xsl dhtml j query xsl xslt fo jndi ldap soap axis application web servers ibm web sphere ibm http server web logic web logic portal j boss j bpm apache tomcat docker sql data base cassandra mongo db database oracle g sql server sql messaging systems jms ibm mq series ide tools ibm web sphere studio application developer wsad rsa rad eclipse j developer net beans professional experience cigna hartford ct till date sr java hadoop developer roles responsibility implementation big data ecosystem hive impala sqoop flume spark lambda cloud architecture talend big datum integration spark hadoop microsoft windows server authenticated client server relationship kerbros protocol experience bi report scale olap big data implement solution ingest datum source process data rest utilize big data technology hadoop map reduce frameworks hbase hive loaded transform large set structured semi structured unstructured datum hadoop big data concept work experience middleware integration product mulesoft designed develop real time stream processing application spark kafka scala hive perform streaming etl apply machine learning identify query duplication complexity dependency minimize migration effort technology stack oracle hortonworks hdp cluster attunity visibility cloudera navigator optimizer aws cloud dynamo db experience aws implement solution service like rds redshift vpc work talend magic quadrant perform fast integration task work hadoop consultant map reduce pig hive sqoop work spark python work apache hadoop ecosystem component like hdfs hive sqoop pig map reduce lead architecture design datum processing warehousing analytic initiative work aws implement client encryption dynamo db support rest encryption time explore spark improve performance optimization exist algorithm hadoop spark context spark sql data frame pair rdd spark yarn data frame api scala convert distribute collection datum organize name column perform datum profiling transformation raw datum pig python java experience batch processing datum source apache spark develop predictive analytic apache spark scala api involve work big datum analysis pig user define function udf create hive external table load datum table query datum hql sqoop efficiently transfer datum database hdfs flume stream log datum server involve develop configuration enterprise component mulesoft esb implement enterprise grade platform mark logic etl mainframe sql cassandra experience bi report scale olap big data responsible import log file source hdfs flume worked tool flume storm spark expert perform business analytical script hive sql good practice design integration module esb data integrator module implement continuous integration deployment cicd jenkins hadoop job work write hadoop jobs analyze datum hive pig access text format file sequence file parquet file experience different hadoop distribution like cloudera horton works distributions hdp experience integrate oozie log kibana dashboard extract datum mysql aws redshift hdfs sqoop develop spark code scala spark sql fast testing datum processing imported million structured datum relational database sqoop import process spark store datum hdfs csv format develop spark streaming application pull datum cloud hive table spark sql process huge structured datum assign column case class option scala implement spark graphx application analyze guest behavior datum science segment enhancement traditional datum warehouse base star schema update data model perform data analytics reporting tableau environment big data sparkspark yarn hive pig scala python hadoop aws dynamo db kibana cloudera emr jdbc redshift nosql sqoop mysql bnfs fort worth tx sr hadoop developer roles responsibility involve big data project implementation support involve coding integration business critical module care application spring hibernate rest web service web sphere application server implement installation configuration multi node cluster cloud aws design develop enterprise eligibility business object domain object object relational mapping framework hibernate hive analyze datum ingest hbase hive hbase integration compute metric report dashboard develop web base rich internet application ria java spring framework light weight container spring frame work provide architectural flexibility inversion controller ioc utilize oozie workflow run pig hive jobs extract file mongo db sqoop place hdfs process flume collect aggregate store web log datum different source like web server mobile network device push hdfs involve end end implementation big data design develop implement new ui angular js html develop spring configuration dependency injection spring ioc spring controllers datum load relational db hive sqoop get flat file different vendor different format text edi xml format objective project build data lake cloud base solution aw apache spark provide visualization etl orchestration cdap tool proof concept determine feasibility product evaluation big data product write hive join query fetch info multiple table write multiple map reduce job collect output hive hive analyze partitioned bucketed datum compute metric report dashboard aws cloud premise environment infrastructure provisioning configuration work write perl script cover datum feed handling implementingmark logic communicate web service soap lite module wsdl involve develop map reduce framework write query schedule map reduce developed code importing export datum hdfs hive sqoop instal configure hadoop responsible maintain cluster manage review hadoop log file develop shell perl python script automate provide control flow pig script design redshift data model redshift performance improvement analysis continuous monitoring manage hadoop cluster cloudera manager work configure manage disaster recovery backup cassandra data performed file system management monitoring hadoop log file implement partitioning dynamic partition bucket hive develop customize class serialization deserialization hadoop analyzed large amount datum set determine optimal way aggregate report implement proof concept deploy product amazon web services aws involve migration datum exist rdbms oracle sql server hadoop sqoop processing datum environment pig sqoop kafka apache cassandra oozie impala cloudera aws aws emr redshift flume apache hadoop hdfs hive map reduce cassandra zookeeper mysql eclipse dynamo db pl sql python ww norton nyc ny java developer roles responsibility web logic build deploy application create stub consume web service automated test code java script frog logic squish smart bear test complete client application code java selenium web application testing develop automation testing process selenium qtp involve study client testing requirement analyze feasible testing strategy development automate test script include testing finally deployment test script spring framework achieve loose coupling layer move service oriented architecture soa expose restful involve perform unit integration testing junit involve build ejb session entity bean maintain transaction management application build web page user interactive java script angular js groovy allow primitive type short form variable declaration compiler translate object extensively spring jdbc datum access layer access update information database develop web services create report module send different agency premium calculation manual class soap restful web service rich face component involve write spring mvc controller write custom validation work struts framework develop end application extensively spring middle tier entire application jax ws soap jax rs rest produce web service involve write program consume web service java spring mvc data jpa jenkins bamboo html jsp javascript jquery ajax angular js involve work struts tiles common look feel web application work web services java api xml services jax ws support building deploy web api services work team business transfer development testing code review build implementation support wrote pl sql statement accord need oracle g database work internal web base client server application build struts framework oracle backend database work establish relation different bean hibernate involve write component spring aop ioc framework involve write jsp jsf component jstl tag library core logic nested beans html tag lib create standard dynamic web page develop connection backend jdbc build entity beans bean managed persistence entity beans design develop ui framework spring mvc angularjs creation rest web services management datum apache cxf docker implementation ejb entry point web service effectively prepare organize technical inspection review code design model peer software architect identify defect selenium ensure business process deliver expect result remain reliable production release spring framework write application code restful web service external client design develop backend application server python manage application deployment python upgrade python python require recompile mode python use python enhance user experience design new web feature mvc framework like jdbc connectivity connect oracle database develop major website service include mongo db backend software good experience create consume restful soap web services develop ability consolidate critical information business financial account datum ejb hibernate perform database transactions environment java struts framework j query oracle html mark logic micro service python groovy pl sql jdbc mark logic talend hibernate ant wsdl ejb infinite computer solutions java developer roles responsibility involve phase software development life cycle sdlc application like requirement gathering design analysis code development develop hibernate mapping db model involve design develop customized tag jsp tag lib implement model view control mvc architecture struts framework spring framework develop browser base java server face end system ajax provide dynamic feature applicable implement restful web service communicate component source system firm provide datum reporting team mvc pattern gui development jsf work closely jsf lifecycle servlets jsp real time reporting complex handle business objects jira bug tracking project management prepare user documentation screenshot uat user acceptance testing implement struts validation framework server validation develop jsp custom tag libraries control business process middle tier involve integration develop web service soap wsdl apache axis interact component implement ejbs session bean business logic parser like sax dom parse xml document xml transformation xslt wrote store procedure trigger cursor oracle pl sql rational clear case version control implement java design pattern like business delegate data transfer object dto data access object service locator interact client understand need propose design team implement requirement build online system xml java script ajax strut jdbc involve technical documentation module design create sql server database stored procedures environment java jsp jdbc cassandra api python j query angular js web service rest spring core struts hibernate design patterns xml oracle apache axis ant junit uml web service soap xslt jira"
}
{
  "filename": "Candidate83_Hadoop.docx",
  "filepath": "D:\\Iqrar Workspace\\AI-Resume-Ranker\\data\\raw\\Candidate83_Hadoop.docx",
  "text": "Candidate83\nSr. Hadoop Developer\nEmail: \tcandidate8308@gmail.com \t\t\t\t\t\t             Contact: (615) 813-1551\nPROFESSIONAL SUMMARY:\n8+ years of overall software development experience on Big Data Technologies, Hadoop Eco system and Java/J2EE Technologies with experience programming in Java, Scala, Python and SQL\n4+ years of strong hands-on experience on Hadoop Ecosystem including Spark, Map-Reduce, HIVE, Pig, HDFS, YARN, HBase, Oozie, Kafka, Sqoop, Flume.\nExperience in architecting, designing, and building distributed software systems.\u00a0\nScala and Java, Created frameworks for processing data pipelines through Spark\nWrote python scripts to parse XML documents and load the data in database.\nDeep knowledge of troubleshooting and tuning Spark applications and Hive scripts to achieve optimal performance.\nUsed Sqoop to import data into HDFS / Hive from RDBMS and exporting data back to HDFS or HIVE from RDBMS.\nWorked with real-time data processing and streaming techniques using Spark streaming, Storm and Kafka.\nExperience developing Kafka producers and Kafka Consumers for streaming millions of events per second on streaming data\nSignificant experience writing custom UDF\u2019s in Hive and custom Input Formats in MapReduce.\nKnowledge of job workflow scheduling and monitoring tools like Oozie.\nStrong experience productionalizing end to end data pipelines on hadoop platform.\nExpertise in Database Design, Creation and Management of Schemas, writing Stored Procedures, Functions, DDL and DML SQL queries and writing complex queries for Oracle\nExperience working with NoSQL database technologies, including MongoDB, Cassandra and HBase.\nGood experience is designing and implementing end to end Data Security and Governance within Hadoop Platform using Kerberos.\nStrong experience with UNIX shell scripts and commands.\nExperience in using various Hadoop Distributions like Cloudera, Hortonworks and Amazon EMR.\nStrong hands-on development experience with Java, J2EE (Servlets, JSP, Java Beans, EJB, JDBC, JMS, Web Services) and related technologies.\nWork with the team to help understand requirements, evaluate new features, architecture and help drive decisions.\nExcellent interpersonal, communication, problem solving and analytical skills with ability to make independent decisions\nExperience successfully delivering applications using agile methodologies including extreme programming, SCRUM and Test-Driven Development (TDD).\nExperience in Object Oriented Analysis, Design, and Programming of distributed web-based applications.\nExtensive experience in developing standalone multithreaded applications.\nConfigured and developed web applications in Spring and employed spring MVC architecture and Inversion of Control.\nExperience in building, deploying and integrating applications in Application Servers with ANT, Maven and Gradle.\nSignificant application development experience with REST Web Services, SOAP, WSDL, and XML.\nTECHNICAL SKILLS:\nPROFESSIONAL EXPERIENCE:\nClient\t\t: \tTMNAS\t\t\t\t\t\t        Sep 2016 \u2013 Present\nLocation\t: \tBala Cynwyd, PA\t\nRole\t\t:\tSr. Hadoop Developer\nProject Description: TMNA offers the security of nearby expertise, enhanced by the diversity and power of one of the world\u2019s most respected insurance groups. Tokio Marine\u2019s companies offer access to leading commercial insurance solutions spanning the property and casualty landscape including professional liability, workers\u2019 compensation and property coverage.. The project deals with analyzing clickstream data of users who are visiting the company websites and applications to derive useful insights that help in optimizing future promotions and advertising.\nResponsibilities:\nInvolved in story-driven agile development methodology and actively participated in daily scrum meetings.\nIngested terabytes of click stream data from external systems like FTP Servers and S3 buckets into HDFS using custom Input Adaptors.\nImplemented end-to-end pipelines for performing user behavioral analytics to identify user-browsing patterns and provide rich experience and personalization to the visitors.\nDeveloped Kafka producers for streaming real-time clickstream events from external Rest services into topics.\nUsed HDFS File System API to connect to FTP Server and HDFS.  S3 AWS SDK for connecting to S3 buckets.\nWritten Scala based Spark applications for performing various data transformations, denormalization, and other custom processing.\nImplemented data pipeline using Spark, Hive, Sqoop and Kafka to ingest customer behavioral data into Hadoop platform to perform user behavioral analytics.\nCreated a multi-threaded Java application running on edge node for pulling the raw clickstream data from FTP servers and AWS S3 buckets.\nDeveloped Spark streaming jobs using Scala for real time processing.\nInvolved in creating external Hive tables from the files stored in the HDFS.\nOptimized the Hive tables utilizing improvement techniques like partitions and bucketing to give better execution Hive QL queries.\nUsed Spark-SQL to read data from hive tables, and perform various transformations like changing date format and breaking complex columns. \nWrote spark application to load the transformed data back into the Hive tables using parquet format.\nUsed Oozie Scheduler system to automate the pipeline workflow to exact data on a timely manner. \nImplemented installation and configuration of multi-node cluster on the cloud using Amazon \nWeb Services (AWS) on EC2.\nWorked on data visualization and analytics with research scientist and business stake holders.\nEnvironment: Hadoop 2.x, Spark, Scala, Hive, Pig, Sqoop, Oozie, Kafka, Cloudera Manager, Storm, ZooKeeper, HBase, Impala, YARN, Cassandra, JIRA, MySQL, Kerberos, Amazon AWS, Shell Scripting, SBT, Git, Maven.\nClient\t\t:\tDavita Inc\t\t\t    \t\t\t    Jan 2015 - Sep 2016 \nLocation\t:\tNashville, Tennessee\nRole\t\t:\tSr.Hadoop Developer\nProject Description: Davita is one of the largest kidney dialysis companies in world. The idea of the project is to ingest data from different multiple sources to Hadoop Data Lake, perform transformations on it according to business requirements and exporting the data to external systems. The system is a scalable BI platform that can adapt to the speed of the business by providing relevant, accessible, timely, connected, and accurate data.\nResponsibilities:\nInvolved in gathering and analyzing business requirements and designing Data Lake as per the requirements.\nBuilt distributed, scalable, and reliable data pipelines that ingest and process data at scale using Hive and MapReduce.\nDeveloped MapReduce jobs in Java for cleansing the data and preprocessing.\nLoaded transactional data from Teradata using Sqoop and create Hive Tables.\nExtensively used Sqoop for efficiently transferring bulk data between HDFS and relational databases.\nWorked on automation of delta feeds from Teradata using Sqoop and from FTP Servers to Hive.\nWorked on various performance optimizations like using distributed cache for small datasets, Partition,\u00a0Bucketing in hive and Map Side joins.\nCreated components like Hive UDFs for missing functionality in HIVE for analytics.\nUsed IMPALA to analyze the data present in Hive tables.\nHandled Avro and JSON data in Hive using Hive SerDe.\nWorked with different compression codecs like GZIP, SNAPPY and BZIP2 in MapReduce, Pig and Hive for better performance.\nAnalyzed the data by performing the Hive queries using Hive QL to study the customer behavior.\nWrote python scripts to parse XML documents and load the data in database.\nGenerate auto mails by using Python scripts.\nImplemented the recurring workflows using Oozie to automate the scheduling flow.\nWorked with application teams to install OS level updates and version upgrades for Hadoop cluster environments.\nParticipated in design and code reviews.\nEnvironment: HDFS, Hadoop, Pig, Hive, HBase, Sqoop, Talend, Flume, Map Reduce, Podium Data, Oozie, Java 6/7, Oracle 10g, YARN, UNIX Shell Scripting, SOAP, REST services, Oracle 10g, Maven, Agile Methodology, JIRA.\nClient\t\t:\tNASBA\t\t\t\t\t\t\t    Aug 2012 - Dec 2014 \nLocation\t:\tNashville, TN\nRole\t\t:\tHadoop Developer\nProject Description: National Association of State Boards of Accountancy enhances the effectiveness and advance the common interests of the Boards of Accountancy. Existing ETL platform is overloaded with data coming from variety of sources and as data is growing day by day, it is not able to perform well and cost of managing the Relational database servers are going up. So, the data is migrated from multiple sources to Hadoop Data Lake and transformations are performed on it according to business requirements and the processed data is exported to external systems.\nResponsibilities: \nAnalysed business requirements and created/updated Software Requirements and design documents\t\nImported the data from relational databases to Hadoop cluster by using Sqoop. \nProvided batch processing solution to certain unstructured and large volume of data by using Hadoop Map Reduce framework.\nDeveloped data pipelines using Hive scripts to transform data from Teradata, DB2 data sources. These pipelines had customized UDF'S to extend the ETL functionality. \nDeveloped UDF for converting data from Hive table to JSON format as per client requirement.\u00a0\nInvolved in creating tables in Hive and writing scripts and queries to load data into Hive tables from HDFS.\nImplemented dynamic partitioning and Bucketing in Hive as part of performance tuning.\u00a0\nCreated custom UDF\u2019s in Pig and Hive.\nPerformed various transformations on data like changing date patterns, converting to other time zones etc.\nDesigned and developed PIG Latin Scripts to process data in a batch to perform trend analysis.\u00a0\nAutomated Sqoop, hive and pig jobs using Oozie scheduling. \nStoring, processing and analyzing huge data-set for getting valuable insights from them.\u00a0\nCreated various aggregated datasets for easy and faster reporting using Tableau.\nEnvironment: HDFS, Map Reduce, Hive, Sqoop, Pig, HBase, Oozie, CDH distribution, Java, Eclipse, Shell Scripts, Tableau, Windows, Linux.\nClient\t\t:\tCopart Inc\t\t\t\t\t\t    Oct 2010 - Aug 2012 \nLocation\t:\tDallas, TX\nRole\t\t:\tJava Developer\nProject Description:  Copart makes it easy for Members to find, bid and win the vehicles that they are looking for. Members can choose from\u00a0classics, early and late model\u00a0cars\u00a0and\u00a0trucks,\u00a0industrial vehicles\u00a0and more. These internal applications are used for content management through which users can request a new site, which is used to store their project related documents.\nResponsibilities:\nDeveloped the J2EE application based on the Service Oriented Architecture by employing SOAP and other tools for data exchanges and updates.\nWorked in all the modules of the application which involved front-end presentation logic - developed using Spring MVC, JSP, JSTL and JavaScript, Business objects - developed using POJOs and data access layer - using Hibernate framework.\u00a0\nDesigned the GUI of the application using JavaScript, HTML, CSS, Servlets, and JSP.\nInvolved in writing AJAX scripts for the requests to process quickly.\nUsed Dependency Injection feature and AOP features of Spring framework to handle exceptions.\nInvolved in writing Hibernate Query Language (HQL) for persistence layer.\nImplemented persistence layer using Hibernate that uses the POJOs to represent the persistence database.\nUsed JDBC to connect to backend databases, Oracle and SQL Server 2005.  \nProficient in writing SQL queries, stored procedures for multiple databases, Oracle and SQL Server.\nWrote backend jobs based on Core Java & Oracle Data Base to be run daily/weekly.\nUsed Restful API and SOAP web services for internal and external consumption.\nUsed Core Java concepts like Collections, Garbage Collection, Multithreading, OOPs concepts and APIs to do encryption and compression of incoming request to provide security.\nWritten and implemented test scripts to support Test driven development (TDD) and continuous integration.\nEnvironment: Java, JSP, HTML, CSS, Ubuntu Operating System, JavaScript, AJAX, Servlets, Struts, Hibernate, EJB (Session Beans), Log4J, WebSphere, JNDI, Oracle, Windows XP, LINUX, ANT, Eclipse.\nClient\t\t:\tAricent\t\t\t\t\t\t\t    Nov 2008 - Sep 2010    \nLocation\t:\tNewyork\nRole\t\t:\tJava Developer\nProject Description:  Aricent is a global design and engineering company innovating in the digital era. help the world's leading companies solve their most important business and technology innovation challenges - from Customer to Chip. I have worked on developing the Aricent internal applications to automate the business process, store the documents. These internal applications are used for content management through which users can request a new site, which is used to store their project related documents.\nResponsibilities:\nAnalyzed user requirements and created Software Requirements and design documents\nResponsible for GUI development using Java, JSP, Struts\nDatabase design and development \nCreated and modified existing database scripts, Tables, Stored Procedures, and Triggers \nUsed XML functions, Cursors, Mail and Utility packages for Advanced search functionality\nCreated data correction and manipulation scripts for Production\nUsed JAXB for marshalling and un-marshalling of the data\nCreated JUnit tests for the service layer\nSupport for Production issues\nAttending the review meetings for scheduling, implementation and resolving issues in software development cycle\nEnvironment: Java, Struts, Java, Jsp, Servlets, JQuery, Ajax, XML, XSLT, JAXB, FOP, JBoss, Weblogic, Tomcat, SQL server 2005 and MyEclipse\nEDUCATION:\nBachelor of Technology in Computer Science.               ",
  "size_bytes": 34833,
  "num_pages": null,
  "extracted_at": "2025-09-10T18:15:51.947085",
  "clean_text": "sr hadoop developer email contact professional summary year overall software development experience big data technologies hadoop eco system java technologies experience programming java scala python sql year strong hand experience hadoop ecosystem include spark map reduce hive pig hdfs yarn hbase oozie kafka sqoop flume experience architecting design building distribute software system scala java create framework process datum pipeline spark write python script parse xml document load datum database deep knowledge troubleshooting tune spark application hive script achieve optimal performance sqoop import datum hdfs hive rdbms export datum hdfs hive rdbms work real time datum processing stream technique spark streaming storm kafka experience develop kafka producer kafka consumers stream million event second stream datum significant experience write custom udf hive custom input formats mapreduce knowledge job workflow scheduling monitoring tool like oozie strong experience productionalizing end end datum pipeline hadoop platform expertise database design creation management schemas write stored procedures functions ddl dml sql query write complex query oracle experience work nosql database technology include mongodb cassandra hbase good experience design implement end end data security governance hadoop platform kerberos strong experience unix shell script command experience hadoop distributions like cloudera hortonworks amazon emr strong hand development experience java servlets jsp java beans ejb jdbc jms web services related technology work team help understand requirement evaluate new feature architecture help drive decision excellent interpersonal communication problem solve analytical skill ability independent decision experience successfully deliver application agile methodology include extreme programming scrum test driven development tdd experience object oriented analysis design programming distribute web base application extensive experience develop standalone multithreaded application configure develop web application spring employ spring mvc architecture inversion control experience building deploy integrate application application servers ant maven gradle significant application development experience rest web services soap wsdl xml technical skills professional experience client tmnas sep present location bala cynwyd pa role sr hadoop developer project description tmna offer security nearby expertise enhance diversity power world respected insurance group tokio marine company offer access lead commercial insurance solution span property casualty landscape include professional liability worker compensation property coverage project deal analyze clickstream datum user visit company website application derive useful insight help optimize future promotion advertising responsibility involve story drive agile development methodology actively participate daily scrum meeting ingested terabyte click stream datum external system like ftp servers bucket hdfs custom input adaptors implement end end pipeline perform user behavioral analytic identify user browse pattern provide rich experience personalization visitor develop kafka producer stream real time clickstream event external rest service topic hdfs file system api connect ftp server hdfs aws sdk connect bucket write scala base spark application perform datum transformation denormalization custom processing implement data pipeline spark hive sqoop kafka ingest customer behavioral datum hadoop platform perform user behavioral analytic create multi thread java application run edge node pull raw clickstream datum ftp server aws bucket develop spark streaming job scala real time processing involve create external hive table file store hdf optimize hive table utilize improvement technique like partition bucket well execution hive ql query spark sql read datum hive table perform transformation like change date format break complex column write spark application load transformed datum hive table parquet format oozie scheduler system automate pipeline workflow exact datum timely manner implement installation configuration multi node cluster cloud amazon web services aws work data visualization analytic research scientist business stake holder environment hadoop spark scala hive pig sqoop oozie kafka cloudera manager storm zookeeper hbase impala yarn cassandra jira mysql kerberos amazon aws shell scripting sbt git maven client davita inc jan sep location nashville tennessee role sr hadoop developer project description davita large kidney dialysis company world idea project ingest datum different multiple source hadoop data lake perform transformation accord business requirement export datum external system system scalable bi platform adapt speed business provide relevant accessible timely connected accurate datum responsibility involve gather analyze business requirement design data lake requirement build distribute scalable reliable data pipeline ingest process datum scale hive mapreduce develop mapreduce job java cleanse datum preprocessing loaded transactional datum teradata sqoop create hive tables extensively sqoop efficiently transfer bulk datum hdfs relational database work automation delta feed teradata sqoop ftp servers hive work performance optimization like distribute cache small dataset partition bucketing hive map join create component like hive udfs miss functionality hive analytic impala analyze datum present hive table handle avro json datum hive hive serde work different compression codec like gzip snappy mapreduce pig hive well performance analyze datum perform hive query hive ql study customer behavior write python script parse xml document load datum database generate auto mail python script implement recur workflow oozie automate scheduling flow work application team install os level update version upgrade hadoop cluster environment participate design code review environment hdfs hadoop pig hive hbase sqoop talend flume map reduce podium data oozie java oracle g yarn unix shell scripting soap rest service oracle g maven agile methodology jira client nasba aug dec location nashville tn role hadoop developer project description national association state boards accountancy enhance effectiveness advance common interest boards accountancy exist etl platform overload datum come variety source datum grow day day able perform cost manage relational database server go data migrate multiple source hadoop data lake transformation perform accord business requirement process data export external system responsibility analyse business requirement create update software requirements design document import datum relational database hadoop cluster sqoop provide batch processing solution certain unstructured large volume datum hadoop map reduce framework develop data pipeline hive script transform datum teradata datum source pipeline customize udf extend etl functionality develop udf convert datum hive table json format client requirement involve create table hive write script query load datum hive table hdfs implement dynamic partitioning bucketing hive performance tuning create custom udf pig hive perform transformation datum like change date pattern convert time zone etc design develop pig latin scripts process datum batch perform trend analysis automated sqoop hive pig job oozie scheduling store processing analyze huge data set get valuable insight create aggregated dataset easy fast reporting tableau environment hdfs map reduce hive sqoop pig hbase oozie cdh distribution java eclipse shell scripts tableau windows linux client copart inc oct aug location dallas tx role java developer project description copart make easy member find bid win vehicle look member choose classic early late model car truck industrial vehicle internal application content management user request new site store project relate document responsibility develop application base service orient architecture employ soap tool datum exchange update work module application involve end presentation logic develop spring mvc jsp jstl javascript business object develop pojo datum access layer hibernate framework design gui application javascript html css servlets jsp involve write ajax script request process quickly dependency injection feature aop feature spring framework handle exception involve write hibernate query language hql persistence layer implement persistence layer hibernate use pojo represent persistence database jdbc connect backend database oracle sql server proficient write sql query store procedure multiple database oracle sql server wrote backend job base core java oracle data base run daily weekly restful api soap web service internal external consumption core java concept like collection garbage collection multithreading oop concept api encryption compression incoming request provide security write implement test script support test drive development tdd continuous integration environment java jsp html css ubuntu operating system javascript ajax servlets struts hibernate ejb session beans websphere jndi oracle windows xp linux ant eclipse client aricent nov sep location newyork role java developer project description aricent global design engineering company innovate digital era help world lead company solve important business technology innovation challenge customer chip work develop aricent internal application automate business process store document internal application content management user request new site store project relate document responsibility analyze user requirement create software requirements design document responsible gui development java jsp struts database design development create modify exist database script tables stored procedure triggers xml function cursors mail utility package advanced search functionality create data correction manipulation script production jaxb marshal un marshalling datum create junit test service layer support production issue attend review meeting scheduling implementation resolve issue software development cycle environment java struts java jsp servlets jquery ajax xml xslt jaxb fop jboss weblogic tomcat sql server myeclipse education bachelor technology computer science"
}
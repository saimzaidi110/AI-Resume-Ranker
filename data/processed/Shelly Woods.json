{
  "filename": "Shelly Woods.docx",
  "filepath": "D:\\Iqrar Workspace\\AI-Resume-Ranker\\data\\raw\\Shelly Woods.docx",
  "text": "Shelly Woods\n(201) 203-3169\nShelly.woods@hrmediary.com                                                                                                                                                      \nProfessional Summary\n10+ years of professional experience in IT on JAVA, JEE including 2+ years of hands on experience in Big Data, Hadoop Ecosystem Components.\nExpertise in developing application with financial domain, using Enterprise Technologies pertaining to  Core Java 1.8 , JEE , Servlets 2.2/2.3, JSP 2.0, Struts 2.0, Hibernate 3.0, Spring IOC, Spring MVC, Spring Boot Hibernate, JMS,XML, JDBC 2.0, JNDI, JAXP ,JAXB, Web Logic ,Web Sphere and Tomcat.\nExperience in Web Services using XML, HTML, SOAP and REST API.\nSolid background in Object Oriented Analysis & Design, Development and Implementation of Client Server/Web/Enterprise development using n-tier architecture\nknowledge of Angular JS practices ,Creating custom, general use modules and components which extend the elements and modules of core Angular JS\nStrong experience creating real time data streaming solutions using Apache Spark Core, Spark SQL & Data Frames, Spark Streaming \nIn depth knowledge of Hadoop Architecture and YARN\nExperience in writing Map Reduce programs using Apache Hadoop for analyzing Big Data.\nHands on experience in writing Ad-hoc Queries for moving data from HDFS to HIVE and analyzing the data using HIVE QL.\nExperience in importing and exporting data using Sqoop from Relational Database Systems to HDFS.\nExperience in writing Hadoop Jobs for analyzing data using Pig Latin.\nWorking Knowledge in NoSQL Databases like HBase.\nIntegrated Apache Kafka for data ingestion\nExperience in using Apache Flume for collecting, aggregating and moving large amounts of data from application servers.\nExperience in using Zookeeper and Oozie Operational Services for coordinating the cluster and scheduling workflows.\nExtensive experience with SQL, PL/SQL, Shell Scripting and database concepts.\nExperience in using version control management tools like CVS, SVN and Rational Clear Case.\nHighly motivated, self-starter with a positive attitude, willingness to learn new concepts and acceptance of challenges.\nAbility to work independently and with a group of peers in a results-driven environment. Strong analytical and problem solving skills. Ability to take initiative and learn emerging technologies and programming languages\nEducation:\nM.C.A (Master of Computer Applications) from Osmania University, 2006.\nB.S.C (Computer Science) from Kakatiya University, 2003.\nTechnical Skills:\t\t\nProfessional Experience:\nClient: \tAT&T, USA\t\t                                                                                               April 2017-Persent.\t\nTitle: Senior Analytical Hadoop / Spark Developer\nECOMP is critical in achieving AT&T\u2019s D2 imperatives to increase the value of our network to customers by rapidly on-boarding new services (created by AT&T or 3rd parties), enabling the creation of a new ecosystem of cloud consumer and enterprise services, reducing Capital and Operational Expenditures, and providing Operations efficiencies. It delivers enhanced customer experience by allowing them in near real time to reconfigure their network, services, and capacity. While ECOMP does not directly support legacy physical elements, it works with traditional OSS\u2019s to provide a seamless customer experience across both virtual and physical elements. ECOMP enables network agility, elasticity, and improves Time-to-Market/Revenue/Scale via the AT&T Service Design and Creation (ASDC) visual modeling and design. The service design and creation capabilities and policy recipes eliminate many of the manual and long running processes performed via traditional OSS\u2019s (e.g., break-fix largely moves to plan and build function). The ECOMP platform provides external applications (OSS/BSS, customer apps, and 3rd party integration) with a secured, RESTful API access control to ECOMP services.\nResponsibilities:\nDeveloped Spark scripts by using Scala shell commands as per the requirement.\nUsed Spark API over Cloudera Hadoop YARN to perform analytics on data in Hive.\nDeveloped Scala scripts, UDFFs using both Data frames/SQL and RDD/MapReduce in Spark  for Data Aggregation, queries and writing data back into OLTP system through Sqoop.\nExperienced in performance tuning of Spark Applications for setting right Batch Interval time, correct level of Parallelism and memory tuning.\nLoaded the data into Spark RDD and do in memory data Computation to generate the Output response.\nOptimizing of existing algorithms in Hadoop using Spark Context, Spark-SQL, Data Frames and Pair RDD's.\nInvolved in loading data from Oracle database into HDFS using Sqoop queries.\nResponsible for building scalable distributed data solutions using Hadoop.\nDeveloped Map Reduce pipeline jobs to process the data and create necessary HFiles. \nDeveloped Pig Latin scripts for data cleansing.\nEnvironment: HDFS, Hive, HBase, Spark Core, Spark SQL, Spark Streaming, Scala, SBT 0.13, Oozie, kafka-0.9.0, Apache NiFi ,Java (jdk1.7 & 1.8 ), UNIX, SVN and Zookeeper, JEE, JSP, JSTL,  Spring 2.5, Oracle 11g/10g,Maven, REST-ful Web Services,  Apache Axis2, LINUX,  Tomcat7,GIt,Jenikins.\nClient: \tAmerican Express, USA\t\t                               August 2016-March2017.\t\nTitle: Senior Java/Hadoop Developer\nProject Description:\nAmerican Express Company is a global travel, financial and network service provider. The company provides individuals with charge and credit cards. In CCSG we build new application pages for the given cards (Charge/Credit). Enables prospect to apply for personal cards. Prospect has an option to apply for a personal card through various channels.\nLong Application: User is not an American Express card holder and wants to apply for a new card.\nShort Application: If already a card member, has an option to apply for new cards, with a difference that user is asked to fill in minimal details\nResponsibilities: \nInvolved in writing MAVEN project, primary responsibilities include development of web application using spring MVC and Hibernate to pull queries from oracle\nInvolved in loading data from Oracle database into HDFS using Sqoop queries.\nResponsible for building scalable distributed data solutions using Hadoop.\nDeveloped Map Reduce pipeline jobs to process the data and create necessary HFiles. \nDeveloped Pig Latin scripts for data cleansing.\nWorked with different File Formats like TEXTFILE, AVROFILE for HIVE querying and processing.\n Developed PIG UDF'S for manipulating the data according to Business Requirements and also worked on developing custom PIG Loaders.\nI have successfully written Spark Streaming application to read streaming twitter data and analyze twitter records in real time using Yardstick framework to measure performance of Apache Ignite Streaming and Apache Spark Streaming. \nUsing Apache Nifi to Stream data Feeds to Kafka.\nInvolved in writing MAVEN project, primary responsibilities include development of web application using spring MVC and Hibernate to pull queries from oracle.\nImplemented test cases for Spark using Scala as language. \nEnvironment: Hadoop, Map Reduce, HDFS, Hive, HBase, Spark Core, Spark SQL, Spark Streaming, Scala, SBT 0.13, Oozie, kafka-0.9.0, Apache NiFi ,Java (jdk1.7 & 1.8 ), UNIX, SVN and Zookeeper, JEE, JSP, JSTL,  Spring 2.5, Oracle 11g/10g,Maven, REST-ful Web Services, SOAP, Apache Axis2, LINUX,  Tomcat7\nClient: \tUnited Overseas Bank, Singapore\t\t\t\tSeptember 2014- August 2016\nTitle:   Tech Lead( Java/Hadoop)\nDescription: \t         \nUOB, The Power Lender CE is a loan origination system supporting all kinds of loans. The Main objective of this project is to build an interface to interact with External Fraud Detection System for both Secured and Unsecured Loans. This system consists of two basic modules such as Secured Loans are mortgage loans, housing loans or any loan that is availed form the bank on providing some security or collateral. Unsecured Loans are credit cards, personal loans, vehicle loans or any loan that is availed from the bank without any security. \nResponsibilities:\nCollaborated with the Business Intelligence team to understand the high level data roadmap and define data discovery priorities.\nInstalled and configured Hadoop-1.0.2 Map Reduce, HDFS, and developed multiple Map Reduce jobs in Java for data cleaning and preprocessing. \nAnalyzed Hadoop cluster using the big data analytical tools such as Pig, Hive, Scoop and Flume. \nCollected and aggregated large amounts of web log data from different sources such as web servers, mobile and network devices using Apache Flume and stored the data into HDFS for analysis.\nDeveloped optimal strategies for distributing the web log data over the cluster, importing and exporting the stored web log data into HDFS and Hive using Scoop.\nInvolved in creating Hive tables, loading millions of records of the stored log data and writing queries that will invoke and run the Map Reduce jobs in the backend. \nTransformed large sets of semi-structured and unstructured data in various formats, to extract parameters such as user location, age, spending time etc.\nAnalyzed the web log data using Hive to calculate metrics such as number of unique visitors, page views, etc.\nExported the analyzed data to relational databases using Sqoop for visualization and generating reports.\nDesigned efficient high-performing applications to extract, transform, load, and query very large datasets, including unstructured data. \nInstalled Apache Oozie workflow engine to run multiple Hive and Pig jobs independently with time and data availability.\nModelled user behavior based upon previous findings and most relevant data available, and contributed to the development of tools for tracking and understanding user behavior.\nWorked on Hive joins to produce the input data set.\nEnvironment: Hadoop, Map Reduce, HDFS, Hive, Oracle 11g/10g, HBase, Spark Core, Spark SQL, Spark Streaming, Scala, SBT  0.13 ,Oozie, Java (jdk1.6), UNIX, SVN and Zookeeper,Java, J2EE, JSP, JSTL, AngularJS, Spring 2.5,Maven, REST-ful Web Services, SOAP, Apache Axis2, LINUX,  Tomcat7,Maven\nUniversal Weather & Aviation.                                        July 2010 - August 2014\t \nTitle: Senior Java Developer\nDescription:\t          \nThe application will provide enhanced electronic versions of Universal's product UVTripPlanner. It will provide search facility for ground services and airport data. This is the web application provides information about weather and aviation details for registered users across the globe. User can get the data to every single airport includes airport address, frequencies, customs, and services available for different types of aircrafts. \u00a0\nResponsibilities: \nUsed Agile Methodologies to manage full life-cycle development of the project.\nDeveloped application using Struts, spring and Hibernate.\nDeveloped rich user interface using JavaScript, JSTL, CSS, JQuery and JSP\u2019s.\nDeveloped custom tags for implementing logic in JSP\u2019s.\nUsed Java script, JQuery, JSTL, CSS and Struts 2 tags for developing the JSP\u2019S.\nInvolved in making release builds for deploying the application for test environments.\nUsed Oracle database as backend database.\nWrote SQL to update and create database tables.\nUsed Eclipse as IDE.\nUsing RIDC Interface get content details and Create Content through application.\nUsed Spring IOC for injecting the beans.\nUsed Hibernate for connecting to the database and mapping the entities by using hibernate annotations.\nCreated JUnit test cases for unit testing application.\nWriting/integration JSP communicating to spring controller  and passing query criteria to hibernate to pull data  and showing reports based on searches both on web gui and streaming data into excel .\nInvolved in writing MAVEN project, primary responsibilities include development of web application using spring MVC and Hibernate to pull queries from oracle.\nDevelopment Tools are Maven, Spring MVC,Hibernate\nAppserversIwebservers:Tomcat7\nUsed JUNIT and JMOCK for unit testing.\nEnvironment: J2EE1.6, JSP, JSTL, Ajax, Spring 2.5, Struts 2.0, Ajax, Hibernate 3.2,JDBC, JNDI,XML, XSLT, Web Services, WSDL, Log4j, ORACLE 11g, Oracle Web logic Server 10.3, SVN, Windows XP, UML.\nHoneywell                                                                                                 December 2009- July 2010\nJava Developer\nDescription:     The Honeywell_AP process covers processing of all vendor related payments for clients, which includes processing of invoices, credit notes and payment requests etc. On an overall basis, timely and accurate payments with complete controllership and diligent customer service are the required deliverables of AP\n\nResponsibilities: \nInvolved in the requirements gathering. Design, Development, Unit testing and Bug fixing.\nUsed Agile Methodologies to manage full life-cycle development of the project.\nInvolved in development of GUI for Client Using Java Swing Development using Java1.6, Java swing worker also developed the entire online help for the application system using the Java help system, the application had perspectives for views to support multiple tab Components in the UI, it was packaged as a web start build.\nInvolved in creating ant scripts for the Jnlp files to update client builds for the Usability tests.\nWorked with Generics using Java1.6 and some other open source tools to build the C2C1M Client like, Table Layout, Jakarta P01etc, Worked with JDIC (Java Desktop integration Components) components to embed a web browser displaying HTML in a Java window.\nWorked on JMS to publish and subscribe to topics messages downstream to update legacy data using IBM MQ   series. \nCreated/Configured Hibernate mapping classes, 1-Hibernate configuration files (XML files) to use the Hibernate frame work to update or view Database records, also created Criteria Queries to retrieve data from the Database.\nDeveloped RMT servers of client and server could communicate for data updates and retrievals. The combination of hibernate with RMI was simple and perfect solution for concurrency issues.\nSoftware Development Methodology used to develop the project was Agile Methodology (with small scrum teams).Connected client to web service (JAX\u2014RPC) to retrieve data and d display for central storage, the application was developed over (lie spring framework.)\nWorked on writing/updating ANT scripts while creating web archive files/enterprise archive files the jars were signed to maintain security for the systems.\nBuilt high performance java servers and used maven for building/packaging.\nDesign of the Client side of the application using Java swing (1.6) using the JSR296 application framework.\nCollaborating with analysis team to review the developed UI and packaging/deployment the application for UAT and SIT using java web start build techniques.\nDevelopment tools: JavaSwing,Intellijidea,YourKit Profiler \nApplication server: Websphere\nServer technology: RMI Remote Method Invocation), Hibernate. Version Control;Clearcase\nSpecial tools; JDIC, Table Layout (opensourceswing\u2019s layout), Glazed Lists, lntelliJ IDEA, Your Kit Java Profiler (to clear memory leaks in the client/server code), Deadlocks P0I (Apache for exporting to excel from the GUI tables), hibernate..\nEnvironment: J2EE1.6, JSP, JSTL, Ajax, Spring 2.5, Struts 2.0, Ajax, Hibernate 3.2,JDBC, JNDI,XML, XSLT, Web Services, WSDL, Log4j, ORACLE 11g, Oracle Web logic Server 10.3, SVN, Windows XP, UML, Sterling Commerce Distributed Order Management(DOM).\nSpecialty Planners Inc                                                                           November 2007- December 2009\nJava/J2EE Developer\nDescription:  \nSpecialty Planners Inc. is an Insurance Agency sales company that sells Long Term Care (LTC) products of various Insurance companies (called Carriers) and gets commission for the business done, from the Carriers. Agents are recruited and trained by Specialty Planners to do business for the Carriers and the Carrier takes agent into contract. Specialty Planners supplies leads (prospect customer information) to its Agents via AIMS IC online application for doing business. The main objective of the AIMS application is that provides support and management for the Specialty Planners business. The system will allow Specialty Planners to operate efficiently and support brokerage business model and third party administration business.\nResponsibilities:\nInvolved in Unit Testing of the Application.\nDeveloped the JSP\u2019s and Modeling data using MVC architecture.\nInvolved in designing front-end screens.\nDocumenting daily weekly status report and sending to client\nImplemented the Servlets and JSP components\nUsed Hibernate for Object Relational Mapping and data persistence.\n Developed the Database interaction classes using JDBC.\nCreated JUnit test cases and ANT scripts for build automation.\nEnvironment: Java, J2EE 1.4, HTML, XML, JDBC, JMS,  Servlets, JSP 1.2, Struts 1.2, Hibernate, Web services, Eclipse 3.3, Web Sphere 7,  Oracle 9i, ANT, Microsoft Visio.",
  "size_bytes": 42803,
  "num_pages": null,
  "extracted_at": "2025-09-10T18:16:03.050694",
  "clean_text": "shelly woods professional summary year professional experience java jee include year hand experience big data hadoop ecosystem components expertise develop application financial domain enterprise technologies pertain core java jee servlets jsp strut hibernate spring ioc spring mvc spring boot hibernate jms xml jdbc jndi jaxp jaxb web logic web sphere tomcat experience web services xml html soap rest api solid background object oriented analysis design development implementation client server web enterprise development n tier architecture knowledge angular js practice create custom general use module component extend element module core angular js strong experience create real time datum stream solution apache spark core spark sql data frames spark streaming depth knowledge hadoop architecture yarn experience write map reduce program apache hadoop analyze big data hand experience write ad hoc query move datum hdfs hive analyze datum hive ql experience import export datum sqoop relational database systems hdfs experience write hadoop jobs analyze datum pig latin work knowledge nosql database like hbase integrated apache kafka datum ingestion experience apache flume collect aggregate move large amount datum application server experience zookeeper oozie operational services coordinate cluster scheduling workflow extensive experience sql pl sql shell scripting database concept experience version control management tool like cvs svn rational clear case highly motivated self starter positive attitude willingness learn new concept acceptance challenge ability work independently group peer result drive environment strong analytical problem solve skill ability initiative learn emerge technology programming language education master computer applications osmania university computer science kakatiya university technical skills professional experience client usa april persent title senior analytical hadoop spark developer ecomp critical achieve imperative increase value network customer rapidly boarding new service create party enable creation new ecosystem cloud consumer enterprise service reduce capital operational expenditures provide operations efficiency deliver enhanced customer experience allow near real time reconfigure network service capacity ecomp directly support legacy physical element work traditional oss provide seamless customer experience virtual physical element ecomp enable network agility elasticity improve time market revenue scale service design creation asdc visual modeling design service design creation capability policy recipe eliminate manual long running process perform traditional oss break fix largely move plan build function ecomp platform provide external application oss bss customer app party integration secure restful api access control ecomp service responsibility develop spark script scala shell command requirement spark api cloudera hadoop yarn perform analytic datum hive develop scala script udff data frame sql rdd mapreduce spark data aggregation query write datum oltp system sqoop experience performance tuning spark applications set right batch interval time correct level parallelism memory tuning load datum spark rdd memory datum computation generate output response optimize exist algorithm hadoop spark context spark sql data frames pair rdd involve load datum oracle database hdfs sqoop query responsible build scalable distribute datum solution hadoop develop map reduce pipeline job process datum create necessary hfiles develop pig latin script datum cleansing environment hdfs hive hbase spark core spark sql spark streaming scala sbt oozie apache nifi java unix svn zookeeper jee jsp jstl spring oracle maven rest ful web services apache linux jenikin client american express usa august title senior java hadoop developer project description american express company global travel financial network service provider company provide individual charge credit card ccsg build new application page give card charge credit enable prospect apply personal card prospect option apply personal card channel long application user american express card holder want apply new card short application card member option apply new card difference user ask fill minimal detail responsibility involve write maven project primary responsibility include development web application spring mvc hibernate pull query oracle involve load datum oracle database hdfs sqoop query responsible build scalable distribute datum solution hadoop develop map reduce pipeline job process datum create necessary hfiles develop pig latin script datum cleansing work different file format like textfile avrofile hive querying processing develop pig udf manipulate datum accord business requirements work develop custom pig loaders successfully write spark streaming application read streaming twitter datum analyze twitter record real time yardstick framework measure performance apache ignite streaming apache spark streaming apache nifi stream datum feed kafka involve write maven project primary responsibility include development web application spring mvc hibernate pull query oracle implement test case spark scala language environment hadoop map reduce hdfs hive hbase spark core spark sql spark streaming scala sbt oozie apache nifi java unix svn zookeeper jee jsp jstl spring oracle maven rest ful web services soap apache linux client united overseas bank singapore september august title tech lead java hadoop description uob power lender ce loan origination system support kind loan main objective project build interface interact external fraud detection system secured unsecured loans system consist basic module secured loans mortgage loan housing loan loan avail form bank provide security collateral unsecured loans credit card personal loan vehicle loan loan avail bank security responsibility collaborate business intelligence team understand high level data roadmap define datum discovery priority instal configure map reduce hdfs develop multiple map reduce job java datum cleaning preprocessing analyzed hadoop cluster big data analytical tool pig hive scoop flume collect aggregate large amount web log datum different source web server mobile network device apache flume store datum hdfs analysis develop optimal strategy distribute web log datum cluster import export store web log datum hdfs hive scoop involve create hive table load million record store log datum write query invoke run map reduce job backend transform large set semi structured unstructured datum format extract parameter user location age spend time etc analyze web log datum hive calculate metric number unique visitor page view etc export analyzed datum relational database sqoop visualization generating report design efficient high perform application extract transform load query large dataset include unstructured datum instal apache oozie workflow engine run multiple hive pig job independently time data availability model user behavior base previous finding relevant datum available contribute development tool tracking understand user behavior work hive join produce input datum set environment hadoop map reduce hdfs hive oracle g hbase spark core spark sql spark streaming scala sbt oozie java unix svn zookeeper java jsp jstl angularjs spring rest ful web services soap apache linux universal weather aviation july august title senior java developer description application provide enhanced electronic version universal product uvtripplanner provide search facility ground service airport datum web application provide information weather aviation detail registered user globe user datum single airport include airport address frequency custom service available different type aircraft responsibility agile methodologies manage life cycle development project develop application strut spring hibernate develop rich user interface javascript jstl css jquery jsp develop custom tag implement logic jsp java script jquery jstl css strut tag develop involve make release build deploy application test environment oracle database backend database write sql update create database table eclipse ide ridc interface content detail create content application spring ioc inject bean hibernate connect database map entity hibernate annotation create junit test case unit testing application writing integration jsp communicating spring controller pass query criterion hibernate pull datum showing report base search web gui streaming datum excel involve write maven project primary responsibility include development web application spring mvc hibernate pull query oracle development tools maven spring mvc hibernate appserversiwebservers junit jmock unit testing environment jsp jstl ajax spring strut ajax hibernate jndi xml xslt web services wsdl oracle g oracle web logic server svn windows xp uml honeywell december july java developer description process cover processing vendor relate payment client include processing invoice credit note payment request etc overall basis timely accurate payment complete controllership diligent customer service require deliverable ap responsibility involve requirement gathering design development unit testing bug fixing agile methodologies manage life cycle development project involve development gui client java swing development java swing worker develop entire online help application system java help system application perspective view support multiple tab components ui package web start build involve create ant script jnlp file update client build usability test work generics open source tool build m client like table layout jakarta work jdic java desktop integration components component embed web browser display html java window work jms publish subscribe topic message downstream update legacy datum ibm mq series create configure hibernate mapping class hibernate configuration file xml file use hibernate frame work update view database record create criteria queries retrieve datum database develop rmt server client server communicate datum update retrieval combination hibernate rmi simple perfect solution concurrency issue software development methodology develop project agile methodology small scrum client web service jax rpc retrieve datum d display central storage application develop lie spring framework work writing update ant script create web archive file enterprise archive file jar sign maintain security system build high performance java server maven building packaging design client application java swing application framework collaborate analysis team review develop ui packaging deployment application uat sit java web start build technique development tool javaswing intellijidea yourkit profiler application server websphere server technology rmi remote method invocation hibernate version special tool jdic table layout opensourceswe layout glazed lists lntellij idea kit java profiler clear memory leak client server code deadlocks apache export excel gui table hibernate environment jsp jstl ajax spring strut ajax hibernate jndi xml xslt web services wsdl oracle g oracle web logic server svn windows xp uml sterling commerce distributed order specialty planners inc november december java developer description specialty planners insurance agency sale company sell long term care ltc product insurance company call carriers get commission business carriers agent recruit train specialty planners business carriers carrier take agent contract specialty planners supply lead prospect customer information agent aims ic online application business main objective aims application provide support management specialty planners business system allow specialty planners operate efficiently support brokerage business model party administration business responsibility involve unit testing application develop jsp modeling datum mvc architecture involve design end screen document daily weekly status report send client implement servlets jsp component hibernate object relational mapping data persistence develop database interaction class jdbc create junit test case ant script build automation environment java html xml jdbc jms servlets jsp strut hibernate web service eclipse web sphere oracle ant microsoft visio"
}
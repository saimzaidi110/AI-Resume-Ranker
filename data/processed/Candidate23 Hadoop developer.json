{
  "filename": "Candidate23 Hadoop developer.docx",
  "filepath": "D:\\Iqrar Workspace\\AI-Resume-Ranker\\data\\raw\\Candidate23 Hadoop developer.docx",
  "text": "Candidate23\nSr. Hadoop Developer\nPhone: +1(224)-706-0020 \u00a0\u00a0\nEmail: candidate23.dbj@gmail.com \n------------------------------------------------------------------------------------------------------------------------------------------\nPROFESSIONAL SUMMARY:\nOver 8+ years of experience including 4 years of Big Data Ecosystem related technologies with full project development, implementation and deployment.\nStrong Experience working with various Hadoop ecosystem components like, Map Reduce, HDFS, Hive, Sqoop, Pig, Flume, and Oozie.\nStrong Knowledge on Architecture of Distributed systems and Parallel processing frameworks. \nIn-depth understanding of MapReduce Framework and Spark execution model.\nWorked extensively on fine-tuning long running Spark Applications to utilize better parallelism and executor memory for more caching.\nStrong experience working with both batch and real-time processing using Spark framework.\nExpertise in developing production ready Spark applications utilizing Spark-Core, Data frames, Spark-SQL, Spark-ML and Spark-Streaming API's.\nHands on experience in installing, configuring and deploying Hadoop distributions in cloud environments (Amazon Web Services).\nExpertise in developing production ready Spark applications utilizing Spark-Core, Data frames, Spark-SQL, Spark-ML and Spark-Streaming API's.\nWorked on building real time data workflows using Kafka, Spark streaming and HBase.\u00a0\nWorked extensively on Hive for building complex data analytical applications.\nVery good understanding of Partitions, bucketing concepts in Hive and designed both Managed and External tables in Hive to optimize performance.\nUsed custom serDes like Regex SerDe, JSON SerDe, CSV SerDe etc., in hive to handle multiple formats of data. \nHaving knowledge in Apache Ambari platform for securing, managing and monitoring Hadoop clusters.\nExperienced in Cluster coordination services through zookeeper.\nStrong experience using different columnar file formats like Avro, RCFile, ORC and Parquet formats.\nWorked with Sqoop to move (import/export) data from a relational database into Hadoop.\nExperience working with Hadoop clusters using Cloudera, Amazon EMR and Hortonworks distributions.\nExtensive experience in performing ETL on structured, semi-structured data using Pig Latin Scripts.\nDesigned and implemented Hive and Pig UDF's using Java for evaluation, filtering, loading and storing of data.\nExperienced in job workflow scheduling and monitoring tools like Oozie.\nWell versed with UNIX and Linux command line and shell script.\nAdequate knowledge and working experience with agile methodology.\nTECHNICAL SKILLS:\nEDUCATION:\nBachelor of Technology in Computer Science Engineering \nWORK EXPERIENCE:\nCigna \u2013 Bloomfield, Connecticut                                                             \t\t       Jul\u201917 \u2013 Present \nRole: Hadoop/Spark Developer \nResponsibilities:\nDeveloped Spark applications using Scala utilizing Data frames and Spark SQL API for faster processing of data.\nDeveloped highly optimized Spark applications to perform various data cleansing, validation, transformation and summarization activities according to the requirement\nData pipeline consists Spark, Hive and Sqoop and custom built Input Adapters to ingest, transform and analyze operational data.\nDeveloped Spark jobs and Hive Jobs to summarize and transform data.\nUsed Spark for interactive queries, processing of streaming data and integration with NoSQL database HBase, Cassandra for interactive access patterns.\nInvolved in converting Hive queries into Spark transformations using Spark Data Frames in Scala.\nAutomated creation and termination of AWS EMR clusters using AWS, java sdk.\nBuilt real time data pipelines by developing Kafka producers and spark streaming applications for consuming.\nIngested syslog messages to Kafka.\nWorked on Apache Airflow to schedule single and sometimes complex chains of tasks that depend on each other on regular intervals.\nHandled importing data from relational databases into HDFS using Sqoop and performing transformations using Hive and Spark.\nHaving knowledge in Apache Ambari platform for securing, managing and monitoring Hadoop clusters.\nExported the processed data to the relational databases using Sqoop, to further visualize and generate reports for the BI team.\nExperienced in cluster coordination services through Zookeeper.\nInstalled, tested and deployed monitoring solutions with Splunk services.\nUsed Hive to analyze the partitioned and bucketed data and computed various metrics for reporting.\nDeveloped Hive scripts in Hive QL to de-normalize and aggregate the data.\nScheduled and executed workflows in Oozie to run various jobs.\nDesigning & creating ETL jobs through Talend to load huge volumes of data into Cassandra, Hadoop Ecosystem and relational databases.\nEnvironment: Hadoop, Spark, Hive, Java, Scala, Maven, Impala, Oozie, Oracle, Ambari, GitHub, Tableau, Unix, Hortonworks, Apache Airflow Kafka, Zookeeper, Sqoop, Cassandra, Talend, Splunk, HBase.\nQualcomm -- San Diego, CA                                                                   \t\t          Dec\u201916 \u2013 Jun\u201917                                                                                            \nRole: Hadoop/Spark Developer\nResponsibilities:\nPart of Big Data Center of Excellence (CoE), responsible for designing and building enterprise data analytics platform.\nWorked with respective business units in understanding the scope of the analytics requirements.\nPerformed core ETL transformations in Spark.\nAutomated data pipelines which involve data ingestion, data cleansing, data preparation and data analytics.\nCreated end to end Spark applications using Scala to perform various data cleansing, validation, transformation and summarization activities on user behavioral data.\nDeveloped end-to-end data pipeline using FTP Adaptor, Spark, Hive and Impala.\nImplemented Spark utilizing Spark-SQL heavily for faster development, and processing of data.\u00a0 \nExploring with Spark for improving the performance and optimization of the existing jobs in Hadoop using Spark-SQL, Data Frame running in Yarn mode.\nHandled importing other enterprise data from different data sources into HDFS using Sqoop and performing transformations using Hive, Map Reduce and then loading data into HBase tables.\nCollecting and aggregating large amounts of log data using Flume and staging data in HDFS for further analysis\nWrapper developed in Python for instantiating multithreaded application and running with other applications.\nAnalyzed the data by performing Hive queries (Hive QL) and running Pig scripts (Pig Latin) to study customer behavior.\nData warehousing, experience in design, development and testing, implementation and support of enterprise data warehouse.\nUsed Hive to analyze the partitioned and bucketed data and compute various metrics for reporting.\nCreated components like Hive UDFs for missing functionality in HIVE for analytics.\nWorked on various performance optimizations like using distributed cache for small datasets, Partition,\u00a0Bucketing in Hive and Map Side joins.\nCreated Oozie workflows and coordinators to automate data pipelines daily, weekly and monthly.\nAutomated creation and termination of AWS EMR clusters using AWS, java sdk.\nEnvironment: AWS EMR, Hadoop, Spark, Hive, Sqoop, HBase, UNIX, Talend, Pig, Linux, Java, Scala, Python, Ambari, Zookeeper.\nHortonworks\nMcKesson - Alpharetta, GA                                                                   \t\t        Dec\u201915 \u2013 Nov\u201916\nHadoop/Spark Developer\nResponsibilities:\nDeveloped multithreaded Java based Input adaptors for ingesting click stream data from external sources like ftp server and S3 buckets on daily basis.\nCreated various spark applications using Scala to perform various enrichment of these click stream data combined with enterprise data of the users.\nImplemented batch processing of jobs using Spark Scala API.\nDeveloped Sqoop scripts to import/export data from Oracle to HDFS and into Hive tables.\u00a0\nStored the data in columnar formats using Hive.\nInvolved building and managing NoSQL Database models using HBase.\nWorked in Spark to read the data from Hive and write it to Hbase.\nOptimized the Hive tables using optimization techniques like partitions and bucketing to provide better performance with Hive QL queries.\u00a0\nWorked with multiple file formats like Avro, Sequence, Parquet and Orc.\nConverted existing MapReduce programs to Spark Applications for handling semi structured data like JSON files, Apache Log files, and other custom log data.\nLoaded the final processed data to HBase tables to allow downstream application team to build rich and data driven applications.\nWorked with a team to improve the performance and optimization of the existing algorithms in Hadoop using Spark, Spark -SQL, Data Frame.\nImplemented business logic in Hive and written UDF\u2019s to process the data for analysis.\nUsed Oozie to define a workflow to coordinate the execution of Spark, Hive and Sqoop jobs.\nAddressing the issues occurring due to the huge volume of data and transitions.\nDesigned, documented operational problems by following standards and procedures using JIRA. \nEnvironment: Java, Hadoop 2.1.0, Map Reduce2, Spark, Unix, Pig 0.12.0, Hive 0.13.0, Linux, Sqoop 1.4.2, Flume 1.3.1, Eclipse, AWS EC2, and Cloudera CDH 4.  \nAmerican Home Shield - Memphis, TN                                                \t\t         Dec\u201914 \u2013 Nov\u201915\nRole: Hadoop Developer\nResponsibilities:\nMigrated the needed data from MySQL into HDFS using Sqoop and importing various formats of flat files in to HDFS.\nMainly worked on Hive queries to categorize data of different claims.\nInvolved in loading data from LINUX file system to HDFS\nWritten customized Hive UDFs in Java where the functionality is too complex.\nImplemented Partitioning, Dynamic Partitions, Buckets in HIVE.\nDesigning and creating Hive external tables using shared meta-store instead of derby with partitioning, dynamic partitioning and buckets.\nGenerate final reporting data using Tableau for testing by connecting to the corresponding Hive tables using Hive ODBC connector.\nResponsible to manage the test data coming from different sources\nReviewing peer table creation in Hive, data loading and queries.\nWeekly meetings with technical collaborators and active participation in code review sessions with senior and junior developers.\nMonitored System health and logs and respond accordingly to any warning or failure conditions.\nGained experience in managing and reviewing Hadoop log files.\nInvolved in scheduling Oozie workflow engine to run multiple Hive and pig jobs\nInvolved unit testing, interface testing, system testing and user acceptance testing of the workflow tool.\nCreated and maintained Technical documentation for launching Hadoop Clusters and for executing Hive queries and Pig Scripts\nEnvironment: Apache Hadoop, HDFS, Hive, Map Reduce, Core Java, Pig, Sqoop, Cloudera CDH4, Oracle, MySQL.\nProtective Life - Edina, MN                                                                      \t\t         Oct\u201913 - Nov\u201914     \nRole: Java Developer\nResponsibilities:\nImplemented a Web based Application using Servlets, JSP, spring, JDBC, XML.\u00a0\nInvolved in writing Spring Configuration XML file that contains declarations and other   dependent objects declarations.\u00a0\nUsed hibernate to connect to Database to create the DAO layer.\u00a0\nDeveloped Application Framework using Model-View-Controller using the technology Spring.\u00a0\nUsed HTML, XHTML, XML, XSLT, XPATH, JSP and Tag Libraries to develop view pages\u00a0\nMultilayer Applications construction using Open JPA, HTML5, Spring MVC.\u00a0\nAnnotated Spring Architecture (Spring Beans)\u00a0\nImplemented UNIX shell scripts to migrate various data files to S&P ratings repository\u00a0\nImplemented smooth pagination capability using\u00a0JSP to remove existing pagination utility\u00a0\nWorked on Geo API to provide geological access capability to S&P.com site.\u00a0\nInvolved in Agile process to streamline development process with iterative development.\u00a0\nCode reviews and Managing the CVS Repository.\u00a0\nPrepare builds for DEV and UAT environments.\u00a0\nParticipating in the regular team meetings sprint planning meetings, user story review meetings etc.\u00a0\nInvolved in preparing High & low level design docs with UML diagrams using Microsoft VISIO tool.\u00a0\nEnvironment: JDK 1.5, XML, HTML, XHTML, JSP, Spring, DAO, Oracle Express edition, Apache ANT, CVS, Junit, UNIX, Log4J, CSS Style Sheets, Apache Tomcat, J2EE, Maven 3\nAccenture \u2013 USA\t\t       Oct\u201911\u2013 Sep\u201913\nRole: Java Developer\nResponsibilities:\nInvolved in Requirements analysis, design, and development and testing.\nInvolved in setting up the different roles & maintained authentication to the application.\nDesigned, deployed and tested Multi-tier application using the Java technologies.\nInvolved in front end development using JSP, HTML & CSS.\nImplemented the Application using Servlets\nDeployed the application on Oracle Web logic server\nImplemented Multithreading concepts in java classes to avoid deadlocking.\nUsed MySQL database to store data and execute SQL queries on the backend.\nPrepared and Maintained test environment.\nTested the application before going live to production.\nDocumented and communicated test result to the team lead on daily basis.\nInvolved in weekly meeting with team leads and manager to discuss the issues and status of the projects.\nEnvironment: J2EE (Java, JSP, JDBC, Multi-Threading), HTML, Oracle Web logic server, Eclipse, MySQL, JUnit.\nGolan Technologies \u2013 Newyork                                                \t\t          Jun\u201909 - Sep\u201911 \nRole: Java Developer\nGolan Technologies range from turnkey solutions to custom, client-driven solutions in a variety of product categories including website development and platform based applications, demand intelligence and business insight generation. Smart sites have the ability to provide a unified user experience and consistent messaging on websites across the globe, driving a favorable brand impression.\u00a0\nResponsibilities:\nInvolved in the analysis, design, implementation, and testing of the project.\nDeveloped UI using HTML, JavaScript, CSS and JSP for interactive cross browser functionality and complex user interface.\nImplemented the end-to-end functionality of the client requirement during the development phase.\nImplemented the functionality of mapping entities to the database using Hibernate.\nWritten SQL queries involved in the JDBC connection in accordance with the business logic.\nPerformed various levels of unit testing for the entire application using the test cases, which included preparation of detail documentation for the results.\nActively participated in client meetings and taking the inputs for the additional functionality.\nInvolved in fixing bugs and unit testing with test cases using JUnit.\nEnvironment: J2EE, Spring, Hibernate, JavaScript, CSS, Servlets, MySQL",
  "size_bytes": 40572,
  "num_pages": null,
  "extracted_at": "2025-09-10T18:15:43.366940",
  "clean_text": "sr hadoop developer phone email professional summary year experience include year big data ecosystem relate technology project development implementation deployment strong experience work hadoop ecosystem component like map reduce hdfs hive sqoop pig flume oozie strong knowledge architecture distributed system parallel processing framework depth understanding mapreduce framework spark execution model work extensively fine tuning long run spark applications utilize well parallelism executor memory caching strong experience work batch real time processing spark framework expertise develop production ready spark application utilize spark core data frame spark sql spark ml spark streaming api hand experience installing configure deploy hadoop distribution cloud environment amazon web services expertise develop production ready spark application utilize spark core data frame spark sql spark ml spark streaming api work build real time datum workflow kafka spark streaming hbase work extensively hive build complex datum analytical application good understanding partitions bucket concept hive design managed external table hive optimize performance custom serdes like regex serde json serde csv serde etc hive handle multiple format datum have knowledge apache ambari platform securing manage monitor hadoop cluster experience cluster coordination service zookeeper strong experience different columnar file format like avro rcfile orc parquet format work sqoop import export datum relational database hadoop experience work hadoop cluster cloudera amazon emr hortonworks distribution extensive experience perform etl structured semi structured datum pig latin scripts design implement hive pig udf java evaluation filtering loading storing datum experience job workflow scheduling monitoring tool like oozie verse unix linux command line shell script adequate knowledge working experience agile methodology technical skill education bachelor technology computer science engineering work experience cigna bloomfield connecticut present role hadoop spark developer responsibility develop spark application scala utilize data frame spark sql api fast processing datum develop highly optimize spark application perform datum cleansing validation transformation summarization activity accord requirement data pipeline consists spark hive sqoop custom build input adapters ingest transform analyze operational datum develop spark job hive jobs summarize transform datum spark interactive query processing stream datum integration nosql database hbase cassandra interactive access pattern involve convert hive query spark transformation spark data frames scala automated creation termination aws emr cluster aws java sdk build real time datum pipeline develop kafka producer spark streaming application consume ingest syslog message kafka work apache airflow schedule single complex chain task depend regular interval handle import datum relational database hdfs sqoop perform transformation hive spark have knowledge apache ambari platform securing manage monitor hadoop cluster export process datum relational database sqoop visualize generate report bi team experience cluster coordination service zookeeper instal test deploy monitoring solution splunk service hive analyze partitioned bucketed datum compute metric report develop hive script hive ql de normalize aggregate datum schedule execute workflow oozie run job designing create etl job talend load huge volume datum cassandra hadoop ecosystem relational database environment hadoop spark hive java scala maven impala oozie oracle ambari github tableau unix hortonworks apache airflow kafka zookeeper sqoop cassandra talend splunk hbase qualcomm san diego role hadoop spark developer responsibility big data center excellence coe responsible design build enterprise datum analytic platform work respective business unit understand scope analytic requirement performed core etl transformation spark automate data pipeline involve datum ingestion datum cleansing datum preparation datum analytic create end end spark application scala perform datum cleansing validation transformation summarization activity user behavioral datum develop end end datum pipeline ftp adaptor spark hive impala implement spark utilize spark sql heavily fast development processing datum explore spark improve performance optimization exist job hadoop spark sql data frame run yarn mode handle import enterprise datum different datum source hdfs sqoop perform transformation hive map reduce load datum hbase table collect aggregate large amount log datum flume stage datum hdfs analysis wrapper develop python instantiate multithreaded application run application analyze datum perform hive query hive ql run pig script pig latin study customer behavior datum warehousing experience design development testing implementation support enterprise datum warehouse hive analyze partitioned bucketed datum compute metric report create component like hive udfs miss functionality hive analytic work performance optimization like distribute cache small dataset partition bucketing hive map join create oozie workflow coordinator automate datum pipeline daily weekly monthly automated creation termination aws emr cluster aws java sdk environment aws emr hadoop spark hive sqoop hbase unix talend pig linux java scala python ambari zookeeper hortonworks mckesson alpharetta ga hadoop spark developer responsibility developed multithreade java base input adaptor ingest click stream datum external source like ftp server bucket daily basis create spark application scala perform enrichment click stream datum combine enterprise datum user implement batch processing job spark scala api develop sqoop script import export datum oracle hdfs hive table store datum columnar format hive involved building manage nosql database model hbase work spark read datum hive write hbase optimize hive table optimization technique like partition bucket provide well performance hive ql query work multiple file format like avro sequence parquet orc convert exist mapreduce program spark applications handle semi structured datum like json file apache log file custom log datum load final process datum hbase table allow downstream application team build rich datum drive application work team improve performance optimization exist algorithm hadoop spark spark data frame implement business logic hive write udf process datum analysis oozie define workflow coordinate execution spark hive sqoop job address issue occur huge volume datum transition design document operational problem follow standard procedure jira environment java hadoop map spark unix pig hive linux sqoop flume eclipse aws cloudera cdh american home shield memphis tn role hadoop developer responsibility migrate need datum mysql hdfs sqoop import format flat file hdfs mainly work hive query categorize datum different claim involve load datum linux file system hdfs write customize hive udfs java functionality complex implement partitioning dynamic partitions buckets hive design create hive external table shared meta store instead derby partitioning dynamic partitioning bucket generate final reporting datum tableau testing connect correspond hive table hive odbc connector responsible manage test datum come different source review peer table creation hive datum loading query weekly meeting technical collaborator active participation code review session senior junior developer monitored system health log respond accordingly warning failure condition gain experience manage review hadoop log file involve schedule oozie workflow engine run multiple hive pig job involve unit testing interface testing system testing user acceptance testing workflow tool create maintain technical documentation launch hadoop clusters execute hive query pig scripts environment apache hadoop hdfs hive map reduce core java pig sqoop cloudera oracle mysql protective life edina mn role java developer responsibility implement web base application servlets jsp spring jdbc xml involve write spring configuration xml file contain declaration dependent object declaration hibernate connect database create dao layer develop application framework model view controller technology spring html xhtml xml xslt xpath jsp tag libraries develop view page multilayer applications construction open jpa spring mvc annotated spring architecture spring beans implement unix shell script migrate data file rating repository implement smooth pagination capability jsp remove exist pagination utility work geo api provide geological access capability site involve agile process streamline development process iterative development code review manage cvs repository prepare build dev uat environment participate regular team meeting sprint planning meeting user story review meeting etc involve prepare high low level design doc uml diagram microsoft visio tool environment jdk xml html xhtml jsp spring dao oracle express edition apache ant cvs junit unix css style sheet apache tomcat maven accenture usa role java developer responsibility involve requirements analysis design development testing involve set different role maintain authentication application design deploy test multi tier application java technology involve end development jsp html css implement application servlets deploy application oracle web logic server implement multithreading concept java class avoid deadlocke mysql database store datum execute sql query backend prepared maintain test environment test application go live production document communicate test result team lead daily basis involve weekly meeting team lead manager discuss issue status project environment java jsp jdbc multi threading html oracle web logic server eclipse mysql junit golan technologies newyork role java developer golan technologies range turnkey solution custom client drive solution variety product category include website development platform base application demand intelligence business insight generation smart site ability provide unified user experience consistent messaging website globe drive favorable brand impression responsibility involve analysis design implementation testing project develop ui html javascript css jsp interactive cross browser functionality complex user interface implement end end functionality client requirement development phase implement functionality mapping entity database hibernate write sql query involve jdbc connection accordance business logic perform level unit testing entire application test case include preparation detail documentation result actively participate client meeting take input additional functionality involve fix bug unit testing test case junit environment spring hibernate javascript css servlets mysql"
}
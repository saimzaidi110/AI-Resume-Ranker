[{"filename": "Candidate23_Hadoop_developer_1.docx", "text": "Candidate23\nSr. Hadoop Developer\nPhone: +1(224)-706-0020 \u00a0\u00a0\nEmail: candidate23.dbj@gmail.com \n------------------------------------------------------------------------------------------------------------------------------------------\nPROFESSIONAL SUMMARY:\nOver 8+ years of experience including 4 years of Big Data Ecosystem related technologies with full project development, implementation and deployment.\nStrong Experience working with various Hadoop ecosystem components like, Map Reduce, HDFS, Hive, Sqoop, Pig, Flume, and Oozie.\nStrong Knowledge on Architecture of Distributed systems and Parallel processing frameworks. \nIn-depth understanding of MapReduce Framework and Spark execution model.\nWorked extensively on fine-tuning long running Spark Applications to utilize better parallelism and executor memory for more caching.\nStrong experience working with both batch and real-time processing using Spark framework.\nExpertise in developing production ready Spark applications utilizing Spark-Core, Data frames, Spark-SQL, Spark-ML and Spark-Streaming API's.\nHands on experience in installing, configuring and deploying Hadoop distributions in cloud environments (Amazon Web Services).\nExpertise in developing production ready Spark applications utilizing Spark-Core, Data frames, Spark-SQL, Spark-ML and Spark-Streaming API's.\nWorked on building real time data workflows using Kafka, Spark streaming and HBase.\u00a0\nWorked extensively on Hive for building complex data analytical applications.\nVery good understanding of Partitions, bucketing concepts in Hive and designed both Managed and External tables in Hive to optimize performance.\nUsed custom serDes like Regex SerDe, JSON SerDe, CSV SerDe etc., in hive to handle multiple formats of data. \nHaving knowledge in Apache Ambari platform for securing, managing and monitoring Hadoop clusters.\nExperienced in Cluster coordination services through zookeeper.\nStrong experience using different columnar file formats like Avro, RCFile, ORC and Parquet formats.\nWorked with Sqoop to move (import/export) data from a relational database into Hadoop.\nExperience working with Hadoop clusters using Cloudera, Amazon EMR and Hortonworks distributions.\nExtensive experience in performing ETL on structured, semi-structured data using Pig Latin Scripts.\nDesigned and implemented Hive and Pig UDF's using Java for evaluation, filtering, loading and storing of data.\nExperienced in job workflow scheduling and monitoring tools like Oozie.\nWell versed with UNIX and Linux command line and shell script.\nAdequate knowledge and working experience with agile methodology.\n\nTECHNICAL SKILLS:\n\n\nEDUCATION:\nBachelor of Technology in Computer Science Engineering \n\nWORK EXPERIENCE:\n\nCigna \u2013 Bloomfield, Connecticut                                                             \t\t       Jul\u201917 \u2013 Present \nRole: Hadoop/Spark Developer \n\nResponsibilities:\nDeveloped Spark applications using Scala utilizing Data frames and Spark SQL API for faster processing of data.\nDeveloped highly optimized Spark applications to perform various data cleansing, validation, transformation and summarization activities according to the requirement\nData pipeline consists Spark, Hive and Sqoop and custom built Input Adapters to ingest, transform and analyze operational data.\nDeveloped Spark jobs and Hive Jobs to summarize and transform data.\nUsed Spark for interactive queries, processing of streaming data and integration with NoSQL database HBase, Cassandra for interactive access patterns.\nInvolved in converting Hive queries into Spark transformations using Spark Data Frames in Scala.\nAutomated creation and termination of AWS EMR clusters using AWS, java sdk.\nBuilt real time data pipelines by developing Kafka producers and spark streaming applications for consuming.\nIngested syslog messages to Kafka.\nWorked on Apache Airflow to schedule single and sometimes complex chains of tasks that depend on each other on regular intervals.\nHandled importing data from relational databases into HDFS using Sqoop and performing transformations using Hive and Spark.\nHaving knowledge in Apache Ambari platform for securing, managing and monitoring Hadoop clusters.\nExported the processed data to the relational databases using Sqoop, to further visualize and generate reports for the BI team.\nExperienced in cluster coordination services through Zookeeper.\nInstalled, tested and deployed monitoring solutions with Splunk services.\nUsed Hive to analyze the partitioned and bucketed data and computed various metrics for reporting.\nDeveloped Hive scripts in Hive QL to de-normalize and aggregate the data.\nScheduled and executed workflows in Oozie to run various jobs.\nDesigning & creating ETL jobs through Talend to load huge volumes of data into Cassandra, Hadoop Ecosystem and relational databases.\n\nEnvironment: Hadoop, Spark, Hive, Java, Scala, Maven, Impala, Oozie, Oracle, Ambari, GitHub, Tableau, Unix, Hortonworks, Apache Airflow Kafka, Zookeeper, Sqoop, Cassandra, Talend, Splunk, HBase.\n\nQualcomm -- San Diego, CA                                                                   \t\t          Dec\u201916 \u2013 Jun\u201917                                                                                            \nRole: Hadoop/Spark Developer\n\nResponsibilities:\n\nPart of Big Data Center of Excellence (CoE), responsible for designing and building enterprise data analytics platform.\nWorked with respective business units in understanding the scope of the analytics requirements.\nPerformed core ETL transformations in Spark.\nAutomated data pipelines which involve data ingestion, data cleansing, data preparation and data analytics.\nCreated end to end Spark applications using Scala to perform various data cleansing, validation, transformation and summarization activities on user behavioral data.\nDeveloped end-to-end data pipeline using FTP Adaptor, Spark, Hive and Impala.\nImplemented Spark utilizing Spark-SQL heavily for faster development, and processing of data.\u00a0 \nExploring with Spark for improving the performance and optimization of the existing jobs in Hadoop using Spark-SQL, Data Frame running in Yarn mode.\nHandled importing other enterprise data from different data sources into HDFS using Sqoop and performing transformations using Hive, Map Reduce and then loading data into HBase tables.\nCollecting and aggregating large amounts of log data using Flume and staging data in HDFS for further analysis\nWrapper developed in Python for instantiating multithreaded application and running with other applications.\nAnalyzed the data by performing Hive queries (Hive QL) and running Pig scripts (Pig Latin) to study customer behavior.\nData warehousing, experience in design, development and testing, implementation and support of enterprise data warehouse.\nUsed Hive to analyze the partitioned and bucketed data and compute various metrics for reporting.\nCreated components like Hive UDFs for missing functionality in HIVE for analytics.\nWorked on various performance optimizations like using distributed cache for small datasets, Partition,\u00a0Bucketing in Hive and Map Side joins.\nCreated Oozie workflows and coordinators to automate data pipelines daily, weekly and monthly.\nAutomated creation and termination of AWS EMR clusters using AWS, java sdk.\n\nEnvironment: AWS EMR, Hadoop, Spark, Hive, Sqoop, HBase, UNIX, Talend, Pig, Linux, Java, Scala, Python, Ambari, Zookeeper.\nHortonworks\n\nMcKesson - Alpharetta, GA                                                                   \t\t        Dec\u201915 \u2013 Nov\u201916\nHadoop/Spark Developer\n\nResponsibilities:\n\nDeveloped multithreaded Java based Input adaptors for ingesting click stream data from external sources like ftp server and S3 buckets on daily basis.\nCreated various spark applications using Scala to perform various enrichment of these click stream data combined with enterprise data of the users.\nImplemented batch processing of jobs using Spark Scala API.\nDeveloped Sqoop scripts to import/export data from Oracle to HDFS and into Hive tables.\u00a0\nStored the data in columnar formats using Hive.\nInvolved building and managing NoSQL Database models using HBase.\nWorked in Spark to read the data from Hive and write it to Hbase.\nOptimized the Hive tables using optimization techniques like partitions and bucketing to provide better performance with Hive QL queries.\u00a0\nWorked with multiple file formats like Avro, Sequence, Parquet and Orc.\nConverted existing MapReduce programs to Spark Applications for handling semi structured data like JSON files, Apache Log files, and other custom log data.\nLoaded the final processed data to HBase tables to allow downstream application team to build rich and data driven applications.\nWorked with a team to improve the performance and optimization of the existing algorithms in Hadoop using Spark, Spark -SQL, Data Frame.\nImplemented business logic in Hive and written UDF\u2019s to process the data for analysis.\nUsed Oozie to define a workflow to coordinate the execution of Spark, Hive and Sqoop jobs.\nAddressing the issues occurring due to the huge volume of data and transitions.\nDesigned, documented operational problems by following standards and procedures using JIRA. \n\nEnvironment: Java, Hadoop 2.1.0, Map Reduce2, Spark, Unix, Pig 0.12.0, Hive 0.13.0, Linux, Sqoop 1.4.2, Flume 1.3.1, Eclipse, AWS EC2, and Cloudera CDH 4.  \n\nAmerican Home Shield - Memphis, TN                                                \t\t         Dec\u201914 \u2013 Nov\u201915\nRole: Hadoop Developer\n\nResponsibilities:\n\nMigrated the needed data from MySQL into HDFS using Sqoop and importing various formats of flat files in to HDFS.\nMainly worked on Hive queries to categorize data of different claims.\nInvolved in loading data from LINUX file system to HDFS\nWritten customized Hive UDFs in Java where the functionality is too complex.\nImplemented Partitioning, Dynamic Partitions, Buckets in HIVE.\nDesigning and creating Hive external tables using shared meta-store instead of derby with partitioning, dynamic partitioning and buckets.\nGenerate final reporting data using Tableau for testing by connecting to the corresponding Hive tables using Hive ODBC connector.\nResponsible to manage the test data coming from different sources\nReviewing peer table creation in Hive, data loading and queries.\nWeekly meetings with technical collaborators and active participation in code review sessions with senior and junior developers.\nMonitored System health and logs and respond accordingly to any warning or failure conditions.\nGained experience in managing and reviewing Hadoop log files.\nInvolved in scheduling Oozie workflow engine to run multiple Hive and pig jobs\nInvolved unit testing, interface testing, system testing and user acceptance testing of the workflow tool.\nCreated and maintained Technical documentation for launching Hadoop Clusters and for executing Hive queries and Pig Scripts\n\nEnvironment: Apache Hadoop, HDFS, Hive, Map Reduce, Core Java, Pig, Sqoop, Cloudera CDH4, Oracle, MySQL.\n\nProtective Life - Edina, MN                                                                      \t\t         Oct\u201913 - Nov\u201914     \nRole: Java Developer\n\nResponsibilities:\n\nImplemented a Web based Application using Servlets, JSP, spring, JDBC, XML.\u00a0\nInvolved in writing Spring Configuration XML file that contains declarations and other   dependent objects declarations.\u00a0\nUsed hibernate to connect to Database to create the DAO layer.\u00a0\nDeveloped Application Framework using Model-View-Controller using the technology Spring.\u00a0\nUsed HTML, XHTML, XML, XSLT, XPATH, JSP and Tag Libraries to develop view pages\u00a0\nMultilayer Applications construction using Open JPA, HTML5, Spring MVC.\u00a0\nAnnotated Spring Architecture (Spring Beans)\u00a0\nImplemented UNIX shell scripts to migrate various data files to S&P ratings repository\u00a0\nImplemented smooth pagination capability using\u00a0JSP to remove existing pagination utility\u00a0\nWorked on Geo API to provide geological access capability to S&P.com site.\u00a0\nInvolved in Agile process to streamline development process with iterative development.\u00a0\nCode reviews and Managing the CVS Repository.\u00a0\nPrepare builds for DEV and UAT environments.\u00a0\nParticipating in the regular team meetings sprint planning meetings, user story review meetings etc.\u00a0\nInvolved in preparing High & low level design docs with UML diagrams using Microsoft VISIO tool.\u00a0\n\nEnvironment: JDK 1.5, XML, HTML, XHTML, JSP, Spring, DAO, Oracle Express edition, Apache ANT, CVS, Junit, UNIX, Log4J, CSS Style Sheets, Apache Tomcat, J2EE, Maven 3\n\nAccenture \u2013 USA\t\t       Oct\u201911\u2013 Sep\u201913\nRole: Java Developer\n\nResponsibilities:\n\nInvolved in Requirements analysis, design, and development and testing.\nInvolved in setting up the different roles & maintained authentication to the application.\nDesigned, deployed and tested Multi-tier application using the Java technologies.\nInvolved in front end development using JSP, HTML & CSS.\nImplemented the Application using Servlets\nDeployed the application on Oracle Web logic server\nImplemented Multithreading concepts in java classes to avoid deadlocking.\nUsed MySQL database to store data and execute SQL queries on the backend.\nPrepared and Maintained test environment.\nTested the application before going live to production.\nDocumented and communicated test result to the team lead on daily basis.\nInvolved in weekly meeting with team leads and manager to discuss the issues and status of the projects.\n\nEnvironment: J2EE (Java, JSP, JDBC, Multi-Threading), HTML, Oracle Web logic server, Eclipse, MySQL, JUnit.\n\nGolan Technologies \u2013 Newyork                                                \t\t          Jun\u201909 - Sep\u201911 \nRole: Java Developer\n\nGolan Technologies range from turnkey solutions to custom, client-driven solutions in a variety of product categories including website development and platform based applications, demand intelligence and business insight generation. Smart sites have the ability to provide a unified user experience and consistent messaging on websites across the globe, driving a favorable brand impression.\u00a0\n\nResponsibilities:\n\nInvolved in the analysis, design, implementation, and testing of the project.\nDeveloped UI using HTML, JavaScript, CSS and JSP for interactive cross browser functionality and complex user interface.\nImplemented the end-to-end functionality of the client requirement during the development phase.\nImplemented the functionality of mapping entities to the database using Hibernate.\nWritten SQL queries involved in the JDBC connection in accordance with the business logic.\nPerformed various levels of unit testing for the entire application using the test cases, which included preparation of detail documentation for the results.\nActively participated in client meetings and taking the inputs for the additional functionality.\nInvolved in fixing bugs and unit testing with test cases using JUnit.\n\nEnvironment: J2EE, Spring, Hibernate, JavaScript, CSS, Servlets, MySQL\n", "years_experience": 8, "score": 0.3533000426843384, "path": "C:\\Users\\admin\\resume\\AI-Resume-Ranker\\data\\uploads\\Candidate23_Hadoop_developer_1.docx"}, {"filename": "Candidate23_Hadoop_developer_1.docx", "text": "Candidate23\nSr. Hadoop Developer\nPhone: +1(224)-706-0020 \u00a0\u00a0\nEmail: candidate23.dbj@gmail.com \n------------------------------------------------------------------------------------------------------------------------------------------\nPROFESSIONAL SUMMARY:\nOver 8+ years of experience including 4 years of Big Data Ecosystem related technologies with full project development, implementation and deployment.\nStrong Experience working with various Hadoop ecosystem components like, Map Reduce, HDFS, Hive, Sqoop, Pig, Flume, and Oozie.\nStrong Knowledge on Architecture of Distributed systems and Parallel processing frameworks. \nIn-depth understanding of MapReduce Framework and Spark execution model.\nWorked extensively on fine-tuning long running Spark Applications to utilize better parallelism and executor memory for more caching.\nStrong experience working with both batch and real-time processing using Spark framework.\nExpertise in developing production ready Spark applications utilizing Spark-Core, Data frames, Spark-SQL, Spark-ML and Spark-Streaming API's.\nHands on experience in installing, configuring and deploying Hadoop distributions in cloud environments (Amazon Web Services).\nExpertise in developing production ready Spark applications utilizing Spark-Core, Data frames, Spark-SQL, Spark-ML and Spark-Streaming API's.\nWorked on building real time data workflows using Kafka, Spark streaming and HBase.\u00a0\nWorked extensively on Hive for building complex data analytical applications.\nVery good understanding of Partitions, bucketing concepts in Hive and designed both Managed and External tables in Hive to optimize performance.\nUsed custom serDes like Regex SerDe, JSON SerDe, CSV SerDe etc., in hive to handle multiple formats of data. \nHaving knowledge in Apache Ambari platform for securing, managing and monitoring Hadoop clusters.\nExperienced in Cluster coordination services through zookeeper.\nStrong experience using different columnar file formats like Avro, RCFile, ORC and Parquet formats.\nWorked with Sqoop to move (import/export) data from a relational database into Hadoop.\nExperience working with Hadoop clusters using Cloudera, Amazon EMR and Hortonworks distributions.\nExtensive experience in performing ETL on structured, semi-structured data using Pig Latin Scripts.\nDesigned and implemented Hive and Pig UDF's using Java for evaluation, filtering, loading and storing of data.\nExperienced in job workflow scheduling and monitoring tools like Oozie.\nWell versed with UNIX and Linux command line and shell script.\nAdequate knowledge and working experience with agile methodology.\n\nTECHNICAL SKILLS:\n\n\nEDUCATION:\nBachelor of Technology in Computer Science Engineering \n\nWORK EXPERIENCE:\n\nCigna \u2013 Bloomfield, Connecticut                                                             \t\t       Jul\u201917 \u2013 Present \nRole: Hadoop/Spark Developer \n\nResponsibilities:\nDeveloped Spark applications using Scala utilizing Data frames and Spark SQL API for faster processing of data.\nDeveloped highly optimized Spark applications to perform various data cleansing, validation, transformation and summarization activities according to the requirement\nData pipeline consists Spark, Hive and Sqoop and custom built Input Adapters to ingest, transform and analyze operational data.\nDeveloped Spark jobs and Hive Jobs to summarize and transform data.\nUsed Spark for interactive queries, processing of streaming data and integration with NoSQL database HBase, Cassandra for interactive access patterns.\nInvolved in converting Hive queries into Spark transformations using Spark Data Frames in Scala.\nAutomated creation and termination of AWS EMR clusters using AWS, java sdk.\nBuilt real time data pipelines by developing Kafka producers and spark streaming applications for consuming.\nIngested syslog messages to Kafka.\nWorked on Apache Airflow to schedule single and sometimes complex chains of tasks that depend on each other on regular intervals.\nHandled importing data from relational databases into HDFS using Sqoop and performing transformations using Hive and Spark.\nHaving knowledge in Apache Ambari platform for securing, managing and monitoring Hadoop clusters.\nExported the processed data to the relational databases using Sqoop, to further visualize and generate reports for the BI team.\nExperienced in cluster coordination services through Zookeeper.\nInstalled, tested and deployed monitoring solutions with Splunk services.\nUsed Hive to analyze the partitioned and bucketed data and computed various metrics for reporting.\nDeveloped Hive scripts in Hive QL to de-normalize and aggregate the data.\nScheduled and executed workflows in Oozie to run various jobs.\nDesigning & creating ETL jobs through Talend to load huge volumes of data into Cassandra, Hadoop Ecosystem and relational databases.\n\nEnvironment: Hadoop, Spark, Hive, Java, Scala, Maven, Impala, Oozie, Oracle, Ambari, GitHub, Tableau, Unix, Hortonworks, Apache Airflow Kafka, Zookeeper, Sqoop, Cassandra, Talend, Splunk, HBase.\n\nQualcomm -- San Diego, CA                                                                   \t\t          Dec\u201916 \u2013 Jun\u201917                                                                                            \nRole: Hadoop/Spark Developer\n\nResponsibilities:\n\nPart of Big Data Center of Excellence (CoE), responsible for designing and building enterprise data analytics platform.\nWorked with respective business units in understanding the scope of the analytics requirements.\nPerformed core ETL transformations in Spark.\nAutomated data pipelines which involve data ingestion, data cleansing, data preparation and data analytics.\nCreated end to end Spark applications using Scala to perform various data cleansing, validation, transformation and summarization activities on user behavioral data.\nDeveloped end-to-end data pipeline using FTP Adaptor, Spark, Hive and Impala.\nImplemented Spark utilizing Spark-SQL heavily for faster development, and processing of data.\u00a0 \nExploring with Spark for improving the performance and optimization of the existing jobs in Hadoop using Spark-SQL, Data Frame running in Yarn mode.\nHandled importing other enterprise data from different data sources into HDFS using Sqoop and performing transformations using Hive, Map Reduce and then loading data into HBase tables.\nCollecting and aggregating large amounts of log data using Flume and staging data in HDFS for further analysis\nWrapper developed in Python for instantiating multithreaded application and running with other applications.\nAnalyzed the data by performing Hive queries (Hive QL) and running Pig scripts (Pig Latin) to study customer behavior.\nData warehousing, experience in design, development and testing, implementation and support of enterprise data warehouse.\nUsed Hive to analyze the partitioned and bucketed data and compute various metrics for reporting.\nCreated components like Hive UDFs for missing functionality in HIVE for analytics.\nWorked on various performance optimizations like using distributed cache for small datasets, Partition,\u00a0Bucketing in Hive and Map Side joins.\nCreated Oozie workflows and coordinators to automate data pipelines daily, weekly and monthly.\nAutomated creation and termination of AWS EMR clusters using AWS, java sdk.\n\nEnvironment: AWS EMR, Hadoop, Spark, Hive, Sqoop, HBase, UNIX, Talend, Pig, Linux, Java, Scala, Python, Ambari, Zookeeper.\nHortonworks\n\nMcKesson - Alpharetta, GA                                                                   \t\t        Dec\u201915 \u2013 Nov\u201916\nHadoop/Spark Developer\n\nResponsibilities:\n\nDeveloped multithreaded Java based Input adaptors for ingesting click stream data from external sources like ftp server and S3 buckets on daily basis.\nCreated various spark applications using Scala to perform various enrichment of these click stream data combined with enterprise data of the users.\nImplemented batch processing of jobs using Spark Scala API.\nDeveloped Sqoop scripts to import/export data from Oracle to HDFS and into Hive tables.\u00a0\nStored the data in columnar formats using Hive.\nInvolved building and managing NoSQL Database models using HBase.\nWorked in Spark to read the data from Hive and write it to Hbase.\nOptimized the Hive tables using optimization techniques like partitions and bucketing to provide better performance with Hive QL queries.\u00a0\nWorked with multiple file formats like Avro, Sequence, Parquet and Orc.\nConverted existing MapReduce programs to Spark Applications for handling semi structured data like JSON files, Apache Log files, and other custom log data.\nLoaded the final processed data to HBase tables to allow downstream application team to build rich and data driven applications.\nWorked with a team to improve the performance and optimization of the existing algorithms in Hadoop using Spark, Spark -SQL, Data Frame.\nImplemented business logic in Hive and written UDF\u2019s to process the data for analysis.\nUsed Oozie to define a workflow to coordinate the execution of Spark, Hive and Sqoop jobs.\nAddressing the issues occurring due to the huge volume of data and transitions.\nDesigned, documented operational problems by following standards and procedures using JIRA. \n\nEnvironment: Java, Hadoop 2.1.0, Map Reduce2, Spark, Unix, Pig 0.12.0, Hive 0.13.0, Linux, Sqoop 1.4.2, Flume 1.3.1, Eclipse, AWS EC2, and Cloudera CDH 4.  \n\nAmerican Home Shield - Memphis, TN                                                \t\t         Dec\u201914 \u2013 Nov\u201915\nRole: Hadoop Developer\n\nResponsibilities:\n\nMigrated the needed data from MySQL into HDFS using Sqoop and importing various formats of flat files in to HDFS.\nMainly worked on Hive queries to categorize data of different claims.\nInvolved in loading data from LINUX file system to HDFS\nWritten customized Hive UDFs in Java where the functionality is too complex.\nImplemented Partitioning, Dynamic Partitions, Buckets in HIVE.\nDesigning and creating Hive external tables using shared meta-store instead of derby with partitioning, dynamic partitioning and buckets.\nGenerate final reporting data using Tableau for testing by connecting to the corresponding Hive tables using Hive ODBC connector.\nResponsible to manage the test data coming from different sources\nReviewing peer table creation in Hive, data loading and queries.\nWeekly meetings with technical collaborators and active participation in code review sessions with senior and junior developers.\nMonitored System health and logs and respond accordingly to any warning or failure conditions.\nGained experience in managing and reviewing Hadoop log files.\nInvolved in scheduling Oozie workflow engine to run multiple Hive and pig jobs\nInvolved unit testing, interface testing, system testing and user acceptance testing of the workflow tool.\nCreated and maintained Technical documentation for launching Hadoop Clusters and for executing Hive queries and Pig Scripts\n\nEnvironment: Apache Hadoop, HDFS, Hive, Map Reduce, Core Java, Pig, Sqoop, Cloudera CDH4, Oracle, MySQL.\n\nProtective Life - Edina, MN                                                                      \t\t         Oct\u201913 - Nov\u201914     \nRole: Java Developer\n\nResponsibilities:\n\nImplemented a Web based Application using Servlets, JSP, spring, JDBC, XML.\u00a0\nInvolved in writing Spring Configuration XML file that contains declarations and other   dependent objects declarations.\u00a0\nUsed hibernate to connect to Database to create the DAO layer.\u00a0\nDeveloped Application Framework using Model-View-Controller using the technology Spring.\u00a0\nUsed HTML, XHTML, XML, XSLT, XPATH, JSP and Tag Libraries to develop view pages\u00a0\nMultilayer Applications construction using Open JPA, HTML5, Spring MVC.\u00a0\nAnnotated Spring Architecture (Spring Beans)\u00a0\nImplemented UNIX shell scripts to migrate various data files to S&P ratings repository\u00a0\nImplemented smooth pagination capability using\u00a0JSP to remove existing pagination utility\u00a0\nWorked on Geo API to provide geological access capability to S&P.com site.\u00a0\nInvolved in Agile process to streamline development process with iterative development.\u00a0\nCode reviews and Managing the CVS Repository.\u00a0\nPrepare builds for DEV and UAT environments.\u00a0\nParticipating in the regular team meetings sprint planning meetings, user story review meetings etc.\u00a0\nInvolved in preparing High & low level design docs with UML diagrams using Microsoft VISIO tool.\u00a0\n\nEnvironment: JDK 1.5, XML, HTML, XHTML, JSP, Spring, DAO, Oracle Express edition, Apache ANT, CVS, Junit, UNIX, Log4J, CSS Style Sheets, Apache Tomcat, J2EE, Maven 3\n\nAccenture \u2013 USA\t\t       Oct\u201911\u2013 Sep\u201913\nRole: Java Developer\n\nResponsibilities:\n\nInvolved in Requirements analysis, design, and development and testing.\nInvolved in setting up the different roles & maintained authentication to the application.\nDesigned, deployed and tested Multi-tier application using the Java technologies.\nInvolved in front end development using JSP, HTML & CSS.\nImplemented the Application using Servlets\nDeployed the application on Oracle Web logic server\nImplemented Multithreading concepts in java classes to avoid deadlocking.\nUsed MySQL database to store data and execute SQL queries on the backend.\nPrepared and Maintained test environment.\nTested the application before going live to production.\nDocumented and communicated test result to the team lead on daily basis.\nInvolved in weekly meeting with team leads and manager to discuss the issues and status of the projects.\n\nEnvironment: J2EE (Java, JSP, JDBC, Multi-Threading), HTML, Oracle Web logic server, Eclipse, MySQL, JUnit.\n\nGolan Technologies \u2013 Newyork                                                \t\t          Jun\u201909 - Sep\u201911 \nRole: Java Developer\n\nGolan Technologies range from turnkey solutions to custom, client-driven solutions in a variety of product categories including website development and platform based applications, demand intelligence and business insight generation. Smart sites have the ability to provide a unified user experience and consistent messaging on websites across the globe, driving a favorable brand impression.\u00a0\n\nResponsibilities:\n\nInvolved in the analysis, design, implementation, and testing of the project.\nDeveloped UI using HTML, JavaScript, CSS and JSP for interactive cross browser functionality and complex user interface.\nImplemented the end-to-end functionality of the client requirement during the development phase.\nImplemented the functionality of mapping entities to the database using Hibernate.\nWritten SQL queries involved in the JDBC connection in accordance with the business logic.\nPerformed various levels of unit testing for the entire application using the test cases, which included preparation of detail documentation for the results.\nActively participated in client meetings and taking the inputs for the additional functionality.\nInvolved in fixing bugs and unit testing with test cases using JUnit.\n\nEnvironment: J2EE, Spring, Hibernate, JavaScript, CSS, Servlets, MySQL\n", "years_experience": 8, "score": 0.3533000426843384, "path": "C:\\Users\\admin\\resume\\AI-Resume-Ranker\\data\\uploads\\Candidate23_Hadoop_developer_1.docx"}, {"filename": "Candidate23_Hadoop_developer.docx", "text": "Candidate23\nSr. Hadoop Developer\nPhone: +1(224)-706-0020 \u00a0\u00a0\nEmail: candidate23.dbj@gmail.com \n------------------------------------------------------------------------------------------------------------------------------------------\nPROFESSIONAL SUMMARY:\nOver 8+ years of experience including 4 years of Big Data Ecosystem related technologies with full project development, implementation and deployment.\nStrong Experience working with various Hadoop ecosystem components like, Map Reduce, HDFS, Hive, Sqoop, Pig, Flume, and Oozie.\nStrong Knowledge on Architecture of Distributed systems and Parallel processing frameworks. \nIn-depth understanding of MapReduce Framework and Spark execution model.\nWorked extensively on fine-tuning long running Spark Applications to utilize better parallelism and executor memory for more caching.\nStrong experience working with both batch and real-time processing using Spark framework.\nExpertise in developing production ready Spark applications utilizing Spark-Core, Data frames, Spark-SQL, Spark-ML and Spark-Streaming API's.\nHands on experience in installing, configuring and deploying Hadoop distributions in cloud environments (Amazon Web Services).\nExpertise in developing production ready Spark applications utilizing Spark-Core, Data frames, Spark-SQL, Spark-ML and Spark-Streaming API's.\nWorked on building real time data workflows using Kafka, Spark streaming and HBase.\u00a0\nWorked extensively on Hive for building complex data analytical applications.\nVery good understanding of Partitions, bucketing concepts in Hive and designed both Managed and External tables in Hive to optimize performance.\nUsed custom serDes like Regex SerDe, JSON SerDe, CSV SerDe etc., in hive to handle multiple formats of data. \nHaving knowledge in Apache Ambari platform for securing, managing and monitoring Hadoop clusters.\nExperienced in Cluster coordination services through zookeeper.\nStrong experience using different columnar file formats like Avro, RCFile, ORC and Parquet formats.\nWorked with Sqoop to move (import/export) data from a relational database into Hadoop.\nExperience working with Hadoop clusters using Cloudera, Amazon EMR and Hortonworks distributions.\nExtensive experience in performing ETL on structured, semi-structured data using Pig Latin Scripts.\nDesigned and implemented Hive and Pig UDF's using Java for evaluation, filtering, loading and storing of data.\nExperienced in job workflow scheduling and monitoring tools like Oozie.\nWell versed with UNIX and Linux command line and shell script.\nAdequate knowledge and working experience with agile methodology.\n\nTECHNICAL SKILLS:\n\n\nEDUCATION:\nBachelor of Technology in Computer Science Engineering \n\nWORK EXPERIENCE:\n\nCigna \u2013 Bloomfield, Connecticut                                                             \t\t       Jul\u201917 \u2013 Present \nRole: Hadoop/Spark Developer \n\nResponsibilities:\nDeveloped Spark applications using Scala utilizing Data frames and Spark SQL API for faster processing of data.\nDeveloped highly optimized Spark applications to perform various data cleansing, validation, transformation and summarization activities according to the requirement\nData pipeline consists Spark, Hive and Sqoop and custom built Input Adapters to ingest, transform and analyze operational data.\nDeveloped Spark jobs and Hive Jobs to summarize and transform data.\nUsed Spark for interactive queries, processing of streaming data and integration with NoSQL database HBase, Cassandra for interactive access patterns.\nInvolved in converting Hive queries into Spark transformations using Spark Data Frames in Scala.\nAutomated creation and termination of AWS EMR clusters using AWS, java sdk.\nBuilt real time data pipelines by developing Kafka producers and spark streaming applications for consuming.\nIngested syslog messages to Kafka.\nWorked on Apache Airflow to schedule single and sometimes complex chains of tasks that depend on each other on regular intervals.\nHandled importing data from relational databases into HDFS using Sqoop and performing transformations using Hive and Spark.\nHaving knowledge in Apache Ambari platform for securing, managing and monitoring Hadoop clusters.\nExported the processed data to the relational databases using Sqoop, to further visualize and generate reports for the BI team.\nExperienced in cluster coordination services through Zookeeper.\nInstalled, tested and deployed monitoring solutions with Splunk services.\nUsed Hive to analyze the partitioned and bucketed data and computed various metrics for reporting.\nDeveloped Hive scripts in Hive QL to de-normalize and aggregate the data.\nScheduled and executed workflows in Oozie to run various jobs.\nDesigning & creating ETL jobs through Talend to load huge volumes of data into Cassandra, Hadoop Ecosystem and relational databases.\n\nEnvironment: Hadoop, Spark, Hive, Java, Scala, Maven, Impala, Oozie, Oracle, Ambari, GitHub, Tableau, Unix, Hortonworks, Apache Airflow Kafka, Zookeeper, Sqoop, Cassandra, Talend, Splunk, HBase.\n\nQualcomm -- San Diego, CA                                                                   \t\t          Dec\u201916 \u2013 Jun\u201917                                                                                            \nRole: Hadoop/Spark Developer\n\nResponsibilities:\n\nPart of Big Data Center of Excellence (CoE), responsible for designing and building enterprise data analytics platform.\nWorked with respective business units in understanding the scope of the analytics requirements.\nPerformed core ETL transformations in Spark.\nAutomated data pipelines which involve data ingestion, data cleansing, data preparation and data analytics.\nCreated end to end Spark applications using Scala to perform various data cleansing, validation, transformation and summarization activities on user behavioral data.\nDeveloped end-to-end data pipeline using FTP Adaptor, Spark, Hive and Impala.\nImplemented Spark utilizing Spark-SQL heavily for faster development, and processing of data.\u00a0 \nExploring with Spark for improving the performance and optimization of the existing jobs in Hadoop using Spark-SQL, Data Frame running in Yarn mode.\nHandled importing other enterprise data from different data sources into HDFS using Sqoop and performing transformations using Hive, Map Reduce and then loading data into HBase tables.\nCollecting and aggregating large amounts of log data using Flume and staging data in HDFS for further analysis\nWrapper developed in Python for instantiating multithreaded application and running with other applications.\nAnalyzed the data by performing Hive queries (Hive QL) and running Pig scripts (Pig Latin) to study customer behavior.\nData warehousing, experience in design, development and testing, implementation and support of enterprise data warehouse.\nUsed Hive to analyze the partitioned and bucketed data and compute various metrics for reporting.\nCreated components like Hive UDFs for missing functionality in HIVE for analytics.\nWorked on various performance optimizations like using distributed cache for small datasets, Partition,\u00a0Bucketing in Hive and Map Side joins.\nCreated Oozie workflows and coordinators to automate data pipelines daily, weekly and monthly.\nAutomated creation and termination of AWS EMR clusters using AWS, java sdk.\n\nEnvironment: AWS EMR, Hadoop, Spark, Hive, Sqoop, HBase, UNIX, Talend, Pig, Linux, Java, Scala, Python, Ambari, Zookeeper.\nHortonworks\n\nMcKesson - Alpharetta, GA                                                                   \t\t        Dec\u201915 \u2013 Nov\u201916\nHadoop/Spark Developer\n\nResponsibilities:\n\nDeveloped multithreaded Java based Input adaptors for ingesting click stream data from external sources like ftp server and S3 buckets on daily basis.\nCreated various spark applications using Scala to perform various enrichment of these click stream data combined with enterprise data of the users.\nImplemented batch processing of jobs using Spark Scala API.\nDeveloped Sqoop scripts to import/export data from Oracle to HDFS and into Hive tables.\u00a0\nStored the data in columnar formats using Hive.\nInvolved building and managing NoSQL Database models using HBase.\nWorked in Spark to read the data from Hive and write it to Hbase.\nOptimized the Hive tables using optimization techniques like partitions and bucketing to provide better performance with Hive QL queries.\u00a0\nWorked with multiple file formats like Avro, Sequence, Parquet and Orc.\nConverted existing MapReduce programs to Spark Applications for handling semi structured data like JSON files, Apache Log files, and other custom log data.\nLoaded the final processed data to HBase tables to allow downstream application team to build rich and data driven applications.\nWorked with a team to improve the performance and optimization of the existing algorithms in Hadoop using Spark, Spark -SQL, Data Frame.\nImplemented business logic in Hive and written UDF\u2019s to process the data for analysis.\nUsed Oozie to define a workflow to coordinate the execution of Spark, Hive and Sqoop jobs.\nAddressing the issues occurring due to the huge volume of data and transitions.\nDesigned, documented operational problems by following standards and procedures using JIRA. \n\nEnvironment: Java, Hadoop 2.1.0, Map Reduce2, Spark, Unix, Pig 0.12.0, Hive 0.13.0, Linux, Sqoop 1.4.2, Flume 1.3.1, Eclipse, AWS EC2, and Cloudera CDH 4.  \n\nAmerican Home Shield - Memphis, TN                                                \t\t         Dec\u201914 \u2013 Nov\u201915\nRole: Hadoop Developer\n\nResponsibilities:\n\nMigrated the needed data from MySQL into HDFS using Sqoop and importing various formats of flat files in to HDFS.\nMainly worked on Hive queries to categorize data of different claims.\nInvolved in loading data from LINUX file system to HDFS\nWritten customized Hive UDFs in Java where the functionality is too complex.\nImplemented Partitioning, Dynamic Partitions, Buckets in HIVE.\nDesigning and creating Hive external tables using shared meta-store instead of derby with partitioning, dynamic partitioning and buckets.\nGenerate final reporting data using Tableau for testing by connecting to the corresponding Hive tables using Hive ODBC connector.\nResponsible to manage the test data coming from different sources\nReviewing peer table creation in Hive, data loading and queries.\nWeekly meetings with technical collaborators and active participation in code review sessions with senior and junior developers.\nMonitored System health and logs and respond accordingly to any warning or failure conditions.\nGained experience in managing and reviewing Hadoop log files.\nInvolved in scheduling Oozie workflow engine to run multiple Hive and pig jobs\nInvolved unit testing, interface testing, system testing and user acceptance testing of the workflow tool.\nCreated and maintained Technical documentation for launching Hadoop Clusters and for executing Hive queries and Pig Scripts\n\nEnvironment: Apache Hadoop, HDFS, Hive, Map Reduce, Core Java, Pig, Sqoop, Cloudera CDH4, Oracle, MySQL.\n\nProtective Life - Edina, MN                                                                      \t\t         Oct\u201913 - Nov\u201914     \nRole: Java Developer\n\nResponsibilities:\n\nImplemented a Web based Application using Servlets, JSP, spring, JDBC, XML.\u00a0\nInvolved in writing Spring Configuration XML file that contains declarations and other   dependent objects declarations.\u00a0\nUsed hibernate to connect to Database to create the DAO layer.\u00a0\nDeveloped Application Framework using Model-View-Controller using the technology Spring.\u00a0\nUsed HTML, XHTML, XML, XSLT, XPATH, JSP and Tag Libraries to develop view pages\u00a0\nMultilayer Applications construction using Open JPA, HTML5, Spring MVC.\u00a0\nAnnotated Spring Architecture (Spring Beans)\u00a0\nImplemented UNIX shell scripts to migrate various data files to S&P ratings repository\u00a0\nImplemented smooth pagination capability using\u00a0JSP to remove existing pagination utility\u00a0\nWorked on Geo API to provide geological access capability to S&P.com site.\u00a0\nInvolved in Agile process to streamline development process with iterative development.\u00a0\nCode reviews and Managing the CVS Repository.\u00a0\nPrepare builds for DEV and UAT environments.\u00a0\nParticipating in the regular team meetings sprint planning meetings, user story review meetings etc.\u00a0\nInvolved in preparing High & low level design docs with UML diagrams using Microsoft VISIO tool.\u00a0\n\nEnvironment: JDK 1.5, XML, HTML, XHTML, JSP, Spring, DAO, Oracle Express edition, Apache ANT, CVS, Junit, UNIX, Log4J, CSS Style Sheets, Apache Tomcat, J2EE, Maven 3\n\nAccenture \u2013 USA\t\t       Oct\u201911\u2013 Sep\u201913\nRole: Java Developer\n\nResponsibilities:\n\nInvolved in Requirements analysis, design, and development and testing.\nInvolved in setting up the different roles & maintained authentication to the application.\nDesigned, deployed and tested Multi-tier application using the Java technologies.\nInvolved in front end development using JSP, HTML & CSS.\nImplemented the Application using Servlets\nDeployed the application on Oracle Web logic server\nImplemented Multithreading concepts in java classes to avoid deadlocking.\nUsed MySQL database to store data and execute SQL queries on the backend.\nPrepared and Maintained test environment.\nTested the application before going live to production.\nDocumented and communicated test result to the team lead on daily basis.\nInvolved in weekly meeting with team leads and manager to discuss the issues and status of the projects.\n\nEnvironment: J2EE (Java, JSP, JDBC, Multi-Threading), HTML, Oracle Web logic server, Eclipse, MySQL, JUnit.\n\nGolan Technologies \u2013 Newyork                                                \t\t          Jun\u201909 - Sep\u201911 \nRole: Java Developer\n\nGolan Technologies range from turnkey solutions to custom, client-driven solutions in a variety of product categories including website development and platform based applications, demand intelligence and business insight generation. Smart sites have the ability to provide a unified user experience and consistent messaging on websites across the globe, driving a favorable brand impression.\u00a0\n\nResponsibilities:\n\nInvolved in the analysis, design, implementation, and testing of the project.\nDeveloped UI using HTML, JavaScript, CSS and JSP for interactive cross browser functionality and complex user interface.\nImplemented the end-to-end functionality of the client requirement during the development phase.\nImplemented the functionality of mapping entities to the database using Hibernate.\nWritten SQL queries involved in the JDBC connection in accordance with the business logic.\nPerformed various levels of unit testing for the entire application using the test cases, which included preparation of detail documentation for the results.\nActively participated in client meetings and taking the inputs for the additional functionality.\nInvolved in fixing bugs and unit testing with test cases using JUnit.\n\nEnvironment: J2EE, Spring, Hibernate, JavaScript, CSS, Servlets, MySQL\n", "years_experience": 8, "score": 0.3533000426843384, "path": "C:\\Users\\admin\\resume\\AI-Resume-Ranker\\data\\uploads\\Candidate23_Hadoop_developer.docx"}, {"filename": "Candidate66_resume.docx", "text": "Candidate66\nPhone: 201-532-6397. Email: Candidate66@gmail.com\n__________________________________________________________________________________________________\nProfessional Summary\nOverall 10 years of experience as Sr. Technical Business System Analyst (Technical BSA) in Information Technology industry with focus on Data Analysis, Information & Data Management, Service-Oriented Architecture (SOA), Data Mapping & Data Modeling, Business Process Improvement, System Analysis & Design; expertise in implementation of IT projects using Project Management methodologies. Extensive experience in diversified industry sectors including Healthcare, Life Insurance, Personal & Commercial Insurance, Property & Casualty Insurance, Telecommunication, Retail & e-Commerce (Web Merchandising), Commercial & Investment Banking and Credit Card services.\nStrong skills in Data Warehouse Design using Star Schema roll out for the fact and dimension tables. Design of cubes, partitions and aggregations with excellent knowledge of Type 2 Data modeling.\nExpertise in application development with Full Lifecycle implementation of large Data Warehouse including Project Scope, requirements gathering, Star Schema Design, Snowflake Schema, Data Modeling and ODS.\nStrong experience in Data Analysis, Data Migration, Data Cleansing, Transformation, Integration, Data Import, and Data Export through the use of multiple ETL tools such as Informatica PowerCenter.\nStrong practical knowledge of analytical techniques and methodologies such as machine learning/supervised and unsupervised techniques, segmentation, mix and time series modeling, response modeling, lift modeling, experimental design, neural networks, data mining and optimization techniques. Expert knowledge of statistical analysis tools such as R-programming, MATLAB, Spark, SAS (Statistical Analysis System), R/Spark, Apache Hadoop and Cassandra.\nStrong background in applying statistical machine learning techniques to predictive modeling and experience with Machine Learning libraries (via R, H2O, Python, Spark, etc.). Proficiency in programming in Python, R-programming, SQL, JavaScript, Java/Scala/Ruby and shell scripting. Proficiency in consuming REST based API (with JSON payload). Fluency in big data platforms including Apache Hadoop, Apache MapReduce, Hive, Spark, and Pig. Familiarity with Cloud based HaaS/PaaS solutions such as AWS EMR (Amazon Web Services Elastic MapReduce), MS Azure. Strong understanding of data profiling and data cleansing techniques.\nThorough knowledge of eCommerce architecture; ATG framework its functional limitations and the platform features, application capabilities, etc. including industry wide terminologies like B2B, B2C, C2B, C2C with its eCommerce categories and strong knowledge of PCI DSS (Payment Card Industry Data Security Standards) standards.\nTechnical expertise in handling large-scale platform architecture, integrations, development using Java/J2EE Technologies. Experience on Oracle Commerce includes ATG Commerce, ATG Multisite, ATG Merchandising, and Content Management working in some of the key areas such as the Business Control Center (BCC), Profile Management, Shopping Cart, Checkout, Pricing, performance tuning, caching, infrastructure setup and Endeca.\u00a0\nStrong knowledge of application and systems performance improvement techniques, integration technologies, approaches and patterns, Web APIs / Web Service API calls (SoapUI / SOAP, RESTful API, SOA, XML, HTTP like GET / PUT / POST / DELETE, JSON, BitTorrent, Web 2.0, etc.). Strong knowledge of security aspects like risks and threats, authentication, authorization, certificates, encryption. Experience in utilizing frameworks and re-usable components.\nProficient in using Business Analysis tools such as MS Visio, Word, Excel (including Pivot tables, Macros and Vlookup), Rational Requiste Pro, ReqTrace Web, Caliber RM, Optimal Trace, ClearCase and ClearQuest.\nProficiency in elaborating the Use cases, writing Test cases and preparing Requirement Traceability Matrices (RTM).\nEducation: Master of Science in Engineering \u2013 New Jersey Institute of Technology (August 2008 to December 2009).\nCertification: Certified Data Scientist i.e. Data Science Professional (from John Hopkins University).\nTechnical Skills: Windows 95/98/NT/2000/XP/Vista/7, MS Visio, MS Access, MS Word, MS Excel, MS Project, Rational Rose Enterprise, IBM Rational Software Architect, Microsoft Project, Rational Requisite Pro, MS SQL Server 7.0/2000, PL/SQL, CSS (Cascading Style Sheet), OLTP & OLAP, Synon/2E, COBOL, IBM AS/400, CICS, Citrix, DB2, SAP ERP R/3 MMWM, SAP R/3 FICO, ERWIN Data Modeler 4.2/4.1/4.0, Business Objects XI r3, r2, r1, 6.5, 6.1, 5.x, 4.x, Crystal Reports XI, Informatica, HTML, Dream Weaver, MS FrontPage, IBM COGNOS (Report Studio, Query Studio, Framework Manager, Virtual View Manager, Business Insight Dashboard), Oracle BI Enterprise Edition (OBIEE), C#, C++, R Programming Language (Version 3.2.5), Minitab, XML, XSD, WSDL, X12, SAP PowerDesigner, SAP PowerBuilder, Analysis of Variance (ANOVA) and Experimental Design, Total Quality Management and Lean Six Sigma Manufacturing Methodologies, SAFe (Scaled Agile Framework) Practitioner, PMI\u2019s PMBOK guidelines & standards, TOGAF (The Open Group Architecture Framework), IIBA\u2019s BABOK guidelines & standards, Business Architecture Guild\u2019s BIZBOK (Business Architecture Body of Knowledge) standards & guidelines, Enterprise Architecture Body of Knowledge (EABOK), Enterprise Information Architecture (EIA) framework.\n\nProfessional Experience\nFidelity Investments (FMR) Smithfield, RI                               \t\t\t                                     Dec 2016 \u2013 Current\nDesignation: Principal Program Analyst / Technical Project Architect\nPrimary objective of the project was to address and resolve corporate Audit issue (Tier C) finding for the Enterprise Cybersecurity group on highly confidential Client Credentials (comprised of PII data) exposed into the server logs written by various Internal (employees within each business units, Broker \u2013 Dealer apps, 3rd party vendor apps) & External (customer / clients) Fidelity hosted applications, thereby applying log masking logic, data redaction techniques on these exposed keys within server logs physically located on multiple servers.\nDelivered elite level security solutions (for its Enterprise Cybersecurity business group) which essentially protects Fidelity and its customer's valuable information. Interacted with stakeholders, facilitated multiple fusion sessions and delivered the end results of work that provide the necessary requirements to the delivery team which thereby built and supported technology solutions.\nWorked as part of the Customer Protection Program, directly with Program and Project Managers as well as technology and business partners.\nWorked with tools such as Jira and Confluence to provide transparency of scope. Worked with our delivery team(s) including QA Data Services and engineering to ensure that we are meeting the needs of our stakeholders plus delivered solid, quality products to production. \nSupported Program and Project managers including providing input to/creation of strategy and execution plans and input to project status. Point person to escalate risks, change requests and participated in corrective actions (path to green).\nPerformed server logs analysis on Linux and Unix servers used by enterprise wide business units, thereby leveraged Splunk GUI tool to read and extrapolate pertinent meta data from these server logs. Used tools like Jira, ITEC PPMC, Confluence, etc. to track, monitor, delegate tasks / activities, operate different project aspects and functions.\nDerived insights out of ambiguity \u2013 understood, processed and interpreted complex data sets. Analyzed complex business data & identified patterns in the data using algorithms from statistics and machine learning. Developed and improved algorithms and methodologies to handle, browse, process and visualize structured and unstructured data.\nTranslated Business requirements, business rules and business criteria to system specifications and system requirements thereby preparing SRS (System Requirement Specifications) and PRS (Performance Requirement Specifications).\nEnvironment: MS Office Suite Package, MS Visio 2013 Pro, MS Excel Pivot Tables, MS Excel Functions & Formulas, MS Excel VLookUp, MS Excel Macros, Splunk Enterprise 6.5 (Splunk GUI Tool), Oracle 11g, IBM DB2, CyberArk EIAM (Enterprise Identity & Access Management), IBM InfoSphere Optim Data Privacy, ServiceNow, VMWare Tools, Oracle 11g Client 11.2.0.2 r01.02, Quest TOAD for Oracle, MS Access, Informatica Power Center, MS SQL Server, ITEC (IT Enablement Center) ALM (Application LifeCycle Management), Atlassian Confluence, ITEC (IT Enablement Center) Atlassian JIRA, Document Central, IBM DB2 Advanced Recovery Expert, IBM DB2 Merge BackUp, HP ITEC (IT Enablement Center) PPMC (Project & Portfolio Management Center).\n\nVerizon Wireless, Warren, NJ                               \t  \t\t\t\t\t\t    May 2016 \u2013 Dec 2016\nDesignation: Sr. Technical Business System Analyst / Technical Project Manager\nThe prime objective of the IoT (Internet Of Things), ReSale, M2M (Machine to Machine) portfolio was to utilize the core capabilities within MCS (Mobile Content Solutions) application suite module in order to web merchandise the non-tangible (like Licenses and Professional Services components within a cart) Items / SKUs through frontend Omni-channels; thereby foster & grow B2B (Business To Business), B2C (Business To Customer) business models from enterprise\u2019s strategic viewpoint and generate billing events at Account / Customer level from backend system application.\nAuthored, owned, managed and maintained 1Pagers to give brief overview of the products / services to be created and maintained in our MCS / SCM portal. Facilitated meeting sessions with Product Managers, Product Owners, Architectures and Analyst to hash out visio IoT (Internet of Things) Vendor flows, Order Fulfillment Flows and Order Processing Flows.\nPlanned, created, designed, managed and baselined draft high-level Project Scope, In-Scope & Out-Scope Functionalities, Draft SLAs (for 3rd party vendors / partners), Baseline Project Timelines & Gantt Chart at project implementation level with external 3rd party vendors / partners (in order to accomplish System Integration Testing [SIT] milestone) system application.\nDefined, designed and elicited core business objectives in order to deliver qualitative product, program and project with optimum utilization and the most efficient use of enterprise resources.\nManaged project charter, project plans, scope statement, project execution, risk mitigation, contingency plan and achievement of project and program objectives. Managed SIT & UAT test activities by ensuring defects are logged accurately in JIRA and issues resolved appropriately.\nCreated, aligned and logged EPICS, User Stories / Features, Tasks (Development, Infrastructure & QA Tasks), Bugs, Defects, BRs (Build Requests), DRs (Deployment Requests), Issues, etc. in JIRA tool.\nStrong knowledge of Unix commands and running those Unix commands in various boxes to validate server logs.\nEffectively and lucidly communicated project risks, issues (especially launch gating), dependencies, progress status, launches, concerns and retrospectives in structured RAID template designed and choreographed by me.\nEnvironment: Activiti BPM, XML Spy, RestFUL API, SoapUI, HP Quality Center ALM (Application Lifecycle Management) 12.20 & 12.01, Quest Toad For Oracle 12.0 (64bit), Programmer\u2019s Notepad, PuTTY.exe, JIRA, CA\u2019s Clarity PPM, Cloudsite SharePoint, Confluence document repository, APEX Search Client RealTime application, DB2 AppDevClient 9.7, MS Office Project Standard 2007, MS Office Standard 2007, MS Office Visio Standard, Open Source XML tools, Oracle Client 11g R2 11.2.0.1.0, Techsmith\u2019s SnagIT Editor 12, SQL Developer 4.1.3.\n\nChubb Insurance / Chubb Group of Insurance Companies, Whitehouse Station, NJ                            July 2015 \u2013 May 2016\nDesignation: Sr. Technical Business System Analyst / Project Architect\nThe purpose of the Client Integration initiative is to be able to view and manage PRS\u2019 business at the client / household / account level in an integrated fashion so that we can make better and faster underwriting decisions, more effectively market and communicate with existing and potential clients, price across clients and deliver better customer service. Integrate PLS & ORCA into the Chubb Client MDM/UI in order to provide an integrated household policy portfolio view across key Chubb Policy Admin Systems for Personal Lines segment, along with the other legacy Chubb\u2019s disparate policy admin systems like MasterPiece, CAAS & Yacht.\nCreated and documented roadmap for meetings with ACE. Baselined documents used for source to target mapping. Understanding CHUBB's system and applications i.e. Target system for CLIENT INT project.\nDesigned, developed and maintained consistency of the Pre-landing and Landing tables \u201cGeneric Landing Layout\u201d file / sheet that the core execution team leveraged for Source To Target Mapping sessions. \nCreated and documented High Level Data Movement and Data Flow diagrams, High Level & Detailed Conceptual Architecture diagrams using BPMN (Business Process Model and Notation) standards encapsulating every system components within visio flows. Conducted thorough analysis on CPI data models for Subject Of Insurance, Policy, Policy Term, Business Party schemas.\nConfirmed on the mapping performed within \u201cORCA Policy Status Mapping\u201d sheet for \u201cTransaction Status\u201d type and \u201cContract Status\u201d type. Confirmed the data points in \u201cMatch Columns\u201d column for the \u201cMatch Rules\u201d from \u201cClient INT ORCA Match Rules\u201d sheet. Gave an overview on \u201cSystem Triggers\u201d and introduced team on triggers. \nIntegrated Survivorship Rules for 6 Personal Lines Systems. Overview of \u201cSurvivorship Rules\u201d (like \u201cWhat is Survivorship and what does it mean to MDM process & Client UI application?\u201d) and introduce team to \u201cSurvivorship / Survival Rules Matrix\u201d sheet.\nWas responsible for identifying and documenting business rules and creating detailed Use Cases, Use Case models, Use Case diagrams and Object Oriented Analysis and Designs (OOAD) using UML (Unified Modeling Language).\nPrepared ER Diagrams, documented Data Dictionary, identified database schemas, its data elements/fields, and studied their nomenclatures from the identified columns, for phased deliverables. \nEnvironment: Informatica Data Director (IDD), Informatica Analyzer, DB2, Informatica Power Center, MDM, Oracle 11g, Information Builder\u2019s WebFocus 8, IBM Cognos Report Studio 10, IBM Lotus Notes, CICS, Cobol, Oracle SQL Server, MS Visio, MS Office Suite, MasterPiece, Guidewire policy admin system, CAAS, Yacht, PLS, ORCA, DRC, SharePoint.\n\nBed Bath and Beyond, Union, NJ                                          \t\t\t\t\t                   July 2014 \u2013 July 2015\nDesignation: Lead Technical Business System Analyst / Technical Product Manager\nPrime focus of the EXIM/Universal Cart was to have the ability to export products/items from BBBY\u2019s online channels (both desktop & mobile applications) to 3rd party vendor sites that offer personalization and/or customization services (like monogramming, etching, sublimation/printing etc.) not currently available on BBBY\u2019s sites/concepts (concepts like BBBY-US, BBBY-Canada, BuyBuyBaby, Christmas Tree Shops, etc.).\nPerformed requirements gathering & analysis by actively soliciting, examining, investigating and negotiating customer requirements with business directors and Product Development Managers, while leading the technical requirements analysis and driving the system design and construction. \nPerformed extensive SKU to Product relationship mapping at UPC level, SKU/item level, Product level, Collection & Accessories level, etc. Thereby understood cardinality between SSWPs (Single SKU Web Products) and MSWPs (Multi SKU Web Products) and process by which they are \u201cWeb Enabled\u201d and \u201cWeb Disabled\u201d for \u201cDigital Merchandising\u201d.\nConducted numerous impact analysis sessions, thereby analyzing the behavior of multiple features & functionalities on different web pages like PLP (Product List Page \u2013 Grid View & List View), PDP (Product Detail Page), Quick View, Collection & Accessories, Your Cart page, Single Shipping & Multi Shipping Location, Order Confirmation, Order Preview, Order History, Track Order, etc. and on the user CTA (Call to Action) button (buttons like Add to Cart, Add to Registry, Save for Later, Find in Store).\nCreated / Documented, managed, controlled and delivered user stories, features and functionalities in form of Story or Functional Requirements (i.e. formal FRD) within the CCM & RM tools respectively. Updated the tools with appropriate COS (Condition of Satisfaction) / Acceptance Criteria, Story Title, Card, Conversation, Assumptions etc. by following standardized Agile SCRUM methodology/process.\nAlso created / documented, managed, controlled and delivered Epic, Features, Story, Tasks, Spikes by breaking these out appropriately and then tie them back to the concerned Feature and Story within the RM (Requirements Management) tool.\nCollaborated with the development team to enforce the implementation of requirements throughout the entire coding cycle and managed change request using Rational Clear Quest.\nEnvironment: IBM\u2019s Rational Tools, IBM Requirement Management Tool, IBM Change & Configuration Management Tool, RMTrack \u2013 Defect Tracking, JIRA, ATG eCommerce Platform, TIBCO, Riversand\u2019s PIM (Product Information Management), Tableau, QlikView, STIBO\u2019s PDM (Product Data Master), Web2.0, Oracle SQL Server Developer, Microstrategy, Oracle Endeca, UNIX, Java, .Net.\n\nMaryland Department of Health & Mental Hygiene / CSC - CNSI, Baltimore, MD                                  Jan 2013 \u2013 July 2014\nDesignation: Sr. Technical Business System Analyst / BI Reporting Lead\nThe prime objective of the project is to replace the current MMIS Claims processing system for DHMH, with a web-based Service Oriented Solution consistent with MITA 2.0 (Medicaid Information Technology Architecture) guidelines that has online web capabilities for all Users including Providers and Recipients/Beneficiaries. The system\u2019s User Interface intent was to provide the capability for online data entry for Provider Enrollment applications; track and automate Workflow Management of the process; and online verification of Provider Enrollment status.\nReviewed, analyzed, evaluated business processes and associated IT application requirements. Analyzed business workflow and system needs for conversions and migrations; performed data mapping and data conversions.\nIdentified specifications for billing and accounts receivable requirements, performed gap analysis & presented information to technical team to identify system requirements. Worked extensively with developing business rules engine enabling the business rules such as referral, prior authorization, eligibility, claims processing and billing essential.\nInvolved in implementation of HIPAA EDI Transactions (835, 837). Facilitated Electronic Data Interchange (EDI). Performed GAP Analysis for HIPAA 4010 and 5010 transactions. Used EDI tools to verify mapping to X12 format. Recommended changes for system design, methods, procedures, policies and workflows affecting Medicare / Medicaid claims processing in compliance with government compliant processes like HIPAA / EDI formats and accredited standards ANSI. Analyzed HIPAA EDI transactions in X12 responses and of 837, 835, 277CA and 999 and looked for defects.\nRecommended changes for system design, methods, procedures, policies and workflows affecting Medicare/Medicaid claims processing in compliance with government compliant processes like HIPAA/ EDI formats and accredited standards ANSI.\nConducted detail oriented analysis on the current \u201cAS-IS\u201d Maryland MMIS legacy system and driving iterative GAP analysis sessions for the same to propose the effective target \u201cTO-BE\u201d MMIS system.\nDesigned Dashboards/Reports using the Oracle Business Intelligence Analytics platform for requirement analysis and data analytics. Enhanced performance of Reports / Dashboards by implementing the Aggregate tables, Materialized Views, Table partitions, re-building of indexes and managing Cache etc.\nResponsible for processing Medicaid claims in a workflow environment. Accurately interpreted benefit & policy provisions applicable to Medicaid enrollees. Reviewed & resolved claim edits using multiple systems, processes and procedures.\nEnvironment: MMIS / CMS Regulations, HIPAA Privacy, Wire Framing, UML, SCRUM, eCAMS & ProviderOne (CNSI owned & developed Claims Administration System), iLotus Notes, IBM COGNOS 10 Report Studio, Oracle Financials, JIRA (Defect Tracking Tool), ReqTrace Web 2.1, Oracle SQL Developer, J2EE, .NET, Oracle ODBC, Oracle BI Enterprise Edition 11g, MS Office Suite, MS Excel, MS Project, SharePoint, Service Oriented Architecture (SOA), RUP (For MMIS Development), Informatica PowerCenter, Enterprise Architect 10 UML (SPARX System\u2019s EA Visual Modeling Platform).\n\nMedco Health Solutions Inc. / Express Scripts Inc., Franklin Lakes, NJ                                                       Oct 2011 \u2013 Jan 2013\nDesignation: Sr. Technical Business System Analyst\nThe purpose of the project was to report the back-end Teradata IW databases utilizing the front-end Cognos BI reporting tool for the Medicare Part D claims Adjustments & Reconciliation and Operations Workbook business process. This project involved creation of a Medicare, Medicaid Solutions (MMMS) dashboard that included views from various channels, operational divisions and products within the Medicare organization.\nGathered all the Reporting Requirements and functional requirements through JAD sessions, formal interviews and brain storming sessions from the Business Owners and stakeholders based on the project scope and documented it in form of FRDs and Use-Cases in CaliberRM / Optimal Trace.\nConducted JAD sessions with different Business Users to develop new policies and procedures for the Service Catalogue, Charge Capture and Service Worklist /Charge Router, Hospital billing, coding, special coding requirements for Medco Health / Express Scripts and Claim processing.\nInvolved in analysis, design, and implementation of the following systems \u2013 Control Management Reporting and Security Systems: generation of daily, weekly, and monthly reports of hospital patients' activity. Medical Billing & Collection Systems: generation of daily billing activity based on patient records & establishment of collection protocols. Accounting Systems: Financial Statement generation and analysis according to government standards and protocols.\nWorked on EDI transactions: X12, 835, and 837 P.I to identify key data set elements for designated record set. Interacted with Claims, Payments and Enrollment hence analyzing and documenting related business processes.\nInvolved in the System Analysis, Design and Development of the project. Integral part of end-to-end implementation of IW Reporting from capturing reporting requirements, Relational data modeling in Cognos, Metadata analysis, mock-up template designing and BI reporting system.\nDisplayed in depth knowledge of Medicare/Medicaid Claims processes from Admin/Provider/Payer side which were later part of the training program to vendors. Worked on improvement of Claims Reimbursement User Interface for a better experience and incorporate changes as per HIPAA guidelines using the gap analysis.\nIdentified the functional as well as technical needs of the department and accordingly engaged as liaison between the Upper Management and IT Team to develop Cognos based BI Reports, applications and softwares.\nAssisted in development of the reports using Cognos Report Studio and Framework Manager. Developed Forms, Reports and Queries using MS Access and Excel (including Pivot tables, Vlookup and Macros).\nEnvironment: SCRUM, RUP, SharePoint, Synon/2E, COBOL, IBM AS/400, IBM Cognos 10 (Report Studio, Query Studio, Framework Manager, Virtual View Manager, Business Insight Dashboard), UNIX, Microsoft SQL Server 2008, T-SQL, MS SQL Server, Teradata IW, BRIO Query Intelligence, MS Office Suite, MS Project, MS Visio, ERWIN, OiM (Oracle Identity Manager), ORACLE SQL Server Developer, Teradata, DB2, Informatica Powercenter, CA ERwin Data Modeler, Oracle BI Enterprise Edition 10g, PL/SQL, CAs ALL Fusion Harvest, HP Quality Center, EDI, XML, .CSV (Comma Delimited), Rational Clear Quest, Anchor DW, Foundation14 (F14), CaliberRM 64Bit Production, Optimal Trace, MagicDraw UML.\n\nCVS Caremark, Richardson, TX                                                                                   \t\t\t      Jan 2010 \u2013 Oct 2011\nDesignation: Sr. Technical Business Analyst\nThe primary goal of the project is to extract common services such as Eligibility, Formulary, Drug Maintenance, etc. out of the disparate systems and host them independently to facilitate economy of operations, isolation of common business services from core adjudication transaction processing and externalize the data in a way that can be consumed by other external applications within the Organization. This will enable common services, tools and interfaces that can improve client experience and drive consistency regardless of which adjudication engine is used.\nGathered, analyzed, documented business and technical requirements from both formal and informal sessions and validate the needs of the business stakeholders, thereby drafting the Technical Design Document (TDD) and System Specification Document (SSD).\nDesigned and developed Use Cases and Use Case scenarios, Activity Diagrams, Sequence Diagrams, High Level and Low Level Process Flow Diagrams, OOAD using UML and Business Process Modeling. Understood client\u2019s business needs related to operational payer departments (i.e. claims, enrollment, billing, etc.).\nProfiled data in the sources prior to defining the data cleaning rules. Perform small enhancements (data cleansing/data quality). Worked on various Professional billing and Hospital billing products.\nDesigned and implemented unique platform, which collected and synchronized information of each person all in one place, including medical claims, lab results, self reported data and other relevant information and also in an efficient and effective manner.\nResponsible for preparing Business Requirement Document (BRD), Functional Requirement Document (FRD) and then translating into functional specifications and test plans. Closely coordinated with both business users and developers for arriving at a mutually acceptable solution.\nDeveloped project definitions, project scope, cost/benefit and risk analysis, work plans, daily and weekly progress reports, and presentations. Created and tested scripts for the premium calculations and claim limits and deductible.\nReviewed the use cases, functions and features list, map the requirements to design, and update the Requirements Traceability Matrix (RTM). Created various reports such as billing payment reports, Billing Grouping Payment and discount reports.\nResponsible to give the payers the clear vision of claim life cycle from submission to CVS Caremark through payer adjunction. Designed & implemented a web based claims processing system & management application to administer and process health insurance claims automatically. It connected the organization to largest all-payer network of commercial & government health plans nationwide to provide wealth of real-time patient benefit information.\nCreated source table definitions in the Data Stage Repository by studying the data sources by importing the data from Mainframe.\nWrote PL/SQL statement and stored procedures in Oracle for extracting as well as writing data. Created and executed claims, enrollment and/or billing test scenarios, including defect tracking.\nEnvironment: RUP, SharePoint, Synon/2E, COBOL, IBM AS/400, CICS, Citrix, Cognos 8, DB2, SAP R/3 FICO, Microsoft SQL Server 2008, T-SQL, SSIS, SSRS, SSAS, MS Excel, MS Word, MS Access, MS Project, MS Visio, ERWIN, Data Stage, OiM (Oracle Identity Manager), Oracle, PL/SQL, PC Media, Oracle BI Enterprise Edition 10g, CAs ALL Fusion Harvest, EDIFECS, HP Quality Center, Velocedi/Claredi, EDI, XML, DIAMOND, COSMOS, FACETS, LOGISTICARE, Rational Rose, Bugzilla, Snagit, Adobe Acrobat Professional, HP Quality Center.\n\nDelta Technologies                                                                                             \t\t\t     July 2006 \u2013 Aug 2008\nDesignation: Lead Technical Business Analyst                                                                                                                   \nThe project was to report corporate data warehouse for online trading system. The reporting system was to capture the KPI such as the trade volumes, number of trades, on the counter trades, and exchange trades, broker dealer trades, trade fees with respect to dimensions such as customer\u2019s location, security type, exchange traded Fees payables, etc.\nGathered the business requirements from the managers through JAD sessions, formal interviews and surveys and worked extensively with the users and with different levels of management to identify requirements, business events to develop functional specifications.\nAnalyzed the AS-IS and TO-BE processes to understand the key findings, the short term considerations, the long term considerations and its benefits.\nOwned the entire reporting process. Interacted with the ETL team, developer(s), management, and account holders to get the requirements, document them, design templates, and write specifications.\nIdentified data elements from the source systems, performed data analysis to come up with data cleansing and integration rules for the ETL process using Informatica.\nAnalyzed research on operational procedures and methods and recommended changes for improvement, with an emphasis on automation and efficiency.\nPerformed analysis and design of applications using OOAD techniques, UML and Design Patterns.\nGathered all the requirements from the stakeholders based on the project scope and documented it in RequisitePro.\nConducted interviews with various business users and Subject Matter Experts (SMEs) to collect Requirements and business process information, using Requisite Pro exercised all the different types of views in Requisite Pro Attribute matrix.\nInvolved in the testing phase right from the Unit testing to the User Acceptance testing.\nDocumented existing and proposed process flow, analyzed current and target system, and conducted GAP analysis from the reporting requirements to the existing data in ODS.\nEnvironment: RUP, Requisite Pro, Data Stage, Rational Tools, Cognos 8, MS Project, MS Word, MS Excel, MS Visio, MS Office Suite, Rational Rose, Windows Vista, ERWIN, MS SQL Server, PL/SQL, SSIS, SSRS, SSAS, Crystal Reports XI.", "years_experience": 10, "score": 0.3052125844958512, "path": "C:\\Users\\admin\\resume\\AI-Resume-Ranker\\data\\uploads\\Candidate66_resume.docx"}, {"filename": "CV_1.pdf", "text": "Syed Saim Abbas Zaidi\nA highly motivated and hardworking individual\nlooking for a responsible role in a reputable\norganization.\n+92 3332133927\nsaimzaidi110786@gmail.com\nHouse#1614, Ghousia Colony,\nNazimabad, Karachi, Karachi,\nPakistan \nE D U C A T I O N\nACCP DIPLOMA\n2023 | ACCP Deploma in Progress\nfrom ( Aptech Learning Shahrah-e-\nFaisal)\nL A N G U A G E S\nP E R S O N A L\nP R O J E C T S  \nWEB DEVELOPMENT\nS K I L L S\nP R O F I L E\nURDU\nNative or Bilingual Proficiency \nENGLISH\nLimited Working Proficiency \nCERTIFICATE\nInternational CATS\ncontest(Computer and Aptitude\nTest).\nInter School Quiz Competition. \nINTERMEDIATE IN  GENERAL SCIENCE\n2023 | Jinnah Govt.College Nazimabad\nMATRICULATION IN COMPUTER SCIENCE \n2021 | MAadinatul-ilm-School\nWEB DEVELOPMENT\nHTML \nCSS\nJavaScript\nBootstrap\nMy SQL\nPhP\nOTHER SKILLS\nMs Word\nCanva \nGit and Github\nIn this Project created a functional shopping\nwebsite for clothes using Html, Css, JavaScript,\nBootstrap and for database connectivity using\nMySQL and Php\nE - Commerce Website \n I am a fresher and currently looking for\na dynamic job where I utilize my skills\nand studying experience as well \nin my life. \nMoreover, I would like to work with\nprofessionals to improve my skills\nwhich will allow me to further improve\nmy skills. \n", "years_experience": 0, "score": 0.06488568091491548, "path": "C:\\Users\\admin\\resume\\AI-Resume-Ranker\\data\\uploads\\CV_1.pdf"}, {"filename": "CV.pdf", "text": "Syed Saim Abbas Zaidi\nA highly motivated and hardworking individual\nlooking for a responsible role in a reputable\norganization.\n+92 3332133927\nsaimzaidi110786@gmail.com\nHouse#1614, Ghousia Colony,\nNazimabad, Karachi, Karachi,\nPakistan \nE D U C A T I O N\nACCP DIPLOMA\n2023 | ACCP Deploma in Progress\nfrom ( Aptech Learning Shahrah-e-\nFaisal)\nL A N G U A G E S\nP E R S O N A L\nP R O J E C T S  \nWEB DEVELOPMENT\nS K I L L S\nP R O F I L E\nURDU\nNative or Bilingual Proficiency \nENGLISH\nLimited Working Proficiency \nCERTIFICATE\nInternational CATS\ncontest(Computer and Aptitude\nTest).\nInter School Quiz Competition. \nINTERMEDIATE IN  GENERAL SCIENCE\n2023 | Jinnah Govt.College Nazimabad\nMATRICULATION IN COMPUTER SCIENCE \n2021 | MAadinatul-ilm-School\nWEB DEVELOPMENT\nHTML \nCSS\nJavaScript\nBootstrap\nMy SQL\nPhP\nOTHER SKILLS\nMs Word\nCanva \nGit and Github\nIn this Project created a functional shopping\nwebsite for clothes using Html, Css, JavaScript,\nBootstrap and for database connectivity using\nMySQL and Php\nE - Commerce Website \n I am a fresher and currently looking for\na dynamic job where I utilize my skills\nand studying experience as well \nin my life. \nMoreover, I would like to work with\nprofessionals to improve my skills\nwhich will allow me to further improve\nmy skills. \n", "years_experience": 0, "score": 0.06488568091491548, "path": "C:\\Users\\admin\\resume\\AI-Resume-Ranker\\data\\uploads\\CV.pdf"}]
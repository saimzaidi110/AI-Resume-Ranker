[{"filename": "Candidate66_resume_4.docx", "text": "Candidate66\nPhone: 201-532-6397. Email: Candidate66@gmail.com\n__________________________________________________________________________________________________\nProfessional Summary\nOverall 10 years of experience as Sr. Technical Business System Analyst (Technical BSA) in Information Technology industry with focus on Data Analysis, Information & Data Management, Service-Oriented Architecture (SOA), Data Mapping & Data Modeling, Business Process Improvement, System Analysis & Design; expertise in implementation of IT projects using Project Management methodologies. Extensive experience in diversified industry sectors including Healthcare, Life Insurance, Personal & Commercial Insurance, Property & Casualty Insurance, Telecommunication, Retail & e-Commerce (Web Merchandising), Commercial & Investment Banking and Credit Card services.\nStrong skills in Data Warehouse Design using Star Schema roll out for the fact and dimension tables. Design of cubes, partitions and aggregations with excellent knowledge of Type 2 Data modeling.\nExpertise in application development with Full Lifecycle implementation of large Data Warehouse including Project Scope, requirements gathering, Star Schema Design, Snowflake Schema, Data Modeling and ODS.\nStrong experience in Data Analysis, Data Migration, Data Cleansing, Transformation, Integration, Data Import, and Data Export through the use of multiple ETL tools such as Informatica PowerCenter.\nStrong practical knowledge of analytical techniques and methodologies such as machine learning/supervised and unsupervised techniques, segmentation, mix and time series modeling, response modeling, lift modeling, experimental design, neural networks, data mining and optimization techniques. Expert knowledge of statistical analysis tools such as R-programming, MATLAB, Spark, SAS (Statistical Analysis System), R/Spark, Apache Hadoop and Cassandra.\nStrong background in applying statistical machine learning techniques to predictive modeling and experience with Machine Learning libraries (via R, H2O, Python, Spark, etc.). Proficiency in programming in Python, R-programming, SQL, JavaScript, Java/Scala/Ruby and shell scripting. Proficiency in consuming REST based API (with JSON payload). Fluency in big data platforms including Apache Hadoop, Apache MapReduce, Hive, Spark, and Pig. Familiarity with Cloud based HaaS/PaaS solutions such as AWS EMR (Amazon Web Services Elastic MapReduce), MS Azure. Strong understanding of data profiling and data cleansing techniques.\nThorough knowledge of eCommerce architecture; ATG framework its functional limitations and the platform features, application capabilities, etc. including industry wide terminologies like B2B, B2C, C2B, C2C with its eCommerce categories and strong knowledge of PCI DSS (Payment Card Industry Data Security Standards) standards.\nTechnical expertise in handling large-scale platform architecture, integrations, development using Java/J2EE Technologies. Experience on Oracle Commerce includes ATG Commerce, ATG Multisite, ATG Merchandising, and Content Management working in some of the key areas such as the Business Control Center (BCC), Profile Management, Shopping Cart, Checkout, Pricing, performance tuning, caching, infrastructure setup and Endeca.\u00a0\nStrong knowledge of application and systems performance improvement techniques, integration technologies, approaches and patterns, Web APIs / Web Service API calls (SoapUI / SOAP, RESTful API, SOA, XML, HTTP like GET / PUT / POST / DELETE, JSON, BitTorrent, Web 2.0, etc.). Strong knowledge of security aspects like risks and threats, authentication, authorization, certificates, encryption. Experience in utilizing frameworks and re-usable components.\nProficient in using Business Analysis tools such as MS Visio, Word, Excel (including Pivot tables, Macros and Vlookup), Rational Requiste Pro, ReqTrace Web, Caliber RM, Optimal Trace, ClearCase and ClearQuest.\nProficiency in elaborating the Use cases, writing Test cases and preparing Requirement Traceability Matrices (RTM).\nEducation: Master of Science in Engineering \u2013 New Jersey Institute of Technology (August 2008 to December 2009).\nCertification: Certified Data Scientist i.e. Data Science Professional (from John Hopkins University).\nTechnical Skills: Windows 95/98/NT/2000/XP/Vista/7, MS Visio, MS Access, MS Word, MS Excel, MS Project, Rational Rose Enterprise, IBM Rational Software Architect, Microsoft Project, Rational Requisite Pro, MS SQL Server 7.0/2000, PL/SQL, CSS (Cascading Style Sheet), OLTP & OLAP, Synon/2E, COBOL, IBM AS/400, CICS, Citrix, DB2, SAP ERP R/3 MMWM, SAP R/3 FICO, ERWIN Data Modeler 4.2/4.1/4.0, Business Objects XI r3, r2, r1, 6.5, 6.1, 5.x, 4.x, Crystal Reports XI, Informatica, HTML, Dream Weaver, MS FrontPage, IBM COGNOS (Report Studio, Query Studio, Framework Manager, Virtual View Manager, Business Insight Dashboard), Oracle BI Enterprise Edition (OBIEE), C#, C++, R Programming Language (Version 3.2.5), Minitab, XML, XSD, WSDL, X12, SAP PowerDesigner, SAP PowerBuilder, Analysis of Variance (ANOVA) and Experimental Design, Total Quality Management and Lean Six Sigma Manufacturing Methodologies, SAFe (Scaled Agile Framework) Practitioner, PMI\u2019s PMBOK guidelines & standards, TOGAF (The Open Group Architecture Framework), IIBA\u2019s BABOK guidelines & standards, Business Architecture Guild\u2019s BIZBOK (Business Architecture Body of Knowledge) standards & guidelines, Enterprise Architecture Body of Knowledge (EABOK), Enterprise Information Architecture (EIA) framework.\n\nProfessional Experience\nFidelity Investments (FMR) Smithfield, RI                               \t\t\t                                     Dec 2016 \u2013 Current\nDesignation: Principal Program Analyst / Technical Project Architect\nPrimary objective of the project was to address and resolve corporate Audit issue (Tier C) finding for the Enterprise Cybersecurity group on highly confidential Client Credentials (comprised of PII data) exposed into the server logs written by various Internal (employees within each business units, Broker \u2013 Dealer apps, 3rd party vendor apps) & External (customer / clients) Fidelity hosted applications, thereby applying log masking logic, data redaction techniques on these exposed keys within server logs physically located on multiple servers.\nDelivered elite level security solutions (for its Enterprise Cybersecurity business group) which essentially protects Fidelity and its customer's valuable information. Interacted with stakeholders, facilitated multiple fusion sessions and delivered the end results of work that provide the necessary requirements to the delivery team which thereby built and supported technology solutions.\nWorked as part of the Customer Protection Program, directly with Program and Project Managers as well as technology and business partners.\nWorked with tools such as Jira and Confluence to provide transparency of scope. Worked with our delivery team(s) including QA Data Services and engineering to ensure that we are meeting the needs of our stakeholders plus delivered solid, quality products to production. \nSupported Program and Project managers including providing input to/creation of strategy and execution plans and input to project status. Point person to escalate risks, change requests and participated in corrective actions (path to green).\nPerformed server logs analysis on Linux and Unix servers used by enterprise wide business units, thereby leveraged Splunk GUI tool to read and extrapolate pertinent meta data from these server logs. Used tools like Jira, ITEC PPMC, Confluence, etc. to track, monitor, delegate tasks / activities, operate different project aspects and functions.\nDerived insights out of ambiguity \u2013 understood, processed and interpreted complex data sets. Analyzed complex business data & identified patterns in the data using algorithms from statistics and machine learning. Developed and improved algorithms and methodologies to handle, browse, process and visualize structured and unstructured data.\nTranslated Business requirements, business rules and business criteria to system specifications and system requirements thereby preparing SRS (System Requirement Specifications) and PRS (Performance Requirement Specifications).\nEnvironment: MS Office Suite Package, MS Visio 2013 Pro, MS Excel Pivot Tables, MS Excel Functions & Formulas, MS Excel VLookUp, MS Excel Macros, Splunk Enterprise 6.5 (Splunk GUI Tool), Oracle 11g, IBM DB2, CyberArk EIAM (Enterprise Identity & Access Management), IBM InfoSphere Optim Data Privacy, ServiceNow, VMWare Tools, Oracle 11g Client 11.2.0.2 r01.02, Quest TOAD for Oracle, MS Access, Informatica Power Center, MS SQL Server, ITEC (IT Enablement Center) ALM (Application LifeCycle Management), Atlassian Confluence, ITEC (IT Enablement Center) Atlassian JIRA, Document Central, IBM DB2 Advanced Recovery Expert, IBM DB2 Merge BackUp, HP ITEC (IT Enablement Center) PPMC (Project & Portfolio Management Center).\n\nVerizon Wireless, Warren, NJ                               \t  \t\t\t\t\t\t    May 2016 \u2013 Dec 2016\nDesignation: Sr. Technical Business System Analyst / Technical Project Manager\nThe prime objective of the IoT (Internet Of Things), ReSale, M2M (Machine to Machine) portfolio was to utilize the core capabilities within MCS (Mobile Content Solutions) application suite module in order to web merchandise the non-tangible (like Licenses and Professional Services components within a cart) Items / SKUs through frontend Omni-channels; thereby foster & grow B2B (Business To Business), B2C (Business To Customer) business models from enterprise\u2019s strategic viewpoint and generate billing events at Account / Customer level from backend system application.\nAuthored, owned, managed and maintained 1Pagers to give brief overview of the products / services to be created and maintained in our MCS / SCM portal. Facilitated meeting sessions with Product Managers, Product Owners, Architectures and Analyst to hash out visio IoT (Internet of Things) Vendor flows, Order Fulfillment Flows and Order Processing Flows.\nPlanned, created, designed, managed and baselined draft high-level Project Scope, In-Scope & Out-Scope Functionalities, Draft SLAs (for 3rd party vendors / partners), Baseline Project Timelines & Gantt Chart at project implementation level with external 3rd party vendors / partners (in order to accomplish System Integration Testing [SIT] milestone) system application.\nDefined, designed and elicited core business objectives in order to deliver qualitative product, program and project with optimum utilization and the most efficient use of enterprise resources.\nManaged project charter, project plans, scope statement, project execution, risk mitigation, contingency plan and achievement of project and program objectives. Managed SIT & UAT test activities by ensuring defects are logged accurately in JIRA and issues resolved appropriately.\nCreated, aligned and logged EPICS, User Stories / Features, Tasks (Development, Infrastructure & QA Tasks), Bugs, Defects, BRs (Build Requests), DRs (Deployment Requests), Issues, etc. in JIRA tool.\nStrong knowledge of Unix commands and running those Unix commands in various boxes to validate server logs.\nEffectively and lucidly communicated project risks, issues (especially launch gating), dependencies, progress status, launches, concerns and retrospectives in structured RAID template designed and choreographed by me.\nEnvironment: Activiti BPM, XML Spy, RestFUL API, SoapUI, HP Quality Center ALM (Application Lifecycle Management) 12.20 & 12.01, Quest Toad For Oracle 12.0 (64bit), Programmer\u2019s Notepad, PuTTY.exe, JIRA, CA\u2019s Clarity PPM, Cloudsite SharePoint, Confluence document repository, APEX Search Client RealTime application, DB2 AppDevClient 9.7, MS Office Project Standard 2007, MS Office Standard 2007, MS Office Visio Standard, Open Source XML tools, Oracle Client 11g R2 11.2.0.1.0, Techsmith\u2019s SnagIT Editor 12, SQL Developer 4.1.3.\n\nChubb Insurance / Chubb Group of Insurance Companies, Whitehouse Station, NJ                            July 2015 \u2013 May 2016\nDesignation: Sr. Technical Business System Analyst / Project Architect\nThe purpose of the Client Integration initiative is to be able to view and manage PRS\u2019 business at the client / household / account level in an integrated fashion so that we can make better and faster underwriting decisions, more effectively market and communicate with existing and potential clients, price across clients and deliver better customer service. Integrate PLS & ORCA into the Chubb Client MDM/UI in order to provide an integrated household policy portfolio view across key Chubb Policy Admin Systems for Personal Lines segment, along with the other legacy Chubb\u2019s disparate policy admin systems like MasterPiece, CAAS & Yacht.\nCreated and documented roadmap for meetings with ACE. Baselined documents used for source to target mapping. Understanding CHUBB's system and applications i.e. Target system for CLIENT INT project.\nDesigned, developed and maintained consistency of the Pre-landing and Landing tables \u201cGeneric Landing Layout\u201d file / sheet that the core execution team leveraged for Source To Target Mapping sessions. \nCreated and documented High Level Data Movement and Data Flow diagrams, High Level & Detailed Conceptual Architecture diagrams using BPMN (Business Process Model and Notation) standards encapsulating every system components within visio flows. Conducted thorough analysis on CPI data models for Subject Of Insurance, Policy, Policy Term, Business Party schemas.\nConfirmed on the mapping performed within \u201cORCA Policy Status Mapping\u201d sheet for \u201cTransaction Status\u201d type and \u201cContract Status\u201d type. Confirmed the data points in \u201cMatch Columns\u201d column for the \u201cMatch Rules\u201d from \u201cClient INT ORCA Match Rules\u201d sheet. Gave an overview on \u201cSystem Triggers\u201d and introduced team on triggers. \nIntegrated Survivorship Rules for 6 Personal Lines Systems. Overview of \u201cSurvivorship Rules\u201d (like \u201cWhat is Survivorship and what does it mean to MDM process & Client UI application?\u201d) and introduce team to \u201cSurvivorship / Survival Rules Matrix\u201d sheet.\nWas responsible for identifying and documenting business rules and creating detailed Use Cases, Use Case models, Use Case diagrams and Object Oriented Analysis and Designs (OOAD) using UML (Unified Modeling Language).\nPrepared ER Diagrams, documented Data Dictionary, identified database schemas, its data elements/fields, and studied their nomenclatures from the identified columns, for phased deliverables. \nEnvironment: Informatica Data Director (IDD), Informatica Analyzer, DB2, Informatica Power Center, MDM, Oracle 11g, Information Builder\u2019s WebFocus 8, IBM Cognos Report Studio 10, IBM Lotus Notes, CICS, Cobol, Oracle SQL Server, MS Visio, MS Office Suite, MasterPiece, Guidewire policy admin system, CAAS, Yacht, PLS, ORCA, DRC, SharePoint.\n\nBed Bath and Beyond, Union, NJ                                          \t\t\t\t\t                   July 2014 \u2013 July 2015\nDesignation: Lead Technical Business System Analyst / Technical Product Manager\nPrime focus of the EXIM/Universal Cart was to have the ability to export products/items from BBBY\u2019s online channels (both desktop & mobile applications) to 3rd party vendor sites that offer personalization and/or customization services (like monogramming, etching, sublimation/printing etc.) not currently available on BBBY\u2019s sites/concepts (concepts like BBBY-US, BBBY-Canada, BuyBuyBaby, Christmas Tree Shops, etc.).\nPerformed requirements gathering & analysis by actively soliciting, examining, investigating and negotiating customer requirements with business directors and Product Development Managers, while leading the technical requirements analysis and driving the system design and construction. \nPerformed extensive SKU to Product relationship mapping at UPC level, SKU/item level, Product level, Collection & Accessories level, etc. Thereby understood cardinality between SSWPs (Single SKU Web Products) and MSWPs (Multi SKU Web Products) and process by which they are \u201cWeb Enabled\u201d and \u201cWeb Disabled\u201d for \u201cDigital Merchandising\u201d.\nConducted numerous impact analysis sessions, thereby analyzing the behavior of multiple features & functionalities on different web pages like PLP (Product List Page \u2013 Grid View & List View), PDP (Product Detail Page), Quick View, Collection & Accessories, Your Cart page, Single Shipping & Multi Shipping Location, Order Confirmation, Order Preview, Order History, Track Order, etc. and on the user CTA (Call to Action) button (buttons like Add to Cart, Add to Registry, Save for Later, Find in Store).\nCreated / Documented, managed, controlled and delivered user stories, features and functionalities in form of Story or Functional Requirements (i.e. formal FRD) within the CCM & RM tools respectively. Updated the tools with appropriate COS (Condition of Satisfaction) / Acceptance Criteria, Story Title, Card, Conversation, Assumptions etc. by following standardized Agile SCRUM methodology/process.\nAlso created / documented, managed, controlled and delivered Epic, Features, Story, Tasks, Spikes by breaking these out appropriately and then tie them back to the concerned Feature and Story within the RM (Requirements Management) tool.\nCollaborated with the development team to enforce the implementation of requirements throughout the entire coding cycle and managed change request using Rational Clear Quest.\nEnvironment: IBM\u2019s Rational Tools, IBM Requirement Management Tool, IBM Change & Configuration Management Tool, RMTrack \u2013 Defect Tracking, JIRA, ATG eCommerce Platform, TIBCO, Riversand\u2019s PIM (Product Information Management), Tableau, QlikView, STIBO\u2019s PDM (Product Data Master), Web2.0, Oracle SQL Server Developer, Microstrategy, Oracle Endeca, UNIX, Java, .Net.\n\nMaryland Department of Health & Mental Hygiene / CSC - CNSI, Baltimore, MD                                  Jan 2013 \u2013 July 2014\nDesignation: Sr. Technical Business System Analyst / BI Reporting Lead\nThe prime objective of the project is to replace the current MMIS Claims processing system for DHMH, with a web-based Service Oriented Solution consistent with MITA 2.0 (Medicaid Information Technology Architecture) guidelines that has online web capabilities for all Users including Providers and Recipients/Beneficiaries. The system\u2019s User Interface intent was to provide the capability for online data entry for Provider Enrollment applications; track and automate Workflow Management of the process; and online verification of Provider Enrollment status.\nReviewed, analyzed, evaluated business processes and associated IT application requirements. Analyzed business workflow and system needs for conversions and migrations; performed data mapping and data conversions.\nIdentified specifications for billing and accounts receivable requirements, performed gap analysis & presented information to technical team to identify system requirements. Worked extensively with developing business rules engine enabling the business rules such as referral, prior authorization, eligibility, claims processing and billing essential.\nInvolved in implementation of HIPAA EDI Transactions (835, 837). Facilitated Electronic Data Interchange (EDI). Performed GAP Analysis for HIPAA 4010 and 5010 transactions. Used EDI tools to verify mapping to X12 format. Recommended changes for system design, methods, procedures, policies and workflows affecting Medicare / Medicaid claims processing in compliance with government compliant processes like HIPAA / EDI formats and accredited standards ANSI. Analyzed HIPAA EDI transactions in X12 responses and of 837, 835, 277CA and 999 and looked for defects.\nRecommended changes for system design, methods, procedures, policies and workflows affecting Medicare/Medicaid claims processing in compliance with government compliant processes like HIPAA/ EDI formats and accredited standards ANSI.\nConducted detail oriented analysis on the current \u201cAS-IS\u201d Maryland MMIS legacy system and driving iterative GAP analysis sessions for the same to propose the effective target \u201cTO-BE\u201d MMIS system.\nDesigned Dashboards/Reports using the Oracle Business Intelligence Analytics platform for requirement analysis and data analytics. Enhanced performance of Reports / Dashboards by implementing the Aggregate tables, Materialized Views, Table partitions, re-building of indexes and managing Cache etc.\nResponsible for processing Medicaid claims in a workflow environment. Accurately interpreted benefit & policy provisions applicable to Medicaid enrollees. Reviewed & resolved claim edits using multiple systems, processes and procedures.\nEnvironment: MMIS / CMS Regulations, HIPAA Privacy, Wire Framing, UML, SCRUM, eCAMS & ProviderOne (CNSI owned & developed Claims Administration System), iLotus Notes, IBM COGNOS 10 Report Studio, Oracle Financials, JIRA (Defect Tracking Tool), ReqTrace Web 2.1, Oracle SQL Developer, J2EE, .NET, Oracle ODBC, Oracle BI Enterprise Edition 11g, MS Office Suite, MS Excel, MS Project, SharePoint, Service Oriented Architecture (SOA), RUP (For MMIS Development), Informatica PowerCenter, Enterprise Architect 10 UML (SPARX System\u2019s EA Visual Modeling Platform).\n\nMedco Health Solutions Inc. / Express Scripts Inc., Franklin Lakes, NJ                                                       Oct 2011 \u2013 Jan 2013\nDesignation: Sr. Technical Business System Analyst\nThe purpose of the project was to report the back-end Teradata IW databases utilizing the front-end Cognos BI reporting tool for the Medicare Part D claims Adjustments & Reconciliation and Operations Workbook business process. This project involved creation of a Medicare, Medicaid Solutions (MMMS) dashboard that included views from various channels, operational divisions and products within the Medicare organization.\nGathered all the Reporting Requirements and functional requirements through JAD sessions, formal interviews and brain storming sessions from the Business Owners and stakeholders based on the project scope and documented it in form of FRDs and Use-Cases in CaliberRM / Optimal Trace.\nConducted JAD sessions with different Business Users to develop new policies and procedures for the Service Catalogue, Charge Capture and Service Worklist /Charge Router, Hospital billing, coding, special coding requirements for Medco Health / Express Scripts and Claim processing.\nInvolved in analysis, design, and implementation of the following systems \u2013 Control Management Reporting and Security Systems: generation of daily, weekly, and monthly reports of hospital patients' activity. Medical Billing & Collection Systems: generation of daily billing activity based on patient records & establishment of collection protocols. Accounting Systems: Financial Statement generation and analysis according to government standards and protocols.\nWorked on EDI transactions: X12, 835, and 837 P.I to identify key data set elements for designated record set. Interacted with Claims, Payments and Enrollment hence analyzing and documenting related business processes.\nInvolved in the System Analysis, Design and Development of the project. Integral part of end-to-end implementation of IW Reporting from capturing reporting requirements, Relational data modeling in Cognos, Metadata analysis, mock-up template designing and BI reporting system.\nDisplayed in depth knowledge of Medicare/Medicaid Claims processes from Admin/Provider/Payer side which were later part of the training program to vendors. Worked on improvement of Claims Reimbursement User Interface for a better experience and incorporate changes as per HIPAA guidelines using the gap analysis.\nIdentified the functional as well as technical needs of the department and accordingly engaged as liaison between the Upper Management and IT Team to develop Cognos based BI Reports, applications and softwares.\nAssisted in development of the reports using Cognos Report Studio and Framework Manager. Developed Forms, Reports and Queries using MS Access and Excel (including Pivot tables, Vlookup and Macros).\nEnvironment: SCRUM, RUP, SharePoint, Synon/2E, COBOL, IBM AS/400, IBM Cognos 10 (Report Studio, Query Studio, Framework Manager, Virtual View Manager, Business Insight Dashboard), UNIX, Microsoft SQL Server 2008, T-SQL, MS SQL Server, Teradata IW, BRIO Query Intelligence, MS Office Suite, MS Project, MS Visio, ERWIN, OiM (Oracle Identity Manager), ORACLE SQL Server Developer, Teradata, DB2, Informatica Powercenter, CA ERwin Data Modeler, Oracle BI Enterprise Edition 10g, PL/SQL, CAs ALL Fusion Harvest, HP Quality Center, EDI, XML, .CSV (Comma Delimited), Rational Clear Quest, Anchor DW, Foundation14 (F14), CaliberRM 64Bit Production, Optimal Trace, MagicDraw UML.\n\nCVS Caremark, Richardson, TX                                                                                   \t\t\t      Jan 2010 \u2013 Oct 2011\nDesignation: Sr. Technical Business Analyst\nThe primary goal of the project is to extract common services such as Eligibility, Formulary, Drug Maintenance, etc. out of the disparate systems and host them independently to facilitate economy of operations, isolation of common business services from core adjudication transaction processing and externalize the data in a way that can be consumed by other external applications within the Organization. This will enable common services, tools and interfaces that can improve client experience and drive consistency regardless of which adjudication engine is used.\nGathered, analyzed, documented business and technical requirements from both formal and informal sessions and validate the needs of the business stakeholders, thereby drafting the Technical Design Document (TDD) and System Specification Document (SSD).\nDesigned and developed Use Cases and Use Case scenarios, Activity Diagrams, Sequence Diagrams, High Level and Low Level Process Flow Diagrams, OOAD using UML and Business Process Modeling. Understood client\u2019s business needs related to operational payer departments (i.e. claims, enrollment, billing, etc.).\nProfiled data in the sources prior to defining the data cleaning rules. Perform small enhancements (data cleansing/data quality). Worked on various Professional billing and Hospital billing products.\nDesigned and implemented unique platform, which collected and synchronized information of each person all in one place, including medical claims, lab results, self reported data and other relevant information and also in an efficient and effective manner.\nResponsible for preparing Business Requirement Document (BRD), Functional Requirement Document (FRD) and then translating into functional specifications and test plans. Closely coordinated with both business users and developers for arriving at a mutually acceptable solution.\nDeveloped project definitions, project scope, cost/benefit and risk analysis, work plans, daily and weekly progress reports, and presentations. Created and tested scripts for the premium calculations and claim limits and deductible.\nReviewed the use cases, functions and features list, map the requirements to design, and update the Requirements Traceability Matrix (RTM). Created various reports such as billing payment reports, Billing Grouping Payment and discount reports.\nResponsible to give the payers the clear vision of claim life cycle from submission to CVS Caremark through payer adjunction. Designed & implemented a web based claims processing system & management application to administer and process health insurance claims automatically. It connected the organization to largest all-payer network of commercial & government health plans nationwide to provide wealth of real-time patient benefit information.\nCreated source table definitions in the Data Stage Repository by studying the data sources by importing the data from Mainframe.\nWrote PL/SQL statement and stored procedures in Oracle for extracting as well as writing data. Created and executed claims, enrollment and/or billing test scenarios, including defect tracking.\nEnvironment: RUP, SharePoint, Synon/2E, COBOL, IBM AS/400, CICS, Citrix, Cognos 8, DB2, SAP R/3 FICO, Microsoft SQL Server 2008, T-SQL, SSIS, SSRS, SSAS, MS Excel, MS Word, MS Access, MS Project, MS Visio, ERWIN, Data Stage, OiM (Oracle Identity Manager), Oracle, PL/SQL, PC Media, Oracle BI Enterprise Edition 10g, CAs ALL Fusion Harvest, EDIFECS, HP Quality Center, Velocedi/Claredi, EDI, XML, DIAMOND, COSMOS, FACETS, LOGISTICARE, Rational Rose, Bugzilla, Snagit, Adobe Acrobat Professional, HP Quality Center.\n\nDelta Technologies                                                                                             \t\t\t     July 2006 \u2013 Aug 2008\nDesignation: Lead Technical Business Analyst                                                                                                                   \nThe project was to report corporate data warehouse for online trading system. The reporting system was to capture the KPI such as the trade volumes, number of trades, on the counter trades, and exchange trades, broker dealer trades, trade fees with respect to dimensions such as customer\u2019s location, security type, exchange traded Fees payables, etc.\nGathered the business requirements from the managers through JAD sessions, formal interviews and surveys and worked extensively with the users and with different levels of management to identify requirements, business events to develop functional specifications.\nAnalyzed the AS-IS and TO-BE processes to understand the key findings, the short term considerations, the long term considerations and its benefits.\nOwned the entire reporting process. Interacted with the ETL team, developer(s), management, and account holders to get the requirements, document them, design templates, and write specifications.\nIdentified data elements from the source systems, performed data analysis to come up with data cleansing and integration rules for the ETL process using Informatica.\nAnalyzed research on operational procedures and methods and recommended changes for improvement, with an emphasis on automation and efficiency.\nPerformed analysis and design of applications using OOAD techniques, UML and Design Patterns.\nGathered all the requirements from the stakeholders based on the project scope and documented it in RequisitePro.\nConducted interviews with various business users and Subject Matter Experts (SMEs) to collect Requirements and business process information, using Requisite Pro exercised all the different types of views in Requisite Pro Attribute matrix.\nInvolved in the testing phase right from the Unit testing to the User Acceptance testing.\nDocumented existing and proposed process flow, analyzed current and target system, and conducted GAP analysis from the reporting requirements to the existing data in ODS.\nEnvironment: RUP, Requisite Pro, Data Stage, Rational Tools, Cognos 8, MS Project, MS Word, MS Excel, MS Visio, MS Office Suite, Rational Rose, Windows Vista, ERWIN, MS SQL Server, PL/SQL, SSIS, SSRS, SSAS, Crystal Reports XI.", "years_experience": 10, "score": 0.3367810583086025, "path": "C:\\Users\\admin\\AI-Resume-Ranker\\data\\uploads\\Candidate66_resume_4.docx"}, {"filename": "Candidate66_resume_3.docx", "text": "Candidate66\nPhone: 201-532-6397. Email: Candidate66@gmail.com\n__________________________________________________________________________________________________\nProfessional Summary\nOverall 10 years of experience as Sr. Technical Business System Analyst (Technical BSA) in Information Technology industry with focus on Data Analysis, Information & Data Management, Service-Oriented Architecture (SOA), Data Mapping & Data Modeling, Business Process Improvement, System Analysis & Design; expertise in implementation of IT projects using Project Management methodologies. Extensive experience in diversified industry sectors including Healthcare, Life Insurance, Personal & Commercial Insurance, Property & Casualty Insurance, Telecommunication, Retail & e-Commerce (Web Merchandising), Commercial & Investment Banking and Credit Card services.\nStrong skills in Data Warehouse Design using Star Schema roll out for the fact and dimension tables. Design of cubes, partitions and aggregations with excellent knowledge of Type 2 Data modeling.\nExpertise in application development with Full Lifecycle implementation of large Data Warehouse including Project Scope, requirements gathering, Star Schema Design, Snowflake Schema, Data Modeling and ODS.\nStrong experience in Data Analysis, Data Migration, Data Cleansing, Transformation, Integration, Data Import, and Data Export through the use of multiple ETL tools such as Informatica PowerCenter.\nStrong practical knowledge of analytical techniques and methodologies such as machine learning/supervised and unsupervised techniques, segmentation, mix and time series modeling, response modeling, lift modeling, experimental design, neural networks, data mining and optimization techniques. Expert knowledge of statistical analysis tools such as R-programming, MATLAB, Spark, SAS (Statistical Analysis System), R/Spark, Apache Hadoop and Cassandra.\nStrong background in applying statistical machine learning techniques to predictive modeling and experience with Machine Learning libraries (via R, H2O, Python, Spark, etc.). Proficiency in programming in Python, R-programming, SQL, JavaScript, Java/Scala/Ruby and shell scripting. Proficiency in consuming REST based API (with JSON payload). Fluency in big data platforms including Apache Hadoop, Apache MapReduce, Hive, Spark, and Pig. Familiarity with Cloud based HaaS/PaaS solutions such as AWS EMR (Amazon Web Services Elastic MapReduce), MS Azure. Strong understanding of data profiling and data cleansing techniques.\nThorough knowledge of eCommerce architecture; ATG framework its functional limitations and the platform features, application capabilities, etc. including industry wide terminologies like B2B, B2C, C2B, C2C with its eCommerce categories and strong knowledge of PCI DSS (Payment Card Industry Data Security Standards) standards.\nTechnical expertise in handling large-scale platform architecture, integrations, development using Java/J2EE Technologies. Experience on Oracle Commerce includes ATG Commerce, ATG Multisite, ATG Merchandising, and Content Management working in some of the key areas such as the Business Control Center (BCC), Profile Management, Shopping Cart, Checkout, Pricing, performance tuning, caching, infrastructure setup and Endeca.\u00a0\nStrong knowledge of application and systems performance improvement techniques, integration technologies, approaches and patterns, Web APIs / Web Service API calls (SoapUI / SOAP, RESTful API, SOA, XML, HTTP like GET / PUT / POST / DELETE, JSON, BitTorrent, Web 2.0, etc.). Strong knowledge of security aspects like risks and threats, authentication, authorization, certificates, encryption. Experience in utilizing frameworks and re-usable components.\nProficient in using Business Analysis tools such as MS Visio, Word, Excel (including Pivot tables, Macros and Vlookup), Rational Requiste Pro, ReqTrace Web, Caliber RM, Optimal Trace, ClearCase and ClearQuest.\nProficiency in elaborating the Use cases, writing Test cases and preparing Requirement Traceability Matrices (RTM).\nEducation: Master of Science in Engineering \u2013 New Jersey Institute of Technology (August 2008 to December 2009).\nCertification: Certified Data Scientist i.e. Data Science Professional (from John Hopkins University).\nTechnical Skills: Windows 95/98/NT/2000/XP/Vista/7, MS Visio, MS Access, MS Word, MS Excel, MS Project, Rational Rose Enterprise, IBM Rational Software Architect, Microsoft Project, Rational Requisite Pro, MS SQL Server 7.0/2000, PL/SQL, CSS (Cascading Style Sheet), OLTP & OLAP, Synon/2E, COBOL, IBM AS/400, CICS, Citrix, DB2, SAP ERP R/3 MMWM, SAP R/3 FICO, ERWIN Data Modeler 4.2/4.1/4.0, Business Objects XI r3, r2, r1, 6.5, 6.1, 5.x, 4.x, Crystal Reports XI, Informatica, HTML, Dream Weaver, MS FrontPage, IBM COGNOS (Report Studio, Query Studio, Framework Manager, Virtual View Manager, Business Insight Dashboard), Oracle BI Enterprise Edition (OBIEE), C#, C++, R Programming Language (Version 3.2.5), Minitab, XML, XSD, WSDL, X12, SAP PowerDesigner, SAP PowerBuilder, Analysis of Variance (ANOVA) and Experimental Design, Total Quality Management and Lean Six Sigma Manufacturing Methodologies, SAFe (Scaled Agile Framework) Practitioner, PMI\u2019s PMBOK guidelines & standards, TOGAF (The Open Group Architecture Framework), IIBA\u2019s BABOK guidelines & standards, Business Architecture Guild\u2019s BIZBOK (Business Architecture Body of Knowledge) standards & guidelines, Enterprise Architecture Body of Knowledge (EABOK), Enterprise Information Architecture (EIA) framework.\n\nProfessional Experience\nFidelity Investments (FMR) Smithfield, RI                               \t\t\t                                     Dec 2016 \u2013 Current\nDesignation: Principal Program Analyst / Technical Project Architect\nPrimary objective of the project was to address and resolve corporate Audit issue (Tier C) finding for the Enterprise Cybersecurity group on highly confidential Client Credentials (comprised of PII data) exposed into the server logs written by various Internal (employees within each business units, Broker \u2013 Dealer apps, 3rd party vendor apps) & External (customer / clients) Fidelity hosted applications, thereby applying log masking logic, data redaction techniques on these exposed keys within server logs physically located on multiple servers.\nDelivered elite level security solutions (for its Enterprise Cybersecurity business group) which essentially protects Fidelity and its customer's valuable information. Interacted with stakeholders, facilitated multiple fusion sessions and delivered the end results of work that provide the necessary requirements to the delivery team which thereby built and supported technology solutions.\nWorked as part of the Customer Protection Program, directly with Program and Project Managers as well as technology and business partners.\nWorked with tools such as Jira and Confluence to provide transparency of scope. Worked with our delivery team(s) including QA Data Services and engineering to ensure that we are meeting the needs of our stakeholders plus delivered solid, quality products to production. \nSupported Program and Project managers including providing input to/creation of strategy and execution plans and input to project status. Point person to escalate risks, change requests and participated in corrective actions (path to green).\nPerformed server logs analysis on Linux and Unix servers used by enterprise wide business units, thereby leveraged Splunk GUI tool to read and extrapolate pertinent meta data from these server logs. Used tools like Jira, ITEC PPMC, Confluence, etc. to track, monitor, delegate tasks / activities, operate different project aspects and functions.\nDerived insights out of ambiguity \u2013 understood, processed and interpreted complex data sets. Analyzed complex business data & identified patterns in the data using algorithms from statistics and machine learning. Developed and improved algorithms and methodologies to handle, browse, process and visualize structured and unstructured data.\nTranslated Business requirements, business rules and business criteria to system specifications and system requirements thereby preparing SRS (System Requirement Specifications) and PRS (Performance Requirement Specifications).\nEnvironment: MS Office Suite Package, MS Visio 2013 Pro, MS Excel Pivot Tables, MS Excel Functions & Formulas, MS Excel VLookUp, MS Excel Macros, Splunk Enterprise 6.5 (Splunk GUI Tool), Oracle 11g, IBM DB2, CyberArk EIAM (Enterprise Identity & Access Management), IBM InfoSphere Optim Data Privacy, ServiceNow, VMWare Tools, Oracle 11g Client 11.2.0.2 r01.02, Quest TOAD for Oracle, MS Access, Informatica Power Center, MS SQL Server, ITEC (IT Enablement Center) ALM (Application LifeCycle Management), Atlassian Confluence, ITEC (IT Enablement Center) Atlassian JIRA, Document Central, IBM DB2 Advanced Recovery Expert, IBM DB2 Merge BackUp, HP ITEC (IT Enablement Center) PPMC (Project & Portfolio Management Center).\n\nVerizon Wireless, Warren, NJ                               \t  \t\t\t\t\t\t    May 2016 \u2013 Dec 2016\nDesignation: Sr. Technical Business System Analyst / Technical Project Manager\nThe prime objective of the IoT (Internet Of Things), ReSale, M2M (Machine to Machine) portfolio was to utilize the core capabilities within MCS (Mobile Content Solutions) application suite module in order to web merchandise the non-tangible (like Licenses and Professional Services components within a cart) Items / SKUs through frontend Omni-channels; thereby foster & grow B2B (Business To Business), B2C (Business To Customer) business models from enterprise\u2019s strategic viewpoint and generate billing events at Account / Customer level from backend system application.\nAuthored, owned, managed and maintained 1Pagers to give brief overview of the products / services to be created and maintained in our MCS / SCM portal. Facilitated meeting sessions with Product Managers, Product Owners, Architectures and Analyst to hash out visio IoT (Internet of Things) Vendor flows, Order Fulfillment Flows and Order Processing Flows.\nPlanned, created, designed, managed and baselined draft high-level Project Scope, In-Scope & Out-Scope Functionalities, Draft SLAs (for 3rd party vendors / partners), Baseline Project Timelines & Gantt Chart at project implementation level with external 3rd party vendors / partners (in order to accomplish System Integration Testing [SIT] milestone) system application.\nDefined, designed and elicited core business objectives in order to deliver qualitative product, program and project with optimum utilization and the most efficient use of enterprise resources.\nManaged project charter, project plans, scope statement, project execution, risk mitigation, contingency plan and achievement of project and program objectives. Managed SIT & UAT test activities by ensuring defects are logged accurately in JIRA and issues resolved appropriately.\nCreated, aligned and logged EPICS, User Stories / Features, Tasks (Development, Infrastructure & QA Tasks), Bugs, Defects, BRs (Build Requests), DRs (Deployment Requests), Issues, etc. in JIRA tool.\nStrong knowledge of Unix commands and running those Unix commands in various boxes to validate server logs.\nEffectively and lucidly communicated project risks, issues (especially launch gating), dependencies, progress status, launches, concerns and retrospectives in structured RAID template designed and choreographed by me.\nEnvironment: Activiti BPM, XML Spy, RestFUL API, SoapUI, HP Quality Center ALM (Application Lifecycle Management) 12.20 & 12.01, Quest Toad For Oracle 12.0 (64bit), Programmer\u2019s Notepad, PuTTY.exe, JIRA, CA\u2019s Clarity PPM, Cloudsite SharePoint, Confluence document repository, APEX Search Client RealTime application, DB2 AppDevClient 9.7, MS Office Project Standard 2007, MS Office Standard 2007, MS Office Visio Standard, Open Source XML tools, Oracle Client 11g R2 11.2.0.1.0, Techsmith\u2019s SnagIT Editor 12, SQL Developer 4.1.3.\n\nChubb Insurance / Chubb Group of Insurance Companies, Whitehouse Station, NJ                            July 2015 \u2013 May 2016\nDesignation: Sr. Technical Business System Analyst / Project Architect\nThe purpose of the Client Integration initiative is to be able to view and manage PRS\u2019 business at the client / household / account level in an integrated fashion so that we can make better and faster underwriting decisions, more effectively market and communicate with existing and potential clients, price across clients and deliver better customer service. Integrate PLS & ORCA into the Chubb Client MDM/UI in order to provide an integrated household policy portfolio view across key Chubb Policy Admin Systems for Personal Lines segment, along with the other legacy Chubb\u2019s disparate policy admin systems like MasterPiece, CAAS & Yacht.\nCreated and documented roadmap for meetings with ACE. Baselined documents used for source to target mapping. Understanding CHUBB's system and applications i.e. Target system for CLIENT INT project.\nDesigned, developed and maintained consistency of the Pre-landing and Landing tables \u201cGeneric Landing Layout\u201d file / sheet that the core execution team leveraged for Source To Target Mapping sessions. \nCreated and documented High Level Data Movement and Data Flow diagrams, High Level & Detailed Conceptual Architecture diagrams using BPMN (Business Process Model and Notation) standards encapsulating every system components within visio flows. Conducted thorough analysis on CPI data models for Subject Of Insurance, Policy, Policy Term, Business Party schemas.\nConfirmed on the mapping performed within \u201cORCA Policy Status Mapping\u201d sheet for \u201cTransaction Status\u201d type and \u201cContract Status\u201d type. Confirmed the data points in \u201cMatch Columns\u201d column for the \u201cMatch Rules\u201d from \u201cClient INT ORCA Match Rules\u201d sheet. Gave an overview on \u201cSystem Triggers\u201d and introduced team on triggers. \nIntegrated Survivorship Rules for 6 Personal Lines Systems. Overview of \u201cSurvivorship Rules\u201d (like \u201cWhat is Survivorship and what does it mean to MDM process & Client UI application?\u201d) and introduce team to \u201cSurvivorship / Survival Rules Matrix\u201d sheet.\nWas responsible for identifying and documenting business rules and creating detailed Use Cases, Use Case models, Use Case diagrams and Object Oriented Analysis and Designs (OOAD) using UML (Unified Modeling Language).\nPrepared ER Diagrams, documented Data Dictionary, identified database schemas, its data elements/fields, and studied their nomenclatures from the identified columns, for phased deliverables. \nEnvironment: Informatica Data Director (IDD), Informatica Analyzer, DB2, Informatica Power Center, MDM, Oracle 11g, Information Builder\u2019s WebFocus 8, IBM Cognos Report Studio 10, IBM Lotus Notes, CICS, Cobol, Oracle SQL Server, MS Visio, MS Office Suite, MasterPiece, Guidewire policy admin system, CAAS, Yacht, PLS, ORCA, DRC, SharePoint.\n\nBed Bath and Beyond, Union, NJ                                          \t\t\t\t\t                   July 2014 \u2013 July 2015\nDesignation: Lead Technical Business System Analyst / Technical Product Manager\nPrime focus of the EXIM/Universal Cart was to have the ability to export products/items from BBBY\u2019s online channels (both desktop & mobile applications) to 3rd party vendor sites that offer personalization and/or customization services (like monogramming, etching, sublimation/printing etc.) not currently available on BBBY\u2019s sites/concepts (concepts like BBBY-US, BBBY-Canada, BuyBuyBaby, Christmas Tree Shops, etc.).\nPerformed requirements gathering & analysis by actively soliciting, examining, investigating and negotiating customer requirements with business directors and Product Development Managers, while leading the technical requirements analysis and driving the system design and construction. \nPerformed extensive SKU to Product relationship mapping at UPC level, SKU/item level, Product level, Collection & Accessories level, etc. Thereby understood cardinality between SSWPs (Single SKU Web Products) and MSWPs (Multi SKU Web Products) and process by which they are \u201cWeb Enabled\u201d and \u201cWeb Disabled\u201d for \u201cDigital Merchandising\u201d.\nConducted numerous impact analysis sessions, thereby analyzing the behavior of multiple features & functionalities on different web pages like PLP (Product List Page \u2013 Grid View & List View), PDP (Product Detail Page), Quick View, Collection & Accessories, Your Cart page, Single Shipping & Multi Shipping Location, Order Confirmation, Order Preview, Order History, Track Order, etc. and on the user CTA (Call to Action) button (buttons like Add to Cart, Add to Registry, Save for Later, Find in Store).\nCreated / Documented, managed, controlled and delivered user stories, features and functionalities in form of Story or Functional Requirements (i.e. formal FRD) within the CCM & RM tools respectively. Updated the tools with appropriate COS (Condition of Satisfaction) / Acceptance Criteria, Story Title, Card, Conversation, Assumptions etc. by following standardized Agile SCRUM methodology/process.\nAlso created / documented, managed, controlled and delivered Epic, Features, Story, Tasks, Spikes by breaking these out appropriately and then tie them back to the concerned Feature and Story within the RM (Requirements Management) tool.\nCollaborated with the development team to enforce the implementation of requirements throughout the entire coding cycle and managed change request using Rational Clear Quest.\nEnvironment: IBM\u2019s Rational Tools, IBM Requirement Management Tool, IBM Change & Configuration Management Tool, RMTrack \u2013 Defect Tracking, JIRA, ATG eCommerce Platform, TIBCO, Riversand\u2019s PIM (Product Information Management), Tableau, QlikView, STIBO\u2019s PDM (Product Data Master), Web2.0, Oracle SQL Server Developer, Microstrategy, Oracle Endeca, UNIX, Java, .Net.\n\nMaryland Department of Health & Mental Hygiene / CSC - CNSI, Baltimore, MD                                  Jan 2013 \u2013 July 2014\nDesignation: Sr. Technical Business System Analyst / BI Reporting Lead\nThe prime objective of the project is to replace the current MMIS Claims processing system for DHMH, with a web-based Service Oriented Solution consistent with MITA 2.0 (Medicaid Information Technology Architecture) guidelines that has online web capabilities for all Users including Providers and Recipients/Beneficiaries. The system\u2019s User Interface intent was to provide the capability for online data entry for Provider Enrollment applications; track and automate Workflow Management of the process; and online verification of Provider Enrollment status.\nReviewed, analyzed, evaluated business processes and associated IT application requirements. Analyzed business workflow and system needs for conversions and migrations; performed data mapping and data conversions.\nIdentified specifications for billing and accounts receivable requirements, performed gap analysis & presented information to technical team to identify system requirements. Worked extensively with developing business rules engine enabling the business rules such as referral, prior authorization, eligibility, claims processing and billing essential.\nInvolved in implementation of HIPAA EDI Transactions (835, 837). Facilitated Electronic Data Interchange (EDI). Performed GAP Analysis for HIPAA 4010 and 5010 transactions. Used EDI tools to verify mapping to X12 format. Recommended changes for system design, methods, procedures, policies and workflows affecting Medicare / Medicaid claims processing in compliance with government compliant processes like HIPAA / EDI formats and accredited standards ANSI. Analyzed HIPAA EDI transactions in X12 responses and of 837, 835, 277CA and 999 and looked for defects.\nRecommended changes for system design, methods, procedures, policies and workflows affecting Medicare/Medicaid claims processing in compliance with government compliant processes like HIPAA/ EDI formats and accredited standards ANSI.\nConducted detail oriented analysis on the current \u201cAS-IS\u201d Maryland MMIS legacy system and driving iterative GAP analysis sessions for the same to propose the effective target \u201cTO-BE\u201d MMIS system.\nDesigned Dashboards/Reports using the Oracle Business Intelligence Analytics platform for requirement analysis and data analytics. Enhanced performance of Reports / Dashboards by implementing the Aggregate tables, Materialized Views, Table partitions, re-building of indexes and managing Cache etc.\nResponsible for processing Medicaid claims in a workflow environment. Accurately interpreted benefit & policy provisions applicable to Medicaid enrollees. Reviewed & resolved claim edits using multiple systems, processes and procedures.\nEnvironment: MMIS / CMS Regulations, HIPAA Privacy, Wire Framing, UML, SCRUM, eCAMS & ProviderOne (CNSI owned & developed Claims Administration System), iLotus Notes, IBM COGNOS 10 Report Studio, Oracle Financials, JIRA (Defect Tracking Tool), ReqTrace Web 2.1, Oracle SQL Developer, J2EE, .NET, Oracle ODBC, Oracle BI Enterprise Edition 11g, MS Office Suite, MS Excel, MS Project, SharePoint, Service Oriented Architecture (SOA), RUP (For MMIS Development), Informatica PowerCenter, Enterprise Architect 10 UML (SPARX System\u2019s EA Visual Modeling Platform).\n\nMedco Health Solutions Inc. / Express Scripts Inc., Franklin Lakes, NJ                                                       Oct 2011 \u2013 Jan 2013\nDesignation: Sr. Technical Business System Analyst\nThe purpose of the project was to report the back-end Teradata IW databases utilizing the front-end Cognos BI reporting tool for the Medicare Part D claims Adjustments & Reconciliation and Operations Workbook business process. This project involved creation of a Medicare, Medicaid Solutions (MMMS) dashboard that included views from various channels, operational divisions and products within the Medicare organization.\nGathered all the Reporting Requirements and functional requirements through JAD sessions, formal interviews and brain storming sessions from the Business Owners and stakeholders based on the project scope and documented it in form of FRDs and Use-Cases in CaliberRM / Optimal Trace.\nConducted JAD sessions with different Business Users to develop new policies and procedures for the Service Catalogue, Charge Capture and Service Worklist /Charge Router, Hospital billing, coding, special coding requirements for Medco Health / Express Scripts and Claim processing.\nInvolved in analysis, design, and implementation of the following systems \u2013 Control Management Reporting and Security Systems: generation of daily, weekly, and monthly reports of hospital patients' activity. Medical Billing & Collection Systems: generation of daily billing activity based on patient records & establishment of collection protocols. Accounting Systems: Financial Statement generation and analysis according to government standards and protocols.\nWorked on EDI transactions: X12, 835, and 837 P.I to identify key data set elements for designated record set. Interacted with Claims, Payments and Enrollment hence analyzing and documenting related business processes.\nInvolved in the System Analysis, Design and Development of the project. Integral part of end-to-end implementation of IW Reporting from capturing reporting requirements, Relational data modeling in Cognos, Metadata analysis, mock-up template designing and BI reporting system.\nDisplayed in depth knowledge of Medicare/Medicaid Claims processes from Admin/Provider/Payer side which were later part of the training program to vendors. Worked on improvement of Claims Reimbursement User Interface for a better experience and incorporate changes as per HIPAA guidelines using the gap analysis.\nIdentified the functional as well as technical needs of the department and accordingly engaged as liaison between the Upper Management and IT Team to develop Cognos based BI Reports, applications and softwares.\nAssisted in development of the reports using Cognos Report Studio and Framework Manager. Developed Forms, Reports and Queries using MS Access and Excel (including Pivot tables, Vlookup and Macros).\nEnvironment: SCRUM, RUP, SharePoint, Synon/2E, COBOL, IBM AS/400, IBM Cognos 10 (Report Studio, Query Studio, Framework Manager, Virtual View Manager, Business Insight Dashboard), UNIX, Microsoft SQL Server 2008, T-SQL, MS SQL Server, Teradata IW, BRIO Query Intelligence, MS Office Suite, MS Project, MS Visio, ERWIN, OiM (Oracle Identity Manager), ORACLE SQL Server Developer, Teradata, DB2, Informatica Powercenter, CA ERwin Data Modeler, Oracle BI Enterprise Edition 10g, PL/SQL, CAs ALL Fusion Harvest, HP Quality Center, EDI, XML, .CSV (Comma Delimited), Rational Clear Quest, Anchor DW, Foundation14 (F14), CaliberRM 64Bit Production, Optimal Trace, MagicDraw UML.\n\nCVS Caremark, Richardson, TX                                                                                   \t\t\t      Jan 2010 \u2013 Oct 2011\nDesignation: Sr. Technical Business Analyst\nThe primary goal of the project is to extract common services such as Eligibility, Formulary, Drug Maintenance, etc. out of the disparate systems and host them independently to facilitate economy of operations, isolation of common business services from core adjudication transaction processing and externalize the data in a way that can be consumed by other external applications within the Organization. This will enable common services, tools and interfaces that can improve client experience and drive consistency regardless of which adjudication engine is used.\nGathered, analyzed, documented business and technical requirements from both formal and informal sessions and validate the needs of the business stakeholders, thereby drafting the Technical Design Document (TDD) and System Specification Document (SSD).\nDesigned and developed Use Cases and Use Case scenarios, Activity Diagrams, Sequence Diagrams, High Level and Low Level Process Flow Diagrams, OOAD using UML and Business Process Modeling. Understood client\u2019s business needs related to operational payer departments (i.e. claims, enrollment, billing, etc.).\nProfiled data in the sources prior to defining the data cleaning rules. Perform small enhancements (data cleansing/data quality). Worked on various Professional billing and Hospital billing products.\nDesigned and implemented unique platform, which collected and synchronized information of each person all in one place, including medical claims, lab results, self reported data and other relevant information and also in an efficient and effective manner.\nResponsible for preparing Business Requirement Document (BRD), Functional Requirement Document (FRD) and then translating into functional specifications and test plans. Closely coordinated with both business users and developers for arriving at a mutually acceptable solution.\nDeveloped project definitions, project scope, cost/benefit and risk analysis, work plans, daily and weekly progress reports, and presentations. Created and tested scripts for the premium calculations and claim limits and deductible.\nReviewed the use cases, functions and features list, map the requirements to design, and update the Requirements Traceability Matrix (RTM). Created various reports such as billing payment reports, Billing Grouping Payment and discount reports.\nResponsible to give the payers the clear vision of claim life cycle from submission to CVS Caremark through payer adjunction. Designed & implemented a web based claims processing system & management application to administer and process health insurance claims automatically. It connected the organization to largest all-payer network of commercial & government health plans nationwide to provide wealth of real-time patient benefit information.\nCreated source table definitions in the Data Stage Repository by studying the data sources by importing the data from Mainframe.\nWrote PL/SQL statement and stored procedures in Oracle for extracting as well as writing data. Created and executed claims, enrollment and/or billing test scenarios, including defect tracking.\nEnvironment: RUP, SharePoint, Synon/2E, COBOL, IBM AS/400, CICS, Citrix, Cognos 8, DB2, SAP R/3 FICO, Microsoft SQL Server 2008, T-SQL, SSIS, SSRS, SSAS, MS Excel, MS Word, MS Access, MS Project, MS Visio, ERWIN, Data Stage, OiM (Oracle Identity Manager), Oracle, PL/SQL, PC Media, Oracle BI Enterprise Edition 10g, CAs ALL Fusion Harvest, EDIFECS, HP Quality Center, Velocedi/Claredi, EDI, XML, DIAMOND, COSMOS, FACETS, LOGISTICARE, Rational Rose, Bugzilla, Snagit, Adobe Acrobat Professional, HP Quality Center.\n\nDelta Technologies                                                                                             \t\t\t     July 2006 \u2013 Aug 2008\nDesignation: Lead Technical Business Analyst                                                                                                                   \nThe project was to report corporate data warehouse for online trading system. The reporting system was to capture the KPI such as the trade volumes, number of trades, on the counter trades, and exchange trades, broker dealer trades, trade fees with respect to dimensions such as customer\u2019s location, security type, exchange traded Fees payables, etc.\nGathered the business requirements from the managers through JAD sessions, formal interviews and surveys and worked extensively with the users and with different levels of management to identify requirements, business events to develop functional specifications.\nAnalyzed the AS-IS and TO-BE processes to understand the key findings, the short term considerations, the long term considerations and its benefits.\nOwned the entire reporting process. Interacted with the ETL team, developer(s), management, and account holders to get the requirements, document them, design templates, and write specifications.\nIdentified data elements from the source systems, performed data analysis to come up with data cleansing and integration rules for the ETL process using Informatica.\nAnalyzed research on operational procedures and methods and recommended changes for improvement, with an emphasis on automation and efficiency.\nPerformed analysis and design of applications using OOAD techniques, UML and Design Patterns.\nGathered all the requirements from the stakeholders based on the project scope and documented it in RequisitePro.\nConducted interviews with various business users and Subject Matter Experts (SMEs) to collect Requirements and business process information, using Requisite Pro exercised all the different types of views in Requisite Pro Attribute matrix.\nInvolved in the testing phase right from the Unit testing to the User Acceptance testing.\nDocumented existing and proposed process flow, analyzed current and target system, and conducted GAP analysis from the reporting requirements to the existing data in ODS.\nEnvironment: RUP, Requisite Pro, Data Stage, Rational Tools, Cognos 8, MS Project, MS Word, MS Excel, MS Visio, MS Office Suite, Rational Rose, Windows Vista, ERWIN, MS SQL Server, PL/SQL, SSIS, SSRS, SSAS, Crystal Reports XI.", "years_experience": 10, "score": 0.3367810583086025, "path": "C:\\Users\\admin\\AI-Resume-Ranker\\data\\uploads\\Candidate66_resume_3.docx"}, {"filename": "Candidate66_resume_2.docx", "text": "Candidate66\nPhone: 201-532-6397. Email: Candidate66@gmail.com\n__________________________________________________________________________________________________\nProfessional Summary\nOverall 10 years of experience as Sr. Technical Business System Analyst (Technical BSA) in Information Technology industry with focus on Data Analysis, Information & Data Management, Service-Oriented Architecture (SOA), Data Mapping & Data Modeling, Business Process Improvement, System Analysis & Design; expertise in implementation of IT projects using Project Management methodologies. Extensive experience in diversified industry sectors including Healthcare, Life Insurance, Personal & Commercial Insurance, Property & Casualty Insurance, Telecommunication, Retail & e-Commerce (Web Merchandising), Commercial & Investment Banking and Credit Card services.\nStrong skills in Data Warehouse Design using Star Schema roll out for the fact and dimension tables. Design of cubes, partitions and aggregations with excellent knowledge of Type 2 Data modeling.\nExpertise in application development with Full Lifecycle implementation of large Data Warehouse including Project Scope, requirements gathering, Star Schema Design, Snowflake Schema, Data Modeling and ODS.\nStrong experience in Data Analysis, Data Migration, Data Cleansing, Transformation, Integration, Data Import, and Data Export through the use of multiple ETL tools such as Informatica PowerCenter.\nStrong practical knowledge of analytical techniques and methodologies such as machine learning/supervised and unsupervised techniques, segmentation, mix and time series modeling, response modeling, lift modeling, experimental design, neural networks, data mining and optimization techniques. Expert knowledge of statistical analysis tools such as R-programming, MATLAB, Spark, SAS (Statistical Analysis System), R/Spark, Apache Hadoop and Cassandra.\nStrong background in applying statistical machine learning techniques to predictive modeling and experience with Machine Learning libraries (via R, H2O, Python, Spark, etc.). Proficiency in programming in Python, R-programming, SQL, JavaScript, Java/Scala/Ruby and shell scripting. Proficiency in consuming REST based API (with JSON payload). Fluency in big data platforms including Apache Hadoop, Apache MapReduce, Hive, Spark, and Pig. Familiarity with Cloud based HaaS/PaaS solutions such as AWS EMR (Amazon Web Services Elastic MapReduce), MS Azure. Strong understanding of data profiling and data cleansing techniques.\nThorough knowledge of eCommerce architecture; ATG framework its functional limitations and the platform features, application capabilities, etc. including industry wide terminologies like B2B, B2C, C2B, C2C with its eCommerce categories and strong knowledge of PCI DSS (Payment Card Industry Data Security Standards) standards.\nTechnical expertise in handling large-scale platform architecture, integrations, development using Java/J2EE Technologies. Experience on Oracle Commerce includes ATG Commerce, ATG Multisite, ATG Merchandising, and Content Management working in some of the key areas such as the Business Control Center (BCC), Profile Management, Shopping Cart, Checkout, Pricing, performance tuning, caching, infrastructure setup and Endeca.\u00a0\nStrong knowledge of application and systems performance improvement techniques, integration technologies, approaches and patterns, Web APIs / Web Service API calls (SoapUI / SOAP, RESTful API, SOA, XML, HTTP like GET / PUT / POST / DELETE, JSON, BitTorrent, Web 2.0, etc.). Strong knowledge of security aspects like risks and threats, authentication, authorization, certificates, encryption. Experience in utilizing frameworks and re-usable components.\nProficient in using Business Analysis tools such as MS Visio, Word, Excel (including Pivot tables, Macros and Vlookup), Rational Requiste Pro, ReqTrace Web, Caliber RM, Optimal Trace, ClearCase and ClearQuest.\nProficiency in elaborating the Use cases, writing Test cases and preparing Requirement Traceability Matrices (RTM).\nEducation: Master of Science in Engineering \u2013 New Jersey Institute of Technology (August 2008 to December 2009).\nCertification: Certified Data Scientist i.e. Data Science Professional (from John Hopkins University).\nTechnical Skills: Windows 95/98/NT/2000/XP/Vista/7, MS Visio, MS Access, MS Word, MS Excel, MS Project, Rational Rose Enterprise, IBM Rational Software Architect, Microsoft Project, Rational Requisite Pro, MS SQL Server 7.0/2000, PL/SQL, CSS (Cascading Style Sheet), OLTP & OLAP, Synon/2E, COBOL, IBM AS/400, CICS, Citrix, DB2, SAP ERP R/3 MMWM, SAP R/3 FICO, ERWIN Data Modeler 4.2/4.1/4.0, Business Objects XI r3, r2, r1, 6.5, 6.1, 5.x, 4.x, Crystal Reports XI, Informatica, HTML, Dream Weaver, MS FrontPage, IBM COGNOS (Report Studio, Query Studio, Framework Manager, Virtual View Manager, Business Insight Dashboard), Oracle BI Enterprise Edition (OBIEE), C#, C++, R Programming Language (Version 3.2.5), Minitab, XML, XSD, WSDL, X12, SAP PowerDesigner, SAP PowerBuilder, Analysis of Variance (ANOVA) and Experimental Design, Total Quality Management and Lean Six Sigma Manufacturing Methodologies, SAFe (Scaled Agile Framework) Practitioner, PMI\u2019s PMBOK guidelines & standards, TOGAF (The Open Group Architecture Framework), IIBA\u2019s BABOK guidelines & standards, Business Architecture Guild\u2019s BIZBOK (Business Architecture Body of Knowledge) standards & guidelines, Enterprise Architecture Body of Knowledge (EABOK), Enterprise Information Architecture (EIA) framework.\n\nProfessional Experience\nFidelity Investments (FMR) Smithfield, RI                               \t\t\t                                     Dec 2016 \u2013 Current\nDesignation: Principal Program Analyst / Technical Project Architect\nPrimary objective of the project was to address and resolve corporate Audit issue (Tier C) finding for the Enterprise Cybersecurity group on highly confidential Client Credentials (comprised of PII data) exposed into the server logs written by various Internal (employees within each business units, Broker \u2013 Dealer apps, 3rd party vendor apps) & External (customer / clients) Fidelity hosted applications, thereby applying log masking logic, data redaction techniques on these exposed keys within server logs physically located on multiple servers.\nDelivered elite level security solutions (for its Enterprise Cybersecurity business group) which essentially protects Fidelity and its customer's valuable information. Interacted with stakeholders, facilitated multiple fusion sessions and delivered the end results of work that provide the necessary requirements to the delivery team which thereby built and supported technology solutions.\nWorked as part of the Customer Protection Program, directly with Program and Project Managers as well as technology and business partners.\nWorked with tools such as Jira and Confluence to provide transparency of scope. Worked with our delivery team(s) including QA Data Services and engineering to ensure that we are meeting the needs of our stakeholders plus delivered solid, quality products to production. \nSupported Program and Project managers including providing input to/creation of strategy and execution plans and input to project status. Point person to escalate risks, change requests and participated in corrective actions (path to green).\nPerformed server logs analysis on Linux and Unix servers used by enterprise wide business units, thereby leveraged Splunk GUI tool to read and extrapolate pertinent meta data from these server logs. Used tools like Jira, ITEC PPMC, Confluence, etc. to track, monitor, delegate tasks / activities, operate different project aspects and functions.\nDerived insights out of ambiguity \u2013 understood, processed and interpreted complex data sets. Analyzed complex business data & identified patterns in the data using algorithms from statistics and machine learning. Developed and improved algorithms and methodologies to handle, browse, process and visualize structured and unstructured data.\nTranslated Business requirements, business rules and business criteria to system specifications and system requirements thereby preparing SRS (System Requirement Specifications) and PRS (Performance Requirement Specifications).\nEnvironment: MS Office Suite Package, MS Visio 2013 Pro, MS Excel Pivot Tables, MS Excel Functions & Formulas, MS Excel VLookUp, MS Excel Macros, Splunk Enterprise 6.5 (Splunk GUI Tool), Oracle 11g, IBM DB2, CyberArk EIAM (Enterprise Identity & Access Management), IBM InfoSphere Optim Data Privacy, ServiceNow, VMWare Tools, Oracle 11g Client 11.2.0.2 r01.02, Quest TOAD for Oracle, MS Access, Informatica Power Center, MS SQL Server, ITEC (IT Enablement Center) ALM (Application LifeCycle Management), Atlassian Confluence, ITEC (IT Enablement Center) Atlassian JIRA, Document Central, IBM DB2 Advanced Recovery Expert, IBM DB2 Merge BackUp, HP ITEC (IT Enablement Center) PPMC (Project & Portfolio Management Center).\n\nVerizon Wireless, Warren, NJ                               \t  \t\t\t\t\t\t    May 2016 \u2013 Dec 2016\nDesignation: Sr. Technical Business System Analyst / Technical Project Manager\nThe prime objective of the IoT (Internet Of Things), ReSale, M2M (Machine to Machine) portfolio was to utilize the core capabilities within MCS (Mobile Content Solutions) application suite module in order to web merchandise the non-tangible (like Licenses and Professional Services components within a cart) Items / SKUs through frontend Omni-channels; thereby foster & grow B2B (Business To Business), B2C (Business To Customer) business models from enterprise\u2019s strategic viewpoint and generate billing events at Account / Customer level from backend system application.\nAuthored, owned, managed and maintained 1Pagers to give brief overview of the products / services to be created and maintained in our MCS / SCM portal. Facilitated meeting sessions with Product Managers, Product Owners, Architectures and Analyst to hash out visio IoT (Internet of Things) Vendor flows, Order Fulfillment Flows and Order Processing Flows.\nPlanned, created, designed, managed and baselined draft high-level Project Scope, In-Scope & Out-Scope Functionalities, Draft SLAs (for 3rd party vendors / partners), Baseline Project Timelines & Gantt Chart at project implementation level with external 3rd party vendors / partners (in order to accomplish System Integration Testing [SIT] milestone) system application.\nDefined, designed and elicited core business objectives in order to deliver qualitative product, program and project with optimum utilization and the most efficient use of enterprise resources.\nManaged project charter, project plans, scope statement, project execution, risk mitigation, contingency plan and achievement of project and program objectives. Managed SIT & UAT test activities by ensuring defects are logged accurately in JIRA and issues resolved appropriately.\nCreated, aligned and logged EPICS, User Stories / Features, Tasks (Development, Infrastructure & QA Tasks), Bugs, Defects, BRs (Build Requests), DRs (Deployment Requests), Issues, etc. in JIRA tool.\nStrong knowledge of Unix commands and running those Unix commands in various boxes to validate server logs.\nEffectively and lucidly communicated project risks, issues (especially launch gating), dependencies, progress status, launches, concerns and retrospectives in structured RAID template designed and choreographed by me.\nEnvironment: Activiti BPM, XML Spy, RestFUL API, SoapUI, HP Quality Center ALM (Application Lifecycle Management) 12.20 & 12.01, Quest Toad For Oracle 12.0 (64bit), Programmer\u2019s Notepad, PuTTY.exe, JIRA, CA\u2019s Clarity PPM, Cloudsite SharePoint, Confluence document repository, APEX Search Client RealTime application, DB2 AppDevClient 9.7, MS Office Project Standard 2007, MS Office Standard 2007, MS Office Visio Standard, Open Source XML tools, Oracle Client 11g R2 11.2.0.1.0, Techsmith\u2019s SnagIT Editor 12, SQL Developer 4.1.3.\n\nChubb Insurance / Chubb Group of Insurance Companies, Whitehouse Station, NJ                            July 2015 \u2013 May 2016\nDesignation: Sr. Technical Business System Analyst / Project Architect\nThe purpose of the Client Integration initiative is to be able to view and manage PRS\u2019 business at the client / household / account level in an integrated fashion so that we can make better and faster underwriting decisions, more effectively market and communicate with existing and potential clients, price across clients and deliver better customer service. Integrate PLS & ORCA into the Chubb Client MDM/UI in order to provide an integrated household policy portfolio view across key Chubb Policy Admin Systems for Personal Lines segment, along with the other legacy Chubb\u2019s disparate policy admin systems like MasterPiece, CAAS & Yacht.\nCreated and documented roadmap for meetings with ACE. Baselined documents used for source to target mapping. Understanding CHUBB's system and applications i.e. Target system for CLIENT INT project.\nDesigned, developed and maintained consistency of the Pre-landing and Landing tables \u201cGeneric Landing Layout\u201d file / sheet that the core execution team leveraged for Source To Target Mapping sessions. \nCreated and documented High Level Data Movement and Data Flow diagrams, High Level & Detailed Conceptual Architecture diagrams using BPMN (Business Process Model and Notation) standards encapsulating every system components within visio flows. Conducted thorough analysis on CPI data models for Subject Of Insurance, Policy, Policy Term, Business Party schemas.\nConfirmed on the mapping performed within \u201cORCA Policy Status Mapping\u201d sheet for \u201cTransaction Status\u201d type and \u201cContract Status\u201d type. Confirmed the data points in \u201cMatch Columns\u201d column for the \u201cMatch Rules\u201d from \u201cClient INT ORCA Match Rules\u201d sheet. Gave an overview on \u201cSystem Triggers\u201d and introduced team on triggers. \nIntegrated Survivorship Rules for 6 Personal Lines Systems. Overview of \u201cSurvivorship Rules\u201d (like \u201cWhat is Survivorship and what does it mean to MDM process & Client UI application?\u201d) and introduce team to \u201cSurvivorship / Survival Rules Matrix\u201d sheet.\nWas responsible for identifying and documenting business rules and creating detailed Use Cases, Use Case models, Use Case diagrams and Object Oriented Analysis and Designs (OOAD) using UML (Unified Modeling Language).\nPrepared ER Diagrams, documented Data Dictionary, identified database schemas, its data elements/fields, and studied their nomenclatures from the identified columns, for phased deliverables. \nEnvironment: Informatica Data Director (IDD), Informatica Analyzer, DB2, Informatica Power Center, MDM, Oracle 11g, Information Builder\u2019s WebFocus 8, IBM Cognos Report Studio 10, IBM Lotus Notes, CICS, Cobol, Oracle SQL Server, MS Visio, MS Office Suite, MasterPiece, Guidewire policy admin system, CAAS, Yacht, PLS, ORCA, DRC, SharePoint.\n\nBed Bath and Beyond, Union, NJ                                          \t\t\t\t\t                   July 2014 \u2013 July 2015\nDesignation: Lead Technical Business System Analyst / Technical Product Manager\nPrime focus of the EXIM/Universal Cart was to have the ability to export products/items from BBBY\u2019s online channels (both desktop & mobile applications) to 3rd party vendor sites that offer personalization and/or customization services (like monogramming, etching, sublimation/printing etc.) not currently available on BBBY\u2019s sites/concepts (concepts like BBBY-US, BBBY-Canada, BuyBuyBaby, Christmas Tree Shops, etc.).\nPerformed requirements gathering & analysis by actively soliciting, examining, investigating and negotiating customer requirements with business directors and Product Development Managers, while leading the technical requirements analysis and driving the system design and construction. \nPerformed extensive SKU to Product relationship mapping at UPC level, SKU/item level, Product level, Collection & Accessories level, etc. Thereby understood cardinality between SSWPs (Single SKU Web Products) and MSWPs (Multi SKU Web Products) and process by which they are \u201cWeb Enabled\u201d and \u201cWeb Disabled\u201d for \u201cDigital Merchandising\u201d.\nConducted numerous impact analysis sessions, thereby analyzing the behavior of multiple features & functionalities on different web pages like PLP (Product List Page \u2013 Grid View & List View), PDP (Product Detail Page), Quick View, Collection & Accessories, Your Cart page, Single Shipping & Multi Shipping Location, Order Confirmation, Order Preview, Order History, Track Order, etc. and on the user CTA (Call to Action) button (buttons like Add to Cart, Add to Registry, Save for Later, Find in Store).\nCreated / Documented, managed, controlled and delivered user stories, features and functionalities in form of Story or Functional Requirements (i.e. formal FRD) within the CCM & RM tools respectively. Updated the tools with appropriate COS (Condition of Satisfaction) / Acceptance Criteria, Story Title, Card, Conversation, Assumptions etc. by following standardized Agile SCRUM methodology/process.\nAlso created / documented, managed, controlled and delivered Epic, Features, Story, Tasks, Spikes by breaking these out appropriately and then tie them back to the concerned Feature and Story within the RM (Requirements Management) tool.\nCollaborated with the development team to enforce the implementation of requirements throughout the entire coding cycle and managed change request using Rational Clear Quest.\nEnvironment: IBM\u2019s Rational Tools, IBM Requirement Management Tool, IBM Change & Configuration Management Tool, RMTrack \u2013 Defect Tracking, JIRA, ATG eCommerce Platform, TIBCO, Riversand\u2019s PIM (Product Information Management), Tableau, QlikView, STIBO\u2019s PDM (Product Data Master), Web2.0, Oracle SQL Server Developer, Microstrategy, Oracle Endeca, UNIX, Java, .Net.\n\nMaryland Department of Health & Mental Hygiene / CSC - CNSI, Baltimore, MD                                  Jan 2013 \u2013 July 2014\nDesignation: Sr. Technical Business System Analyst / BI Reporting Lead\nThe prime objective of the project is to replace the current MMIS Claims processing system for DHMH, with a web-based Service Oriented Solution consistent with MITA 2.0 (Medicaid Information Technology Architecture) guidelines that has online web capabilities for all Users including Providers and Recipients/Beneficiaries. The system\u2019s User Interface intent was to provide the capability for online data entry for Provider Enrollment applications; track and automate Workflow Management of the process; and online verification of Provider Enrollment status.\nReviewed, analyzed, evaluated business processes and associated IT application requirements. Analyzed business workflow and system needs for conversions and migrations; performed data mapping and data conversions.\nIdentified specifications for billing and accounts receivable requirements, performed gap analysis & presented information to technical team to identify system requirements. Worked extensively with developing business rules engine enabling the business rules such as referral, prior authorization, eligibility, claims processing and billing essential.\nInvolved in implementation of HIPAA EDI Transactions (835, 837). Facilitated Electronic Data Interchange (EDI). Performed GAP Analysis for HIPAA 4010 and 5010 transactions. Used EDI tools to verify mapping to X12 format. Recommended changes for system design, methods, procedures, policies and workflows affecting Medicare / Medicaid claims processing in compliance with government compliant processes like HIPAA / EDI formats and accredited standards ANSI. Analyzed HIPAA EDI transactions in X12 responses and of 837, 835, 277CA and 999 and looked for defects.\nRecommended changes for system design, methods, procedures, policies and workflows affecting Medicare/Medicaid claims processing in compliance with government compliant processes like HIPAA/ EDI formats and accredited standards ANSI.\nConducted detail oriented analysis on the current \u201cAS-IS\u201d Maryland MMIS legacy system and driving iterative GAP analysis sessions for the same to propose the effective target \u201cTO-BE\u201d MMIS system.\nDesigned Dashboards/Reports using the Oracle Business Intelligence Analytics platform for requirement analysis and data analytics. Enhanced performance of Reports / Dashboards by implementing the Aggregate tables, Materialized Views, Table partitions, re-building of indexes and managing Cache etc.\nResponsible for processing Medicaid claims in a workflow environment. Accurately interpreted benefit & policy provisions applicable to Medicaid enrollees. Reviewed & resolved claim edits using multiple systems, processes and procedures.\nEnvironment: MMIS / CMS Regulations, HIPAA Privacy, Wire Framing, UML, SCRUM, eCAMS & ProviderOne (CNSI owned & developed Claims Administration System), iLotus Notes, IBM COGNOS 10 Report Studio, Oracle Financials, JIRA (Defect Tracking Tool), ReqTrace Web 2.1, Oracle SQL Developer, J2EE, .NET, Oracle ODBC, Oracle BI Enterprise Edition 11g, MS Office Suite, MS Excel, MS Project, SharePoint, Service Oriented Architecture (SOA), RUP (For MMIS Development), Informatica PowerCenter, Enterprise Architect 10 UML (SPARX System\u2019s EA Visual Modeling Platform).\n\nMedco Health Solutions Inc. / Express Scripts Inc., Franklin Lakes, NJ                                                       Oct 2011 \u2013 Jan 2013\nDesignation: Sr. Technical Business System Analyst\nThe purpose of the project was to report the back-end Teradata IW databases utilizing the front-end Cognos BI reporting tool for the Medicare Part D claims Adjustments & Reconciliation and Operations Workbook business process. This project involved creation of a Medicare, Medicaid Solutions (MMMS) dashboard that included views from various channels, operational divisions and products within the Medicare organization.\nGathered all the Reporting Requirements and functional requirements through JAD sessions, formal interviews and brain storming sessions from the Business Owners and stakeholders based on the project scope and documented it in form of FRDs and Use-Cases in CaliberRM / Optimal Trace.\nConducted JAD sessions with different Business Users to develop new policies and procedures for the Service Catalogue, Charge Capture and Service Worklist /Charge Router, Hospital billing, coding, special coding requirements for Medco Health / Express Scripts and Claim processing.\nInvolved in analysis, design, and implementation of the following systems \u2013 Control Management Reporting and Security Systems: generation of daily, weekly, and monthly reports of hospital patients' activity. Medical Billing & Collection Systems: generation of daily billing activity based on patient records & establishment of collection protocols. Accounting Systems: Financial Statement generation and analysis according to government standards and protocols.\nWorked on EDI transactions: X12, 835, and 837 P.I to identify key data set elements for designated record set. Interacted with Claims, Payments and Enrollment hence analyzing and documenting related business processes.\nInvolved in the System Analysis, Design and Development of the project. Integral part of end-to-end implementation of IW Reporting from capturing reporting requirements, Relational data modeling in Cognos, Metadata analysis, mock-up template designing and BI reporting system.\nDisplayed in depth knowledge of Medicare/Medicaid Claims processes from Admin/Provider/Payer side which were later part of the training program to vendors. Worked on improvement of Claims Reimbursement User Interface for a better experience and incorporate changes as per HIPAA guidelines using the gap analysis.\nIdentified the functional as well as technical needs of the department and accordingly engaged as liaison between the Upper Management and IT Team to develop Cognos based BI Reports, applications and softwares.\nAssisted in development of the reports using Cognos Report Studio and Framework Manager. Developed Forms, Reports and Queries using MS Access and Excel (including Pivot tables, Vlookup and Macros).\nEnvironment: SCRUM, RUP, SharePoint, Synon/2E, COBOL, IBM AS/400, IBM Cognos 10 (Report Studio, Query Studio, Framework Manager, Virtual View Manager, Business Insight Dashboard), UNIX, Microsoft SQL Server 2008, T-SQL, MS SQL Server, Teradata IW, BRIO Query Intelligence, MS Office Suite, MS Project, MS Visio, ERWIN, OiM (Oracle Identity Manager), ORACLE SQL Server Developer, Teradata, DB2, Informatica Powercenter, CA ERwin Data Modeler, Oracle BI Enterprise Edition 10g, PL/SQL, CAs ALL Fusion Harvest, HP Quality Center, EDI, XML, .CSV (Comma Delimited), Rational Clear Quest, Anchor DW, Foundation14 (F14), CaliberRM 64Bit Production, Optimal Trace, MagicDraw UML.\n\nCVS Caremark, Richardson, TX                                                                                   \t\t\t      Jan 2010 \u2013 Oct 2011\nDesignation: Sr. Technical Business Analyst\nThe primary goal of the project is to extract common services such as Eligibility, Formulary, Drug Maintenance, etc. out of the disparate systems and host them independently to facilitate economy of operations, isolation of common business services from core adjudication transaction processing and externalize the data in a way that can be consumed by other external applications within the Organization. This will enable common services, tools and interfaces that can improve client experience and drive consistency regardless of which adjudication engine is used.\nGathered, analyzed, documented business and technical requirements from both formal and informal sessions and validate the needs of the business stakeholders, thereby drafting the Technical Design Document (TDD) and System Specification Document (SSD).\nDesigned and developed Use Cases and Use Case scenarios, Activity Diagrams, Sequence Diagrams, High Level and Low Level Process Flow Diagrams, OOAD using UML and Business Process Modeling. Understood client\u2019s business needs related to operational payer departments (i.e. claims, enrollment, billing, etc.).\nProfiled data in the sources prior to defining the data cleaning rules. Perform small enhancements (data cleansing/data quality). Worked on various Professional billing and Hospital billing products.\nDesigned and implemented unique platform, which collected and synchronized information of each person all in one place, including medical claims, lab results, self reported data and other relevant information and also in an efficient and effective manner.\nResponsible for preparing Business Requirement Document (BRD), Functional Requirement Document (FRD) and then translating into functional specifications and test plans. Closely coordinated with both business users and developers for arriving at a mutually acceptable solution.\nDeveloped project definitions, project scope, cost/benefit and risk analysis, work plans, daily and weekly progress reports, and presentations. Created and tested scripts for the premium calculations and claim limits and deductible.\nReviewed the use cases, functions and features list, map the requirements to design, and update the Requirements Traceability Matrix (RTM). Created various reports such as billing payment reports, Billing Grouping Payment and discount reports.\nResponsible to give the payers the clear vision of claim life cycle from submission to CVS Caremark through payer adjunction. Designed & implemented a web based claims processing system & management application to administer and process health insurance claims automatically. It connected the organization to largest all-payer network of commercial & government health plans nationwide to provide wealth of real-time patient benefit information.\nCreated source table definitions in the Data Stage Repository by studying the data sources by importing the data from Mainframe.\nWrote PL/SQL statement and stored procedures in Oracle for extracting as well as writing data. Created and executed claims, enrollment and/or billing test scenarios, including defect tracking.\nEnvironment: RUP, SharePoint, Synon/2E, COBOL, IBM AS/400, CICS, Citrix, Cognos 8, DB2, SAP R/3 FICO, Microsoft SQL Server 2008, T-SQL, SSIS, SSRS, SSAS, MS Excel, MS Word, MS Access, MS Project, MS Visio, ERWIN, Data Stage, OiM (Oracle Identity Manager), Oracle, PL/SQL, PC Media, Oracle BI Enterprise Edition 10g, CAs ALL Fusion Harvest, EDIFECS, HP Quality Center, Velocedi/Claredi, EDI, XML, DIAMOND, COSMOS, FACETS, LOGISTICARE, Rational Rose, Bugzilla, Snagit, Adobe Acrobat Professional, HP Quality Center.\n\nDelta Technologies                                                                                             \t\t\t     July 2006 \u2013 Aug 2008\nDesignation: Lead Technical Business Analyst                                                                                                                   \nThe project was to report corporate data warehouse for online trading system. The reporting system was to capture the KPI such as the trade volumes, number of trades, on the counter trades, and exchange trades, broker dealer trades, trade fees with respect to dimensions such as customer\u2019s location, security type, exchange traded Fees payables, etc.\nGathered the business requirements from the managers through JAD sessions, formal interviews and surveys and worked extensively with the users and with different levels of management to identify requirements, business events to develop functional specifications.\nAnalyzed the AS-IS and TO-BE processes to understand the key findings, the short term considerations, the long term considerations and its benefits.\nOwned the entire reporting process. Interacted with the ETL team, developer(s), management, and account holders to get the requirements, document them, design templates, and write specifications.\nIdentified data elements from the source systems, performed data analysis to come up with data cleansing and integration rules for the ETL process using Informatica.\nAnalyzed research on operational procedures and methods and recommended changes for improvement, with an emphasis on automation and efficiency.\nPerformed analysis and design of applications using OOAD techniques, UML and Design Patterns.\nGathered all the requirements from the stakeholders based on the project scope and documented it in RequisitePro.\nConducted interviews with various business users and Subject Matter Experts (SMEs) to collect Requirements and business process information, using Requisite Pro exercised all the different types of views in Requisite Pro Attribute matrix.\nInvolved in the testing phase right from the Unit testing to the User Acceptance testing.\nDocumented existing and proposed process flow, analyzed current and target system, and conducted GAP analysis from the reporting requirements to the existing data in ODS.\nEnvironment: RUP, Requisite Pro, Data Stage, Rational Tools, Cognos 8, MS Project, MS Word, MS Excel, MS Visio, MS Office Suite, Rational Rose, Windows Vista, ERWIN, MS SQL Server, PL/SQL, SSIS, SSRS, SSAS, Crystal Reports XI.", "years_experience": 10, "score": 0.3367810583086025, "path": "C:\\Users\\admin\\AI-Resume-Ranker\\data\\uploads\\Candidate66_resume_2.docx"}, {"filename": "Candidate66_resume_1.docx", "text": "Candidate66\nPhone: 201-532-6397. Email: Candidate66@gmail.com\n__________________________________________________________________________________________________\nProfessional Summary\nOverall 10 years of experience as Sr. Technical Business System Analyst (Technical BSA) in Information Technology industry with focus on Data Analysis, Information & Data Management, Service-Oriented Architecture (SOA), Data Mapping & Data Modeling, Business Process Improvement, System Analysis & Design; expertise in implementation of IT projects using Project Management methodologies. Extensive experience in diversified industry sectors including Healthcare, Life Insurance, Personal & Commercial Insurance, Property & Casualty Insurance, Telecommunication, Retail & e-Commerce (Web Merchandising), Commercial & Investment Banking and Credit Card services.\nStrong skills in Data Warehouse Design using Star Schema roll out for the fact and dimension tables. Design of cubes, partitions and aggregations with excellent knowledge of Type 2 Data modeling.\nExpertise in application development with Full Lifecycle implementation of large Data Warehouse including Project Scope, requirements gathering, Star Schema Design, Snowflake Schema, Data Modeling and ODS.\nStrong experience in Data Analysis, Data Migration, Data Cleansing, Transformation, Integration, Data Import, and Data Export through the use of multiple ETL tools such as Informatica PowerCenter.\nStrong practical knowledge of analytical techniques and methodologies such as machine learning/supervised and unsupervised techniques, segmentation, mix and time series modeling, response modeling, lift modeling, experimental design, neural networks, data mining and optimization techniques. Expert knowledge of statistical analysis tools such as R-programming, MATLAB, Spark, SAS (Statistical Analysis System), R/Spark, Apache Hadoop and Cassandra.\nStrong background in applying statistical machine learning techniques to predictive modeling and experience with Machine Learning libraries (via R, H2O, Python, Spark, etc.). Proficiency in programming in Python, R-programming, SQL, JavaScript, Java/Scala/Ruby and shell scripting. Proficiency in consuming REST based API (with JSON payload). Fluency in big data platforms including Apache Hadoop, Apache MapReduce, Hive, Spark, and Pig. Familiarity with Cloud based HaaS/PaaS solutions such as AWS EMR (Amazon Web Services Elastic MapReduce), MS Azure. Strong understanding of data profiling and data cleansing techniques.\nThorough knowledge of eCommerce architecture; ATG framework its functional limitations and the platform features, application capabilities, etc. including industry wide terminologies like B2B, B2C, C2B, C2C with its eCommerce categories and strong knowledge of PCI DSS (Payment Card Industry Data Security Standards) standards.\nTechnical expertise in handling large-scale platform architecture, integrations, development using Java/J2EE Technologies. Experience on Oracle Commerce includes ATG Commerce, ATG Multisite, ATG Merchandising, and Content Management working in some of the key areas such as the Business Control Center (BCC), Profile Management, Shopping Cart, Checkout, Pricing, performance tuning, caching, infrastructure setup and Endeca.\u00a0\nStrong knowledge of application and systems performance improvement techniques, integration technologies, approaches and patterns, Web APIs / Web Service API calls (SoapUI / SOAP, RESTful API, SOA, XML, HTTP like GET / PUT / POST / DELETE, JSON, BitTorrent, Web 2.0, etc.). Strong knowledge of security aspects like risks and threats, authentication, authorization, certificates, encryption. Experience in utilizing frameworks and re-usable components.\nProficient in using Business Analysis tools such as MS Visio, Word, Excel (including Pivot tables, Macros and Vlookup), Rational Requiste Pro, ReqTrace Web, Caliber RM, Optimal Trace, ClearCase and ClearQuest.\nProficiency in elaborating the Use cases, writing Test cases and preparing Requirement Traceability Matrices (RTM).\nEducation: Master of Science in Engineering \u2013 New Jersey Institute of Technology (August 2008 to December 2009).\nCertification: Certified Data Scientist i.e. Data Science Professional (from John Hopkins University).\nTechnical Skills: Windows 95/98/NT/2000/XP/Vista/7, MS Visio, MS Access, MS Word, MS Excel, MS Project, Rational Rose Enterprise, IBM Rational Software Architect, Microsoft Project, Rational Requisite Pro, MS SQL Server 7.0/2000, PL/SQL, CSS (Cascading Style Sheet), OLTP & OLAP, Synon/2E, COBOL, IBM AS/400, CICS, Citrix, DB2, SAP ERP R/3 MMWM, SAP R/3 FICO, ERWIN Data Modeler 4.2/4.1/4.0, Business Objects XI r3, r2, r1, 6.5, 6.1, 5.x, 4.x, Crystal Reports XI, Informatica, HTML, Dream Weaver, MS FrontPage, IBM COGNOS (Report Studio, Query Studio, Framework Manager, Virtual View Manager, Business Insight Dashboard), Oracle BI Enterprise Edition (OBIEE), C#, C++, R Programming Language (Version 3.2.5), Minitab, XML, XSD, WSDL, X12, SAP PowerDesigner, SAP PowerBuilder, Analysis of Variance (ANOVA) and Experimental Design, Total Quality Management and Lean Six Sigma Manufacturing Methodologies, SAFe (Scaled Agile Framework) Practitioner, PMI\u2019s PMBOK guidelines & standards, TOGAF (The Open Group Architecture Framework), IIBA\u2019s BABOK guidelines & standards, Business Architecture Guild\u2019s BIZBOK (Business Architecture Body of Knowledge) standards & guidelines, Enterprise Architecture Body of Knowledge (EABOK), Enterprise Information Architecture (EIA) framework.\n\nProfessional Experience\nFidelity Investments (FMR) Smithfield, RI                               \t\t\t                                     Dec 2016 \u2013 Current\nDesignation: Principal Program Analyst / Technical Project Architect\nPrimary objective of the project was to address and resolve corporate Audit issue (Tier C) finding for the Enterprise Cybersecurity group on highly confidential Client Credentials (comprised of PII data) exposed into the server logs written by various Internal (employees within each business units, Broker \u2013 Dealer apps, 3rd party vendor apps) & External (customer / clients) Fidelity hosted applications, thereby applying log masking logic, data redaction techniques on these exposed keys within server logs physically located on multiple servers.\nDelivered elite level security solutions (for its Enterprise Cybersecurity business group) which essentially protects Fidelity and its customer's valuable information. Interacted with stakeholders, facilitated multiple fusion sessions and delivered the end results of work that provide the necessary requirements to the delivery team which thereby built and supported technology solutions.\nWorked as part of the Customer Protection Program, directly with Program and Project Managers as well as technology and business partners.\nWorked with tools such as Jira and Confluence to provide transparency of scope. Worked with our delivery team(s) including QA Data Services and engineering to ensure that we are meeting the needs of our stakeholders plus delivered solid, quality products to production. \nSupported Program and Project managers including providing input to/creation of strategy and execution plans and input to project status. Point person to escalate risks, change requests and participated in corrective actions (path to green).\nPerformed server logs analysis on Linux and Unix servers used by enterprise wide business units, thereby leveraged Splunk GUI tool to read and extrapolate pertinent meta data from these server logs. Used tools like Jira, ITEC PPMC, Confluence, etc. to track, monitor, delegate tasks / activities, operate different project aspects and functions.\nDerived insights out of ambiguity \u2013 understood, processed and interpreted complex data sets. Analyzed complex business data & identified patterns in the data using algorithms from statistics and machine learning. Developed and improved algorithms and methodologies to handle, browse, process and visualize structured and unstructured data.\nTranslated Business requirements, business rules and business criteria to system specifications and system requirements thereby preparing SRS (System Requirement Specifications) and PRS (Performance Requirement Specifications).\nEnvironment: MS Office Suite Package, MS Visio 2013 Pro, MS Excel Pivot Tables, MS Excel Functions & Formulas, MS Excel VLookUp, MS Excel Macros, Splunk Enterprise 6.5 (Splunk GUI Tool), Oracle 11g, IBM DB2, CyberArk EIAM (Enterprise Identity & Access Management), IBM InfoSphere Optim Data Privacy, ServiceNow, VMWare Tools, Oracle 11g Client 11.2.0.2 r01.02, Quest TOAD for Oracle, MS Access, Informatica Power Center, MS SQL Server, ITEC (IT Enablement Center) ALM (Application LifeCycle Management), Atlassian Confluence, ITEC (IT Enablement Center) Atlassian JIRA, Document Central, IBM DB2 Advanced Recovery Expert, IBM DB2 Merge BackUp, HP ITEC (IT Enablement Center) PPMC (Project & Portfolio Management Center).\n\nVerizon Wireless, Warren, NJ                               \t  \t\t\t\t\t\t    May 2016 \u2013 Dec 2016\nDesignation: Sr. Technical Business System Analyst / Technical Project Manager\nThe prime objective of the IoT (Internet Of Things), ReSale, M2M (Machine to Machine) portfolio was to utilize the core capabilities within MCS (Mobile Content Solutions) application suite module in order to web merchandise the non-tangible (like Licenses and Professional Services components within a cart) Items / SKUs through frontend Omni-channels; thereby foster & grow B2B (Business To Business), B2C (Business To Customer) business models from enterprise\u2019s strategic viewpoint and generate billing events at Account / Customer level from backend system application.\nAuthored, owned, managed and maintained 1Pagers to give brief overview of the products / services to be created and maintained in our MCS / SCM portal. Facilitated meeting sessions with Product Managers, Product Owners, Architectures and Analyst to hash out visio IoT (Internet of Things) Vendor flows, Order Fulfillment Flows and Order Processing Flows.\nPlanned, created, designed, managed and baselined draft high-level Project Scope, In-Scope & Out-Scope Functionalities, Draft SLAs (for 3rd party vendors / partners), Baseline Project Timelines & Gantt Chart at project implementation level with external 3rd party vendors / partners (in order to accomplish System Integration Testing [SIT] milestone) system application.\nDefined, designed and elicited core business objectives in order to deliver qualitative product, program and project with optimum utilization and the most efficient use of enterprise resources.\nManaged project charter, project plans, scope statement, project execution, risk mitigation, contingency plan and achievement of project and program objectives. Managed SIT & UAT test activities by ensuring defects are logged accurately in JIRA and issues resolved appropriately.\nCreated, aligned and logged EPICS, User Stories / Features, Tasks (Development, Infrastructure & QA Tasks), Bugs, Defects, BRs (Build Requests), DRs (Deployment Requests), Issues, etc. in JIRA tool.\nStrong knowledge of Unix commands and running those Unix commands in various boxes to validate server logs.\nEffectively and lucidly communicated project risks, issues (especially launch gating), dependencies, progress status, launches, concerns and retrospectives in structured RAID template designed and choreographed by me.\nEnvironment: Activiti BPM, XML Spy, RestFUL API, SoapUI, HP Quality Center ALM (Application Lifecycle Management) 12.20 & 12.01, Quest Toad For Oracle 12.0 (64bit), Programmer\u2019s Notepad, PuTTY.exe, JIRA, CA\u2019s Clarity PPM, Cloudsite SharePoint, Confluence document repository, APEX Search Client RealTime application, DB2 AppDevClient 9.7, MS Office Project Standard 2007, MS Office Standard 2007, MS Office Visio Standard, Open Source XML tools, Oracle Client 11g R2 11.2.0.1.0, Techsmith\u2019s SnagIT Editor 12, SQL Developer 4.1.3.\n\nChubb Insurance / Chubb Group of Insurance Companies, Whitehouse Station, NJ                            July 2015 \u2013 May 2016\nDesignation: Sr. Technical Business System Analyst / Project Architect\nThe purpose of the Client Integration initiative is to be able to view and manage PRS\u2019 business at the client / household / account level in an integrated fashion so that we can make better and faster underwriting decisions, more effectively market and communicate with existing and potential clients, price across clients and deliver better customer service. Integrate PLS & ORCA into the Chubb Client MDM/UI in order to provide an integrated household policy portfolio view across key Chubb Policy Admin Systems for Personal Lines segment, along with the other legacy Chubb\u2019s disparate policy admin systems like MasterPiece, CAAS & Yacht.\nCreated and documented roadmap for meetings with ACE. Baselined documents used for source to target mapping. Understanding CHUBB's system and applications i.e. Target system for CLIENT INT project.\nDesigned, developed and maintained consistency of the Pre-landing and Landing tables \u201cGeneric Landing Layout\u201d file / sheet that the core execution team leveraged for Source To Target Mapping sessions. \nCreated and documented High Level Data Movement and Data Flow diagrams, High Level & Detailed Conceptual Architecture diagrams using BPMN (Business Process Model and Notation) standards encapsulating every system components within visio flows. Conducted thorough analysis on CPI data models for Subject Of Insurance, Policy, Policy Term, Business Party schemas.\nConfirmed on the mapping performed within \u201cORCA Policy Status Mapping\u201d sheet for \u201cTransaction Status\u201d type and \u201cContract Status\u201d type. Confirmed the data points in \u201cMatch Columns\u201d column for the \u201cMatch Rules\u201d from \u201cClient INT ORCA Match Rules\u201d sheet. Gave an overview on \u201cSystem Triggers\u201d and introduced team on triggers. \nIntegrated Survivorship Rules for 6 Personal Lines Systems. Overview of \u201cSurvivorship Rules\u201d (like \u201cWhat is Survivorship and what does it mean to MDM process & Client UI application?\u201d) and introduce team to \u201cSurvivorship / Survival Rules Matrix\u201d sheet.\nWas responsible for identifying and documenting business rules and creating detailed Use Cases, Use Case models, Use Case diagrams and Object Oriented Analysis and Designs (OOAD) using UML (Unified Modeling Language).\nPrepared ER Diagrams, documented Data Dictionary, identified database schemas, its data elements/fields, and studied their nomenclatures from the identified columns, for phased deliverables. \nEnvironment: Informatica Data Director (IDD), Informatica Analyzer, DB2, Informatica Power Center, MDM, Oracle 11g, Information Builder\u2019s WebFocus 8, IBM Cognos Report Studio 10, IBM Lotus Notes, CICS, Cobol, Oracle SQL Server, MS Visio, MS Office Suite, MasterPiece, Guidewire policy admin system, CAAS, Yacht, PLS, ORCA, DRC, SharePoint.\n\nBed Bath and Beyond, Union, NJ                                          \t\t\t\t\t                   July 2014 \u2013 July 2015\nDesignation: Lead Technical Business System Analyst / Technical Product Manager\nPrime focus of the EXIM/Universal Cart was to have the ability to export products/items from BBBY\u2019s online channels (both desktop & mobile applications) to 3rd party vendor sites that offer personalization and/or customization services (like monogramming, etching, sublimation/printing etc.) not currently available on BBBY\u2019s sites/concepts (concepts like BBBY-US, BBBY-Canada, BuyBuyBaby, Christmas Tree Shops, etc.).\nPerformed requirements gathering & analysis by actively soliciting, examining, investigating and negotiating customer requirements with business directors and Product Development Managers, while leading the technical requirements analysis and driving the system design and construction. \nPerformed extensive SKU to Product relationship mapping at UPC level, SKU/item level, Product level, Collection & Accessories level, etc. Thereby understood cardinality between SSWPs (Single SKU Web Products) and MSWPs (Multi SKU Web Products) and process by which they are \u201cWeb Enabled\u201d and \u201cWeb Disabled\u201d for \u201cDigital Merchandising\u201d.\nConducted numerous impact analysis sessions, thereby analyzing the behavior of multiple features & functionalities on different web pages like PLP (Product List Page \u2013 Grid View & List View), PDP (Product Detail Page), Quick View, Collection & Accessories, Your Cart page, Single Shipping & Multi Shipping Location, Order Confirmation, Order Preview, Order History, Track Order, etc. and on the user CTA (Call to Action) button (buttons like Add to Cart, Add to Registry, Save for Later, Find in Store).\nCreated / Documented, managed, controlled and delivered user stories, features and functionalities in form of Story or Functional Requirements (i.e. formal FRD) within the CCM & RM tools respectively. Updated the tools with appropriate COS (Condition of Satisfaction) / Acceptance Criteria, Story Title, Card, Conversation, Assumptions etc. by following standardized Agile SCRUM methodology/process.\nAlso created / documented, managed, controlled and delivered Epic, Features, Story, Tasks, Spikes by breaking these out appropriately and then tie them back to the concerned Feature and Story within the RM (Requirements Management) tool.\nCollaborated with the development team to enforce the implementation of requirements throughout the entire coding cycle and managed change request using Rational Clear Quest.\nEnvironment: IBM\u2019s Rational Tools, IBM Requirement Management Tool, IBM Change & Configuration Management Tool, RMTrack \u2013 Defect Tracking, JIRA, ATG eCommerce Platform, TIBCO, Riversand\u2019s PIM (Product Information Management), Tableau, QlikView, STIBO\u2019s PDM (Product Data Master), Web2.0, Oracle SQL Server Developer, Microstrategy, Oracle Endeca, UNIX, Java, .Net.\n\nMaryland Department of Health & Mental Hygiene / CSC - CNSI, Baltimore, MD                                  Jan 2013 \u2013 July 2014\nDesignation: Sr. Technical Business System Analyst / BI Reporting Lead\nThe prime objective of the project is to replace the current MMIS Claims processing system for DHMH, with a web-based Service Oriented Solution consistent with MITA 2.0 (Medicaid Information Technology Architecture) guidelines that has online web capabilities for all Users including Providers and Recipients/Beneficiaries. The system\u2019s User Interface intent was to provide the capability for online data entry for Provider Enrollment applications; track and automate Workflow Management of the process; and online verification of Provider Enrollment status.\nReviewed, analyzed, evaluated business processes and associated IT application requirements. Analyzed business workflow and system needs for conversions and migrations; performed data mapping and data conversions.\nIdentified specifications for billing and accounts receivable requirements, performed gap analysis & presented information to technical team to identify system requirements. Worked extensively with developing business rules engine enabling the business rules such as referral, prior authorization, eligibility, claims processing and billing essential.\nInvolved in implementation of HIPAA EDI Transactions (835, 837). Facilitated Electronic Data Interchange (EDI). Performed GAP Analysis for HIPAA 4010 and 5010 transactions. Used EDI tools to verify mapping to X12 format. Recommended changes for system design, methods, procedures, policies and workflows affecting Medicare / Medicaid claims processing in compliance with government compliant processes like HIPAA / EDI formats and accredited standards ANSI. Analyzed HIPAA EDI transactions in X12 responses and of 837, 835, 277CA and 999 and looked for defects.\nRecommended changes for system design, methods, procedures, policies and workflows affecting Medicare/Medicaid claims processing in compliance with government compliant processes like HIPAA/ EDI formats and accredited standards ANSI.\nConducted detail oriented analysis on the current \u201cAS-IS\u201d Maryland MMIS legacy system and driving iterative GAP analysis sessions for the same to propose the effective target \u201cTO-BE\u201d MMIS system.\nDesigned Dashboards/Reports using the Oracle Business Intelligence Analytics platform for requirement analysis and data analytics. Enhanced performance of Reports / Dashboards by implementing the Aggregate tables, Materialized Views, Table partitions, re-building of indexes and managing Cache etc.\nResponsible for processing Medicaid claims in a workflow environment. Accurately interpreted benefit & policy provisions applicable to Medicaid enrollees. Reviewed & resolved claim edits using multiple systems, processes and procedures.\nEnvironment: MMIS / CMS Regulations, HIPAA Privacy, Wire Framing, UML, SCRUM, eCAMS & ProviderOne (CNSI owned & developed Claims Administration System), iLotus Notes, IBM COGNOS 10 Report Studio, Oracle Financials, JIRA (Defect Tracking Tool), ReqTrace Web 2.1, Oracle SQL Developer, J2EE, .NET, Oracle ODBC, Oracle BI Enterprise Edition 11g, MS Office Suite, MS Excel, MS Project, SharePoint, Service Oriented Architecture (SOA), RUP (For MMIS Development), Informatica PowerCenter, Enterprise Architect 10 UML (SPARX System\u2019s EA Visual Modeling Platform).\n\nMedco Health Solutions Inc. / Express Scripts Inc., Franklin Lakes, NJ                                                       Oct 2011 \u2013 Jan 2013\nDesignation: Sr. Technical Business System Analyst\nThe purpose of the project was to report the back-end Teradata IW databases utilizing the front-end Cognos BI reporting tool for the Medicare Part D claims Adjustments & Reconciliation and Operations Workbook business process. This project involved creation of a Medicare, Medicaid Solutions (MMMS) dashboard that included views from various channels, operational divisions and products within the Medicare organization.\nGathered all the Reporting Requirements and functional requirements through JAD sessions, formal interviews and brain storming sessions from the Business Owners and stakeholders based on the project scope and documented it in form of FRDs and Use-Cases in CaliberRM / Optimal Trace.\nConducted JAD sessions with different Business Users to develop new policies and procedures for the Service Catalogue, Charge Capture and Service Worklist /Charge Router, Hospital billing, coding, special coding requirements for Medco Health / Express Scripts and Claim processing.\nInvolved in analysis, design, and implementation of the following systems \u2013 Control Management Reporting and Security Systems: generation of daily, weekly, and monthly reports of hospital patients' activity. Medical Billing & Collection Systems: generation of daily billing activity based on patient records & establishment of collection protocols. Accounting Systems: Financial Statement generation and analysis according to government standards and protocols.\nWorked on EDI transactions: X12, 835, and 837 P.I to identify key data set elements for designated record set. Interacted with Claims, Payments and Enrollment hence analyzing and documenting related business processes.\nInvolved in the System Analysis, Design and Development of the project. Integral part of end-to-end implementation of IW Reporting from capturing reporting requirements, Relational data modeling in Cognos, Metadata analysis, mock-up template designing and BI reporting system.\nDisplayed in depth knowledge of Medicare/Medicaid Claims processes from Admin/Provider/Payer side which were later part of the training program to vendors. Worked on improvement of Claims Reimbursement User Interface for a better experience and incorporate changes as per HIPAA guidelines using the gap analysis.\nIdentified the functional as well as technical needs of the department and accordingly engaged as liaison between the Upper Management and IT Team to develop Cognos based BI Reports, applications and softwares.\nAssisted in development of the reports using Cognos Report Studio and Framework Manager. Developed Forms, Reports and Queries using MS Access and Excel (including Pivot tables, Vlookup and Macros).\nEnvironment: SCRUM, RUP, SharePoint, Synon/2E, COBOL, IBM AS/400, IBM Cognos 10 (Report Studio, Query Studio, Framework Manager, Virtual View Manager, Business Insight Dashboard), UNIX, Microsoft SQL Server 2008, T-SQL, MS SQL Server, Teradata IW, BRIO Query Intelligence, MS Office Suite, MS Project, MS Visio, ERWIN, OiM (Oracle Identity Manager), ORACLE SQL Server Developer, Teradata, DB2, Informatica Powercenter, CA ERwin Data Modeler, Oracle BI Enterprise Edition 10g, PL/SQL, CAs ALL Fusion Harvest, HP Quality Center, EDI, XML, .CSV (Comma Delimited), Rational Clear Quest, Anchor DW, Foundation14 (F14), CaliberRM 64Bit Production, Optimal Trace, MagicDraw UML.\n\nCVS Caremark, Richardson, TX                                                                                   \t\t\t      Jan 2010 \u2013 Oct 2011\nDesignation: Sr. Technical Business Analyst\nThe primary goal of the project is to extract common services such as Eligibility, Formulary, Drug Maintenance, etc. out of the disparate systems and host them independently to facilitate economy of operations, isolation of common business services from core adjudication transaction processing and externalize the data in a way that can be consumed by other external applications within the Organization. This will enable common services, tools and interfaces that can improve client experience and drive consistency regardless of which adjudication engine is used.\nGathered, analyzed, documented business and technical requirements from both formal and informal sessions and validate the needs of the business stakeholders, thereby drafting the Technical Design Document (TDD) and System Specification Document (SSD).\nDesigned and developed Use Cases and Use Case scenarios, Activity Diagrams, Sequence Diagrams, High Level and Low Level Process Flow Diagrams, OOAD using UML and Business Process Modeling. Understood client\u2019s business needs related to operational payer departments (i.e. claims, enrollment, billing, etc.).\nProfiled data in the sources prior to defining the data cleaning rules. Perform small enhancements (data cleansing/data quality). Worked on various Professional billing and Hospital billing products.\nDesigned and implemented unique platform, which collected and synchronized information of each person all in one place, including medical claims, lab results, self reported data and other relevant information and also in an efficient and effective manner.\nResponsible for preparing Business Requirement Document (BRD), Functional Requirement Document (FRD) and then translating into functional specifications and test plans. Closely coordinated with both business users and developers for arriving at a mutually acceptable solution.\nDeveloped project definitions, project scope, cost/benefit and risk analysis, work plans, daily and weekly progress reports, and presentations. Created and tested scripts for the premium calculations and claim limits and deductible.\nReviewed the use cases, functions and features list, map the requirements to design, and update the Requirements Traceability Matrix (RTM). Created various reports such as billing payment reports, Billing Grouping Payment and discount reports.\nResponsible to give the payers the clear vision of claim life cycle from submission to CVS Caremark through payer adjunction. Designed & implemented a web based claims processing system & management application to administer and process health insurance claims automatically. It connected the organization to largest all-payer network of commercial & government health plans nationwide to provide wealth of real-time patient benefit information.\nCreated source table definitions in the Data Stage Repository by studying the data sources by importing the data from Mainframe.\nWrote PL/SQL statement and stored procedures in Oracle for extracting as well as writing data. Created and executed claims, enrollment and/or billing test scenarios, including defect tracking.\nEnvironment: RUP, SharePoint, Synon/2E, COBOL, IBM AS/400, CICS, Citrix, Cognos 8, DB2, SAP R/3 FICO, Microsoft SQL Server 2008, T-SQL, SSIS, SSRS, SSAS, MS Excel, MS Word, MS Access, MS Project, MS Visio, ERWIN, Data Stage, OiM (Oracle Identity Manager), Oracle, PL/SQL, PC Media, Oracle BI Enterprise Edition 10g, CAs ALL Fusion Harvest, EDIFECS, HP Quality Center, Velocedi/Claredi, EDI, XML, DIAMOND, COSMOS, FACETS, LOGISTICARE, Rational Rose, Bugzilla, Snagit, Adobe Acrobat Professional, HP Quality Center.\n\nDelta Technologies                                                                                             \t\t\t     July 2006 \u2013 Aug 2008\nDesignation: Lead Technical Business Analyst                                                                                                                   \nThe project was to report corporate data warehouse for online trading system. The reporting system was to capture the KPI such as the trade volumes, number of trades, on the counter trades, and exchange trades, broker dealer trades, trade fees with respect to dimensions such as customer\u2019s location, security type, exchange traded Fees payables, etc.\nGathered the business requirements from the managers through JAD sessions, formal interviews and surveys and worked extensively with the users and with different levels of management to identify requirements, business events to develop functional specifications.\nAnalyzed the AS-IS and TO-BE processes to understand the key findings, the short term considerations, the long term considerations and its benefits.\nOwned the entire reporting process. Interacted with the ETL team, developer(s), management, and account holders to get the requirements, document them, design templates, and write specifications.\nIdentified data elements from the source systems, performed data analysis to come up with data cleansing and integration rules for the ETL process using Informatica.\nAnalyzed research on operational procedures and methods and recommended changes for improvement, with an emphasis on automation and efficiency.\nPerformed analysis and design of applications using OOAD techniques, UML and Design Patterns.\nGathered all the requirements from the stakeholders based on the project scope and documented it in RequisitePro.\nConducted interviews with various business users and Subject Matter Experts (SMEs) to collect Requirements and business process information, using Requisite Pro exercised all the different types of views in Requisite Pro Attribute matrix.\nInvolved in the testing phase right from the Unit testing to the User Acceptance testing.\nDocumented existing and proposed process flow, analyzed current and target system, and conducted GAP analysis from the reporting requirements to the existing data in ODS.\nEnvironment: RUP, Requisite Pro, Data Stage, Rational Tools, Cognos 8, MS Project, MS Word, MS Excel, MS Visio, MS Office Suite, Rational Rose, Windows Vista, ERWIN, MS SQL Server, PL/SQL, SSIS, SSRS, SSAS, Crystal Reports XI.", "years_experience": 10, "score": 0.3367810583086025, "path": "C:\\Users\\admin\\AI-Resume-Ranker\\data\\uploads\\Candidate66_resume_1.docx"}, {"filename": "Candidate66_resume.docx", "text": "Candidate66\nPhone: 201-532-6397. Email: Candidate66@gmail.com\n__________________________________________________________________________________________________\nProfessional Summary\nOverall 10 years of experience as Sr. Technical Business System Analyst (Technical BSA) in Information Technology industry with focus on Data Analysis, Information & Data Management, Service-Oriented Architecture (SOA), Data Mapping & Data Modeling, Business Process Improvement, System Analysis & Design; expertise in implementation of IT projects using Project Management methodologies. Extensive experience in diversified industry sectors including Healthcare, Life Insurance, Personal & Commercial Insurance, Property & Casualty Insurance, Telecommunication, Retail & e-Commerce (Web Merchandising), Commercial & Investment Banking and Credit Card services.\nStrong skills in Data Warehouse Design using Star Schema roll out for the fact and dimension tables. Design of cubes, partitions and aggregations with excellent knowledge of Type 2 Data modeling.\nExpertise in application development with Full Lifecycle implementation of large Data Warehouse including Project Scope, requirements gathering, Star Schema Design, Snowflake Schema, Data Modeling and ODS.\nStrong experience in Data Analysis, Data Migration, Data Cleansing, Transformation, Integration, Data Import, and Data Export through the use of multiple ETL tools such as Informatica PowerCenter.\nStrong practical knowledge of analytical techniques and methodologies such as machine learning/supervised and unsupervised techniques, segmentation, mix and time series modeling, response modeling, lift modeling, experimental design, neural networks, data mining and optimization techniques. Expert knowledge of statistical analysis tools such as R-programming, MATLAB, Spark, SAS (Statistical Analysis System), R/Spark, Apache Hadoop and Cassandra.\nStrong background in applying statistical machine learning techniques to predictive modeling and experience with Machine Learning libraries (via R, H2O, Python, Spark, etc.). Proficiency in programming in Python, R-programming, SQL, JavaScript, Java/Scala/Ruby and shell scripting. Proficiency in consuming REST based API (with JSON payload). Fluency in big data platforms including Apache Hadoop, Apache MapReduce, Hive, Spark, and Pig. Familiarity with Cloud based HaaS/PaaS solutions such as AWS EMR (Amazon Web Services Elastic MapReduce), MS Azure. Strong understanding of data profiling and data cleansing techniques.\nThorough knowledge of eCommerce architecture; ATG framework its functional limitations and the platform features, application capabilities, etc. including industry wide terminologies like B2B, B2C, C2B, C2C with its eCommerce categories and strong knowledge of PCI DSS (Payment Card Industry Data Security Standards) standards.\nTechnical expertise in handling large-scale platform architecture, integrations, development using Java/J2EE Technologies. Experience on Oracle Commerce includes ATG Commerce, ATG Multisite, ATG Merchandising, and Content Management working in some of the key areas such as the Business Control Center (BCC), Profile Management, Shopping Cart, Checkout, Pricing, performance tuning, caching, infrastructure setup and Endeca.\u00a0\nStrong knowledge of application and systems performance improvement techniques, integration technologies, approaches and patterns, Web APIs / Web Service API calls (SoapUI / SOAP, RESTful API, SOA, XML, HTTP like GET / PUT / POST / DELETE, JSON, BitTorrent, Web 2.0, etc.). Strong knowledge of security aspects like risks and threats, authentication, authorization, certificates, encryption. Experience in utilizing frameworks and re-usable components.\nProficient in using Business Analysis tools such as MS Visio, Word, Excel (including Pivot tables, Macros and Vlookup), Rational Requiste Pro, ReqTrace Web, Caliber RM, Optimal Trace, ClearCase and ClearQuest.\nProficiency in elaborating the Use cases, writing Test cases and preparing Requirement Traceability Matrices (RTM).\nEducation: Master of Science in Engineering \u2013 New Jersey Institute of Technology (August 2008 to December 2009).\nCertification: Certified Data Scientist i.e. Data Science Professional (from John Hopkins University).\nTechnical Skills: Windows 95/98/NT/2000/XP/Vista/7, MS Visio, MS Access, MS Word, MS Excel, MS Project, Rational Rose Enterprise, IBM Rational Software Architect, Microsoft Project, Rational Requisite Pro, MS SQL Server 7.0/2000, PL/SQL, CSS (Cascading Style Sheet), OLTP & OLAP, Synon/2E, COBOL, IBM AS/400, CICS, Citrix, DB2, SAP ERP R/3 MMWM, SAP R/3 FICO, ERWIN Data Modeler 4.2/4.1/4.0, Business Objects XI r3, r2, r1, 6.5, 6.1, 5.x, 4.x, Crystal Reports XI, Informatica, HTML, Dream Weaver, MS FrontPage, IBM COGNOS (Report Studio, Query Studio, Framework Manager, Virtual View Manager, Business Insight Dashboard), Oracle BI Enterprise Edition (OBIEE), C#, C++, R Programming Language (Version 3.2.5), Minitab, XML, XSD, WSDL, X12, SAP PowerDesigner, SAP PowerBuilder, Analysis of Variance (ANOVA) and Experimental Design, Total Quality Management and Lean Six Sigma Manufacturing Methodologies, SAFe (Scaled Agile Framework) Practitioner, PMI\u2019s PMBOK guidelines & standards, TOGAF (The Open Group Architecture Framework), IIBA\u2019s BABOK guidelines & standards, Business Architecture Guild\u2019s BIZBOK (Business Architecture Body of Knowledge) standards & guidelines, Enterprise Architecture Body of Knowledge (EABOK), Enterprise Information Architecture (EIA) framework.\n\nProfessional Experience\nFidelity Investments (FMR) Smithfield, RI                               \t\t\t                                     Dec 2016 \u2013 Current\nDesignation: Principal Program Analyst / Technical Project Architect\nPrimary objective of the project was to address and resolve corporate Audit issue (Tier C) finding for the Enterprise Cybersecurity group on highly confidential Client Credentials (comprised of PII data) exposed into the server logs written by various Internal (employees within each business units, Broker \u2013 Dealer apps, 3rd party vendor apps) & External (customer / clients) Fidelity hosted applications, thereby applying log masking logic, data redaction techniques on these exposed keys within server logs physically located on multiple servers.\nDelivered elite level security solutions (for its Enterprise Cybersecurity business group) which essentially protects Fidelity and its customer's valuable information. Interacted with stakeholders, facilitated multiple fusion sessions and delivered the end results of work that provide the necessary requirements to the delivery team which thereby built and supported technology solutions.\nWorked as part of the Customer Protection Program, directly with Program and Project Managers as well as technology and business partners.\nWorked with tools such as Jira and Confluence to provide transparency of scope. Worked with our delivery team(s) including QA Data Services and engineering to ensure that we are meeting the needs of our stakeholders plus delivered solid, quality products to production. \nSupported Program and Project managers including providing input to/creation of strategy and execution plans and input to project status. Point person to escalate risks, change requests and participated in corrective actions (path to green).\nPerformed server logs analysis on Linux and Unix servers used by enterprise wide business units, thereby leveraged Splunk GUI tool to read and extrapolate pertinent meta data from these server logs. Used tools like Jira, ITEC PPMC, Confluence, etc. to track, monitor, delegate tasks / activities, operate different project aspects and functions.\nDerived insights out of ambiguity \u2013 understood, processed and interpreted complex data sets. Analyzed complex business data & identified patterns in the data using algorithms from statistics and machine learning. Developed and improved algorithms and methodologies to handle, browse, process and visualize structured and unstructured data.\nTranslated Business requirements, business rules and business criteria to system specifications and system requirements thereby preparing SRS (System Requirement Specifications) and PRS (Performance Requirement Specifications).\nEnvironment: MS Office Suite Package, MS Visio 2013 Pro, MS Excel Pivot Tables, MS Excel Functions & Formulas, MS Excel VLookUp, MS Excel Macros, Splunk Enterprise 6.5 (Splunk GUI Tool), Oracle 11g, IBM DB2, CyberArk EIAM (Enterprise Identity & Access Management), IBM InfoSphere Optim Data Privacy, ServiceNow, VMWare Tools, Oracle 11g Client 11.2.0.2 r01.02, Quest TOAD for Oracle, MS Access, Informatica Power Center, MS SQL Server, ITEC (IT Enablement Center) ALM (Application LifeCycle Management), Atlassian Confluence, ITEC (IT Enablement Center) Atlassian JIRA, Document Central, IBM DB2 Advanced Recovery Expert, IBM DB2 Merge BackUp, HP ITEC (IT Enablement Center) PPMC (Project & Portfolio Management Center).\n\nVerizon Wireless, Warren, NJ                               \t  \t\t\t\t\t\t    May 2016 \u2013 Dec 2016\nDesignation: Sr. Technical Business System Analyst / Technical Project Manager\nThe prime objective of the IoT (Internet Of Things), ReSale, M2M (Machine to Machine) portfolio was to utilize the core capabilities within MCS (Mobile Content Solutions) application suite module in order to web merchandise the non-tangible (like Licenses and Professional Services components within a cart) Items / SKUs through frontend Omni-channels; thereby foster & grow B2B (Business To Business), B2C (Business To Customer) business models from enterprise\u2019s strategic viewpoint and generate billing events at Account / Customer level from backend system application.\nAuthored, owned, managed and maintained 1Pagers to give brief overview of the products / services to be created and maintained in our MCS / SCM portal. Facilitated meeting sessions with Product Managers, Product Owners, Architectures and Analyst to hash out visio IoT (Internet of Things) Vendor flows, Order Fulfillment Flows and Order Processing Flows.\nPlanned, created, designed, managed and baselined draft high-level Project Scope, In-Scope & Out-Scope Functionalities, Draft SLAs (for 3rd party vendors / partners), Baseline Project Timelines & Gantt Chart at project implementation level with external 3rd party vendors / partners (in order to accomplish System Integration Testing [SIT] milestone) system application.\nDefined, designed and elicited core business objectives in order to deliver qualitative product, program and project with optimum utilization and the most efficient use of enterprise resources.\nManaged project charter, project plans, scope statement, project execution, risk mitigation, contingency plan and achievement of project and program objectives. Managed SIT & UAT test activities by ensuring defects are logged accurately in JIRA and issues resolved appropriately.\nCreated, aligned and logged EPICS, User Stories / Features, Tasks (Development, Infrastructure & QA Tasks), Bugs, Defects, BRs (Build Requests), DRs (Deployment Requests), Issues, etc. in JIRA tool.\nStrong knowledge of Unix commands and running those Unix commands in various boxes to validate server logs.\nEffectively and lucidly communicated project risks, issues (especially launch gating), dependencies, progress status, launches, concerns and retrospectives in structured RAID template designed and choreographed by me.\nEnvironment: Activiti BPM, XML Spy, RestFUL API, SoapUI, HP Quality Center ALM (Application Lifecycle Management) 12.20 & 12.01, Quest Toad For Oracle 12.0 (64bit), Programmer\u2019s Notepad, PuTTY.exe, JIRA, CA\u2019s Clarity PPM, Cloudsite SharePoint, Confluence document repository, APEX Search Client RealTime application, DB2 AppDevClient 9.7, MS Office Project Standard 2007, MS Office Standard 2007, MS Office Visio Standard, Open Source XML tools, Oracle Client 11g R2 11.2.0.1.0, Techsmith\u2019s SnagIT Editor 12, SQL Developer 4.1.3.\n\nChubb Insurance / Chubb Group of Insurance Companies, Whitehouse Station, NJ                            July 2015 \u2013 May 2016\nDesignation: Sr. Technical Business System Analyst / Project Architect\nThe purpose of the Client Integration initiative is to be able to view and manage PRS\u2019 business at the client / household / account level in an integrated fashion so that we can make better and faster underwriting decisions, more effectively market and communicate with existing and potential clients, price across clients and deliver better customer service. Integrate PLS & ORCA into the Chubb Client MDM/UI in order to provide an integrated household policy portfolio view across key Chubb Policy Admin Systems for Personal Lines segment, along with the other legacy Chubb\u2019s disparate policy admin systems like MasterPiece, CAAS & Yacht.\nCreated and documented roadmap for meetings with ACE. Baselined documents used for source to target mapping. Understanding CHUBB's system and applications i.e. Target system for CLIENT INT project.\nDesigned, developed and maintained consistency of the Pre-landing and Landing tables \u201cGeneric Landing Layout\u201d file / sheet that the core execution team leveraged for Source To Target Mapping sessions. \nCreated and documented High Level Data Movement and Data Flow diagrams, High Level & Detailed Conceptual Architecture diagrams using BPMN (Business Process Model and Notation) standards encapsulating every system components within visio flows. Conducted thorough analysis on CPI data models for Subject Of Insurance, Policy, Policy Term, Business Party schemas.\nConfirmed on the mapping performed within \u201cORCA Policy Status Mapping\u201d sheet for \u201cTransaction Status\u201d type and \u201cContract Status\u201d type. Confirmed the data points in \u201cMatch Columns\u201d column for the \u201cMatch Rules\u201d from \u201cClient INT ORCA Match Rules\u201d sheet. Gave an overview on \u201cSystem Triggers\u201d and introduced team on triggers. \nIntegrated Survivorship Rules for 6 Personal Lines Systems. Overview of \u201cSurvivorship Rules\u201d (like \u201cWhat is Survivorship and what does it mean to MDM process & Client UI application?\u201d) and introduce team to \u201cSurvivorship / Survival Rules Matrix\u201d sheet.\nWas responsible for identifying and documenting business rules and creating detailed Use Cases, Use Case models, Use Case diagrams and Object Oriented Analysis and Designs (OOAD) using UML (Unified Modeling Language).\nPrepared ER Diagrams, documented Data Dictionary, identified database schemas, its data elements/fields, and studied their nomenclatures from the identified columns, for phased deliverables. \nEnvironment: Informatica Data Director (IDD), Informatica Analyzer, DB2, Informatica Power Center, MDM, Oracle 11g, Information Builder\u2019s WebFocus 8, IBM Cognos Report Studio 10, IBM Lotus Notes, CICS, Cobol, Oracle SQL Server, MS Visio, MS Office Suite, MasterPiece, Guidewire policy admin system, CAAS, Yacht, PLS, ORCA, DRC, SharePoint.\n\nBed Bath and Beyond, Union, NJ                                          \t\t\t\t\t                   July 2014 \u2013 July 2015\nDesignation: Lead Technical Business System Analyst / Technical Product Manager\nPrime focus of the EXIM/Universal Cart was to have the ability to export products/items from BBBY\u2019s online channels (both desktop & mobile applications) to 3rd party vendor sites that offer personalization and/or customization services (like monogramming, etching, sublimation/printing etc.) not currently available on BBBY\u2019s sites/concepts (concepts like BBBY-US, BBBY-Canada, BuyBuyBaby, Christmas Tree Shops, etc.).\nPerformed requirements gathering & analysis by actively soliciting, examining, investigating and negotiating customer requirements with business directors and Product Development Managers, while leading the technical requirements analysis and driving the system design and construction. \nPerformed extensive SKU to Product relationship mapping at UPC level, SKU/item level, Product level, Collection & Accessories level, etc. Thereby understood cardinality between SSWPs (Single SKU Web Products) and MSWPs (Multi SKU Web Products) and process by which they are \u201cWeb Enabled\u201d and \u201cWeb Disabled\u201d for \u201cDigital Merchandising\u201d.\nConducted numerous impact analysis sessions, thereby analyzing the behavior of multiple features & functionalities on different web pages like PLP (Product List Page \u2013 Grid View & List View), PDP (Product Detail Page), Quick View, Collection & Accessories, Your Cart page, Single Shipping & Multi Shipping Location, Order Confirmation, Order Preview, Order History, Track Order, etc. and on the user CTA (Call to Action) button (buttons like Add to Cart, Add to Registry, Save for Later, Find in Store).\nCreated / Documented, managed, controlled and delivered user stories, features and functionalities in form of Story or Functional Requirements (i.e. formal FRD) within the CCM & RM tools respectively. Updated the tools with appropriate COS (Condition of Satisfaction) / Acceptance Criteria, Story Title, Card, Conversation, Assumptions etc. by following standardized Agile SCRUM methodology/process.\nAlso created / documented, managed, controlled and delivered Epic, Features, Story, Tasks, Spikes by breaking these out appropriately and then tie them back to the concerned Feature and Story within the RM (Requirements Management) tool.\nCollaborated with the development team to enforce the implementation of requirements throughout the entire coding cycle and managed change request using Rational Clear Quest.\nEnvironment: IBM\u2019s Rational Tools, IBM Requirement Management Tool, IBM Change & Configuration Management Tool, RMTrack \u2013 Defect Tracking, JIRA, ATG eCommerce Platform, TIBCO, Riversand\u2019s PIM (Product Information Management), Tableau, QlikView, STIBO\u2019s PDM (Product Data Master), Web2.0, Oracle SQL Server Developer, Microstrategy, Oracle Endeca, UNIX, Java, .Net.\n\nMaryland Department of Health & Mental Hygiene / CSC - CNSI, Baltimore, MD                                  Jan 2013 \u2013 July 2014\nDesignation: Sr. Technical Business System Analyst / BI Reporting Lead\nThe prime objective of the project is to replace the current MMIS Claims processing system for DHMH, with a web-based Service Oriented Solution consistent with MITA 2.0 (Medicaid Information Technology Architecture) guidelines that has online web capabilities for all Users including Providers and Recipients/Beneficiaries. The system\u2019s User Interface intent was to provide the capability for online data entry for Provider Enrollment applications; track and automate Workflow Management of the process; and online verification of Provider Enrollment status.\nReviewed, analyzed, evaluated business processes and associated IT application requirements. Analyzed business workflow and system needs for conversions and migrations; performed data mapping and data conversions.\nIdentified specifications for billing and accounts receivable requirements, performed gap analysis & presented information to technical team to identify system requirements. Worked extensively with developing business rules engine enabling the business rules such as referral, prior authorization, eligibility, claims processing and billing essential.\nInvolved in implementation of HIPAA EDI Transactions (835, 837). Facilitated Electronic Data Interchange (EDI). Performed GAP Analysis for HIPAA 4010 and 5010 transactions. Used EDI tools to verify mapping to X12 format. Recommended changes for system design, methods, procedures, policies and workflows affecting Medicare / Medicaid claims processing in compliance with government compliant processes like HIPAA / EDI formats and accredited standards ANSI. Analyzed HIPAA EDI transactions in X12 responses and of 837, 835, 277CA and 999 and looked for defects.\nRecommended changes for system design, methods, procedures, policies and workflows affecting Medicare/Medicaid claims processing in compliance with government compliant processes like HIPAA/ EDI formats and accredited standards ANSI.\nConducted detail oriented analysis on the current \u201cAS-IS\u201d Maryland MMIS legacy system and driving iterative GAP analysis sessions for the same to propose the effective target \u201cTO-BE\u201d MMIS system.\nDesigned Dashboards/Reports using the Oracle Business Intelligence Analytics platform for requirement analysis and data analytics. Enhanced performance of Reports / Dashboards by implementing the Aggregate tables, Materialized Views, Table partitions, re-building of indexes and managing Cache etc.\nResponsible for processing Medicaid claims in a workflow environment. Accurately interpreted benefit & policy provisions applicable to Medicaid enrollees. Reviewed & resolved claim edits using multiple systems, processes and procedures.\nEnvironment: MMIS / CMS Regulations, HIPAA Privacy, Wire Framing, UML, SCRUM, eCAMS & ProviderOne (CNSI owned & developed Claims Administration System), iLotus Notes, IBM COGNOS 10 Report Studio, Oracle Financials, JIRA (Defect Tracking Tool), ReqTrace Web 2.1, Oracle SQL Developer, J2EE, .NET, Oracle ODBC, Oracle BI Enterprise Edition 11g, MS Office Suite, MS Excel, MS Project, SharePoint, Service Oriented Architecture (SOA), RUP (For MMIS Development), Informatica PowerCenter, Enterprise Architect 10 UML (SPARX System\u2019s EA Visual Modeling Platform).\n\nMedco Health Solutions Inc. / Express Scripts Inc., Franklin Lakes, NJ                                                       Oct 2011 \u2013 Jan 2013\nDesignation: Sr. Technical Business System Analyst\nThe purpose of the project was to report the back-end Teradata IW databases utilizing the front-end Cognos BI reporting tool for the Medicare Part D claims Adjustments & Reconciliation and Operations Workbook business process. This project involved creation of a Medicare, Medicaid Solutions (MMMS) dashboard that included views from various channels, operational divisions and products within the Medicare organization.\nGathered all the Reporting Requirements and functional requirements through JAD sessions, formal interviews and brain storming sessions from the Business Owners and stakeholders based on the project scope and documented it in form of FRDs and Use-Cases in CaliberRM / Optimal Trace.\nConducted JAD sessions with different Business Users to develop new policies and procedures for the Service Catalogue, Charge Capture and Service Worklist /Charge Router, Hospital billing, coding, special coding requirements for Medco Health / Express Scripts and Claim processing.\nInvolved in analysis, design, and implementation of the following systems \u2013 Control Management Reporting and Security Systems: generation of daily, weekly, and monthly reports of hospital patients' activity. Medical Billing & Collection Systems: generation of daily billing activity based on patient records & establishment of collection protocols. Accounting Systems: Financial Statement generation and analysis according to government standards and protocols.\nWorked on EDI transactions: X12, 835, and 837 P.I to identify key data set elements for designated record set. Interacted with Claims, Payments and Enrollment hence analyzing and documenting related business processes.\nInvolved in the System Analysis, Design and Development of the project. Integral part of end-to-end implementation of IW Reporting from capturing reporting requirements, Relational data modeling in Cognos, Metadata analysis, mock-up template designing and BI reporting system.\nDisplayed in depth knowledge of Medicare/Medicaid Claims processes from Admin/Provider/Payer side which were later part of the training program to vendors. Worked on improvement of Claims Reimbursement User Interface for a better experience and incorporate changes as per HIPAA guidelines using the gap analysis.\nIdentified the functional as well as technical needs of the department and accordingly engaged as liaison between the Upper Management and IT Team to develop Cognos based BI Reports, applications and softwares.\nAssisted in development of the reports using Cognos Report Studio and Framework Manager. Developed Forms, Reports and Queries using MS Access and Excel (including Pivot tables, Vlookup and Macros).\nEnvironment: SCRUM, RUP, SharePoint, Synon/2E, COBOL, IBM AS/400, IBM Cognos 10 (Report Studio, Query Studio, Framework Manager, Virtual View Manager, Business Insight Dashboard), UNIX, Microsoft SQL Server 2008, T-SQL, MS SQL Server, Teradata IW, BRIO Query Intelligence, MS Office Suite, MS Project, MS Visio, ERWIN, OiM (Oracle Identity Manager), ORACLE SQL Server Developer, Teradata, DB2, Informatica Powercenter, CA ERwin Data Modeler, Oracle BI Enterprise Edition 10g, PL/SQL, CAs ALL Fusion Harvest, HP Quality Center, EDI, XML, .CSV (Comma Delimited), Rational Clear Quest, Anchor DW, Foundation14 (F14), CaliberRM 64Bit Production, Optimal Trace, MagicDraw UML.\n\nCVS Caremark, Richardson, TX                                                                                   \t\t\t      Jan 2010 \u2013 Oct 2011\nDesignation: Sr. Technical Business Analyst\nThe primary goal of the project is to extract common services such as Eligibility, Formulary, Drug Maintenance, etc. out of the disparate systems and host them independently to facilitate economy of operations, isolation of common business services from core adjudication transaction processing and externalize the data in a way that can be consumed by other external applications within the Organization. This will enable common services, tools and interfaces that can improve client experience and drive consistency regardless of which adjudication engine is used.\nGathered, analyzed, documented business and technical requirements from both formal and informal sessions and validate the needs of the business stakeholders, thereby drafting the Technical Design Document (TDD) and System Specification Document (SSD).\nDesigned and developed Use Cases and Use Case scenarios, Activity Diagrams, Sequence Diagrams, High Level and Low Level Process Flow Diagrams, OOAD using UML and Business Process Modeling. Understood client\u2019s business needs related to operational payer departments (i.e. claims, enrollment, billing, etc.).\nProfiled data in the sources prior to defining the data cleaning rules. Perform small enhancements (data cleansing/data quality). Worked on various Professional billing and Hospital billing products.\nDesigned and implemented unique platform, which collected and synchronized information of each person all in one place, including medical claims, lab results, self reported data and other relevant information and also in an efficient and effective manner.\nResponsible for preparing Business Requirement Document (BRD), Functional Requirement Document (FRD) and then translating into functional specifications and test plans. Closely coordinated with both business users and developers for arriving at a mutually acceptable solution.\nDeveloped project definitions, project scope, cost/benefit and risk analysis, work plans, daily and weekly progress reports, and presentations. Created and tested scripts for the premium calculations and claim limits and deductible.\nReviewed the use cases, functions and features list, map the requirements to design, and update the Requirements Traceability Matrix (RTM). Created various reports such as billing payment reports, Billing Grouping Payment and discount reports.\nResponsible to give the payers the clear vision of claim life cycle from submission to CVS Caremark through payer adjunction. Designed & implemented a web based claims processing system & management application to administer and process health insurance claims automatically. It connected the organization to largest all-payer network of commercial & government health plans nationwide to provide wealth of real-time patient benefit information.\nCreated source table definitions in the Data Stage Repository by studying the data sources by importing the data from Mainframe.\nWrote PL/SQL statement and stored procedures in Oracle for extracting as well as writing data. Created and executed claims, enrollment and/or billing test scenarios, including defect tracking.\nEnvironment: RUP, SharePoint, Synon/2E, COBOL, IBM AS/400, CICS, Citrix, Cognos 8, DB2, SAP R/3 FICO, Microsoft SQL Server 2008, T-SQL, SSIS, SSRS, SSAS, MS Excel, MS Word, MS Access, MS Project, MS Visio, ERWIN, Data Stage, OiM (Oracle Identity Manager), Oracle, PL/SQL, PC Media, Oracle BI Enterprise Edition 10g, CAs ALL Fusion Harvest, EDIFECS, HP Quality Center, Velocedi/Claredi, EDI, XML, DIAMOND, COSMOS, FACETS, LOGISTICARE, Rational Rose, Bugzilla, Snagit, Adobe Acrobat Professional, HP Quality Center.\n\nDelta Technologies                                                                                             \t\t\t     July 2006 \u2013 Aug 2008\nDesignation: Lead Technical Business Analyst                                                                                                                   \nThe project was to report corporate data warehouse for online trading system. The reporting system was to capture the KPI such as the trade volumes, number of trades, on the counter trades, and exchange trades, broker dealer trades, trade fees with respect to dimensions such as customer\u2019s location, security type, exchange traded Fees payables, etc.\nGathered the business requirements from the managers through JAD sessions, formal interviews and surveys and worked extensively with the users and with different levels of management to identify requirements, business events to develop functional specifications.\nAnalyzed the AS-IS and TO-BE processes to understand the key findings, the short term considerations, the long term considerations and its benefits.\nOwned the entire reporting process. Interacted with the ETL team, developer(s), management, and account holders to get the requirements, document them, design templates, and write specifications.\nIdentified data elements from the source systems, performed data analysis to come up with data cleansing and integration rules for the ETL process using Informatica.\nAnalyzed research on operational procedures and methods and recommended changes for improvement, with an emphasis on automation and efficiency.\nPerformed analysis and design of applications using OOAD techniques, UML and Design Patterns.\nGathered all the requirements from the stakeholders based on the project scope and documented it in RequisitePro.\nConducted interviews with various business users and Subject Matter Experts (SMEs) to collect Requirements and business process information, using Requisite Pro exercised all the different types of views in Requisite Pro Attribute matrix.\nInvolved in the testing phase right from the Unit testing to the User Acceptance testing.\nDocumented existing and proposed process flow, analyzed current and target system, and conducted GAP analysis from the reporting requirements to the existing data in ODS.\nEnvironment: RUP, Requisite Pro, Data Stage, Rational Tools, Cognos 8, MS Project, MS Word, MS Excel, MS Visio, MS Office Suite, Rational Rose, Windows Vista, ERWIN, MS SQL Server, PL/SQL, SSIS, SSRS, SSAS, Crystal Reports XI.", "years_experience": 10, "score": 0.3367810583086025, "path": "C:\\Users\\admin\\AI-Resume-Ranker\\data\\uploads\\Candidate66_resume.docx"}, {"filename": "Candidate83_Hadoop.docx", "text": "Candidate83\nSr. Hadoop Developer\nEmail: \tcandidate8308@gmail.com \t\t\t\t\t\t             Contact: (615) 813-1551\n\nPROFESSIONAL SUMMARY:\n\n8+ years of overall software development experience on Big Data Technologies, Hadoop Eco system and Java/J2EE Technologies with experience programming in Java, Scala, Python and SQL\n4+ years of strong hands-on experience on Hadoop Ecosystem including Spark, Map-Reduce, HIVE, Pig, HDFS, YARN, HBase, Oozie, Kafka, Sqoop, Flume.\nExperience in architecting, designing, and building distributed software systems.\u00a0\nScala and Java, Created frameworks for processing data pipelines through Spark\nWrote python scripts to parse XML documents and load the data in database.\nDeep knowledge of troubleshooting and tuning Spark applications and Hive scripts to achieve optimal performance.\nUsed Sqoop to import data into HDFS / Hive from RDBMS and exporting data back to HDFS or HIVE from RDBMS.\nWorked with real-time data processing and streaming techniques using Spark streaming, Storm and Kafka.\nExperience developing Kafka producers and Kafka Consumers for streaming millions of events per second on streaming data\nSignificant experience writing custom UDF\u2019s in Hive and custom Input Formats in MapReduce.\nKnowledge of job workflow scheduling and monitoring tools like Oozie.\nStrong experience productionalizing end to end data pipelines on hadoop platform.\nExpertise in Database Design, Creation and Management of Schemas, writing Stored Procedures, Functions, DDL and DML SQL queries and writing complex queries for Oracle\nExperience working with NoSQL database technologies, including MongoDB, Cassandra and HBase.\nGood experience is designing and implementing end to end Data Security and Governance within Hadoop Platform using Kerberos.\nStrong experience with UNIX shell scripts and commands.\nExperience in using various Hadoop Distributions like Cloudera, Hortonworks and Amazon EMR.\nStrong hands-on development experience with Java, J2EE (Servlets, JSP, Java Beans, EJB, JDBC, JMS, Web Services) and related technologies.\nWork with the team to help understand requirements, evaluate new features, architecture and help drive decisions.\nExcellent interpersonal, communication, problem solving and analytical skills with ability to make independent decisions\nExperience successfully delivering applications using agile methodologies including extreme programming, SCRUM and Test-Driven Development (TDD).\nExperience in Object Oriented Analysis, Design, and Programming of distributed web-based applications.\nExtensive experience in developing standalone multithreaded applications.\nConfigured and developed web applications in Spring and employed spring MVC architecture and Inversion of Control.\nExperience in building, deploying and integrating applications in Application Servers with ANT, Maven and Gradle.\nSignificant application development experience with REST Web Services, SOAP, WSDL, and XML.\n\nTECHNICAL SKILLS:\n\n\n\n\nPROFESSIONAL EXPERIENCE:\n\nClient\t\t: \tTMNAS\t\t\t\t\t\t        Sep 2016 \u2013 Present\nLocation\t: \tBala Cynwyd, PA\t\nRole\t\t:\tSr. Hadoop Developer\n\nProject Description: TMNA offers the security of nearby expertise, enhanced by the diversity and power of one of the world\u2019s most respected insurance groups. Tokio Marine\u2019s companies offer access to leading commercial insurance solutions spanning the property and casualty landscape including professional liability, workers\u2019 compensation and property coverage.. The project deals with analyzing clickstream data of users who are visiting the company websites and applications to derive useful insights that help in optimizing future promotions and advertising.\n\nResponsibilities:\n\nInvolved in story-driven agile development methodology and actively participated in daily scrum meetings.\nIngested terabytes of click stream data from external systems like FTP Servers and S3 buckets into HDFS using custom Input Adaptors.\nImplemented end-to-end pipelines for performing user behavioral analytics to identify user-browsing patterns and provide rich experience and personalization to the visitors.\nDeveloped Kafka producers for streaming real-time clickstream events from external Rest services into topics.\nUsed HDFS File System API to connect to FTP Server and HDFS.  S3 AWS SDK for connecting to S3 buckets.\nWritten Scala based Spark applications for performing various data transformations, denormalization, and other custom processing.\nImplemented data pipeline using Spark, Hive, Sqoop and Kafka to ingest customer behavioral data into Hadoop platform to perform user behavioral analytics.\nCreated a multi-threaded Java application running on edge node for pulling the raw clickstream data from FTP servers and AWS S3 buckets.\nDeveloped Spark streaming jobs using Scala for real time processing.\nInvolved in creating external Hive tables from the files stored in the HDFS.\nOptimized the Hive tables utilizing improvement techniques like partitions and bucketing to give better execution Hive QL queries.\nUsed Spark-SQL to read data from hive tables, and perform various transformations like changing date format and breaking complex columns. \nWrote spark application to load the transformed data back into the Hive tables using parquet format.\nUsed Oozie Scheduler system to automate the pipeline workflow to exact data on a timely manner. \nImplemented installation and configuration of multi-node cluster on the cloud using Amazon \nWeb Services (AWS) on EC2.\nWorked on data visualization and analytics with research scientist and business stake holders.\n\nEnvironment: Hadoop 2.x, Spark, Scala, Hive, Pig, Sqoop, Oozie, Kafka, Cloudera Manager, Storm, ZooKeeper, HBase, Impala, YARN, Cassandra, JIRA, MySQL, Kerberos, Amazon AWS, Shell Scripting, SBT, Git, Maven.\n\n\nClient\t\t:\tDavita Inc\t\t\t    \t\t\t    Jan 2015 - Sep 2016 \nLocation\t:\tNashville, Tennessee\nRole\t\t:\tSr.Hadoop Developer\n\nProject Description: Davita is one of the largest kidney dialysis companies in world. The idea of the project is to ingest data from different multiple sources to Hadoop Data Lake, perform transformations on it according to business requirements and exporting the data to external systems. The system is a scalable BI platform that can adapt to the speed of the business by providing relevant, accessible, timely, connected, and accurate data.\n\nResponsibilities:\nInvolved in gathering and analyzing business requirements and designing Data Lake as per the requirements.\nBuilt distributed, scalable, and reliable data pipelines that ingest and process data at scale using Hive and MapReduce.\nDeveloped MapReduce jobs in Java for cleansing the data and preprocessing.\nLoaded transactional data from Teradata using Sqoop and create Hive Tables.\nExtensively used Sqoop for efficiently transferring bulk data between HDFS and relational databases.\nWorked on automation of delta feeds from Teradata using Sqoop and from FTP Servers to Hive.\nWorked on various performance optimizations like using distributed cache for small datasets, Partition,\u00a0Bucketing in hive and Map Side joins.\nCreated components like Hive UDFs for missing functionality in HIVE for analytics.\nUsed IMPALA to analyze the data present in Hive tables.\nHandled Avro and JSON data in Hive using Hive SerDe.\nWorked with different compression codecs like GZIP, SNAPPY and BZIP2 in MapReduce, Pig and Hive for better performance.\nAnalyzed the data by performing the Hive queries using Hive QL to study the customer behavior.\nWrote python scripts to parse XML documents and load the data in database.\nGenerate auto mails by using Python scripts.\nImplemented the recurring workflows using Oozie to automate the scheduling flow.\nWorked with application teams to install OS level updates and version upgrades for Hadoop cluster environments.\nParticipated in design and code reviews.\n\nEnvironment: HDFS, Hadoop, Pig, Hive, HBase, Sqoop, Talend, Flume, Map Reduce, Podium Data, Oozie, Java 6/7, Oracle 10g, YARN, UNIX Shell Scripting, SOAP, REST services, Oracle 10g, Maven, Agile Methodology, JIRA.\n\nClient\t\t:\tNASBA\t\t\t\t\t\t\t    Aug 2012 - Dec 2014 \nLocation\t:\tNashville, TN\nRole\t\t:\tHadoop Developer\n\nProject Description: National Association of State Boards of Accountancy enhances the effectiveness and advance the common interests of the Boards of Accountancy. Existing ETL platform is overloaded with data coming from variety of sources and as data is growing day by day, it is not able to perform well and cost of managing the Relational database servers are going up. So, the data is migrated from multiple sources to Hadoop Data Lake and transformations are performed on it according to business requirements and the processed data is exported to external systems.\n\nResponsibilities: \nAnalysed business requirements and created/updated Software Requirements and design documents\t\nImported the data from relational databases to Hadoop cluster by using Sqoop. \nProvided batch processing solution to certain unstructured and large volume of data by using Hadoop Map Reduce framework.\nDeveloped data pipelines using Hive scripts to transform data from Teradata, DB2 data sources. These pipelines had customized UDF'S to extend the ETL functionality. \nDeveloped UDF for converting data from Hive table to JSON format as per client requirement.\u00a0\nInvolved in creating tables in Hive and writing scripts and queries to load data into Hive tables from HDFS.\nImplemented dynamic partitioning and Bucketing in Hive as part of performance tuning.\u00a0\nCreated custom UDF\u2019s in Pig and Hive.\nPerformed various transformations on data like changing date patterns, converting to other time zones etc.\nDesigned and developed PIG Latin Scripts to process data in a batch to perform trend analysis.\u00a0\nAutomated Sqoop, hive and pig jobs using Oozie scheduling. \nStoring, processing and analyzing huge data-set for getting valuable insights from them.\u00a0\nCreated various aggregated datasets for easy and faster reporting using Tableau.\n\nEnvironment: HDFS, Map Reduce, Hive, Sqoop, Pig, HBase, Oozie, CDH distribution, Java, Eclipse, Shell Scripts, Tableau, Windows, Linux.\n\nClient\t\t:\tCopart Inc\t\t\t\t\t\t    Oct 2010 - Aug 2012 \nLocation\t:\tDallas, TX\nRole\t\t:\tJava Developer\n\nProject Description:  Copart makes it easy for Members to find, bid and win the vehicles that they are looking for. Members can choose from\u00a0classics, early and late model\u00a0cars\u00a0and\u00a0trucks,\u00a0industrial vehicles\u00a0and more. These internal applications are used for content management through which users can request a new site, which is used to store their project related documents.\n\nResponsibilities:\nDeveloped the J2EE application based on the Service Oriented Architecture by employing SOAP and other tools for data exchanges and updates.\nWorked in all the modules of the application which involved front-end presentation logic - developed using Spring MVC, JSP, JSTL and JavaScript, Business objects - developed using POJOs and data access layer - using Hibernate framework.\u00a0\nDesigned the GUI of the application using JavaScript, HTML, CSS, Servlets, and JSP.\nInvolved in writing AJAX scripts for the requests to process quickly.\nUsed Dependency Injection feature and AOP features of Spring framework to handle exceptions.\nInvolved in writing Hibernate Query Language (HQL) for persistence layer.\nImplemented persistence layer using Hibernate that uses the POJOs to represent the persistence database.\nUsed JDBC to connect to backend databases, Oracle and SQL Server 2005.  \nProficient in writing SQL queries, stored procedures for multiple databases, Oracle and SQL Server.\nWrote backend jobs based on Core Java & Oracle Data Base to be run daily/weekly.\nUsed Restful API and SOAP web services for internal and external consumption.\nUsed Core Java concepts like Collections, Garbage Collection, Multithreading, OOPs concepts and APIs to do encryption and compression of incoming request to provide security.\nWritten and implemented test scripts to support Test driven development (TDD) and continuous integration.\n\nEnvironment: Java, JSP, HTML, CSS, Ubuntu Operating System, JavaScript, AJAX, Servlets, Struts, Hibernate, EJB (Session Beans), Log4J, WebSphere, JNDI, Oracle, Windows XP, LINUX, ANT, Eclipse.\n\n\nClient\t\t:\tAricent\t\t\t\t\t\t\t    Nov 2008 - Sep 2010    \nLocation\t:\tNewyork\nRole\t\t:\tJava Developer\n\nProject Description:  Aricent is a global design and engineering company innovating in the digital era. help the world's leading companies solve their most important business and technology innovation challenges - from Customer to Chip. I have worked on developing the Aricent internal applications to automate the business process, store the documents. These internal applications are used for content management through which users can request a new site, which is used to store their project related documents.\n\nResponsibilities:\nAnalyzed user requirements and created Software Requirements and design documents\nResponsible for GUI development using Java, JSP, Struts\nDatabase design and development \nCreated and modified existing database scripts, Tables, Stored Procedures, and Triggers \nUsed XML functions, Cursors, Mail and Utility packages for Advanced search functionality\nCreated data correction and manipulation scripts for Production\nUsed JAXB for marshalling and un-marshalling of the data\nCreated JUnit tests for the service layer\nSupport for Production issues\nAttending the review meetings for scheduling, implementation and resolving issues in software development cycle\nEnvironment: Java, Struts, Java, Jsp, Servlets, JQuery, Ajax, XML, XSLT, JAXB, FOP, JBoss, Weblogic, Tomcat, SQL server 2005 and MyEclipse\n\n\nEDUCATION:\nBachelor of Technology in Computer Science.               \n\n", "years_experience": 8, "score": 0.3256866953299197, "path": "C:\\Users\\admin\\AI-Resume-Ranker\\data\\uploads\\Candidate83_Hadoop.docx"}, {"filename": "Yohan_BSA.docx", "text": "YOHAN \nSr. Business Analyst\n\nVersatile, effective professional with 8+ years of IT Experience in conceiving, developing and implementing cutting edge solutions in Software Development Life Cycle and Agile environments. Seasoned technical solution provider, with an intuitive sense of end-user requirements. Keen relationship builder, tech savvy team leader, and engaging communicator. Detail oriented professional with proven leadership and experience in Project management, Functional and Business Analysis with specialization in Supply Chain, Retail and E-Commerce.\nProfessional Summary: \nCompetent experience from working on different phases of Software Development Lifecycle (SDLC) utilizing the Waterfall methodology, and Agile methodologies like SCRUM, Kanban and Waterfall-Agile hybrid. Have a sound understanding of Scaled Agile Framework (SAFe)\nExperience in strategic sourcing, strategic planning, resource management, procurement and sourcing in the supply chain of projects using tools such as NetSuite, Maximo and SAP Ariba\nIn depth understanding of the Gap Analysis i.e., As-Is and To-Be business processes and strong experience in gathering requirements and converting these requirements into Technical Specifications documents\nPossess expertise in tracing requirements throughout the development process and verifying adherence to Requirement Traceability Matrix (RTM) using tools such as Excel and Rational Requisite Pro. Performed Cost benefit analysis, and SWOT analysis to obtain clarity on requirements\nExperience in creating Business Requirement Documents (BRD), Functional requirement Document (FRD), Use Case Narratives & Use Case Diagrams among others\nStrong analysis, communication and training skills with proven leadership and ability to work as the primary interface between Business partners and Developers/SME\u2019s/Architect/UX team\nExperienced in Migration of Legacy system into implementing Content Management Systems using SharePoint\nExperience in content and metadata migration and consolidation of templates\nWorking knowledge in data warehousing concepts, Data Marts, Operational Data Stores and ETL Tool - IBM\u00ae WebSphere\u00ae (InfoSphere) DataStage\u00ae & Informatica Power Center. Worked extensively with Data warehouses by migrating data from various OLTP\u2019s to OLAP cube using Informatica Power Center \nStrong understanding of Multi-Dimensional Database schemas like Star, Snowflake, Fact Constellation Schema; Fact and Dimensional tables\nWorked on multiple data analysis task to analyze data, tables and other database contents\nExperience in creating SQL Queries in SQL Server including DDL and DML, with a strong understanding of data & analytics; creating and requesting the updates of data dictionaries\nExperience in documenting and executing quality Test Plans, Test Cases, and verifying and validating complex scenarios for quality purposes including User Acceptance Testing (UAT), Blackbox testing, Whitebox Testing, Regression testing, Load & Spike testing\nInvolved in reviewing and analyzing various test metrics including process and product metrics such as Test Design Coverage, Test Execution Productivity, Test Execution Coverage, Test Cases passed, Test Cases failed, Error Discovery rate, Defect Fix Rate, Defect Removal Efficiency\nCollaborated with QA team in performing automated testing using tools such as Selenium, Cucumber, HP UFT \nExtensive experience in creating SQL reports, graphical interactive UI and dashboards using QlikSense, Power BI and Tableau from multiple data sources like SQL, Oracle and flat files like JSON, CSV, Excel\nWell acquainted with Unified Modeling Language (UML) diagrams including Use Cases, Activity, and Sequence diagrams therefore assisting in Business Process Modeling\nExpertise Reporting, Visualizations and Project Management, creating meaningful insights on KPI\u2019s such as Order fill rate, Stockout rate, Backorder levels, inventory levels, inventory turnover rate etc.\nExcellent technical skills with, Oracle - Advanced Benefits, SAP - Ariba, Informatica Power Center, Exact Target, SQL Navigator, Jira, Confluence, SharePoint, HP ALM \nExperience in documenting tags of REST and SOAP UI and involved in API testing using Postman\nExperience in SAP ERP \u2013 Ariba integrated environments, actively involved in the Sell Side of the system\nEducation and Certification:\nBusiness Analytics & Economics \u2013 Harvard Business School\nBachelor of Engineering \u2013 Mechanical Engineering\nTechnical Skills/Knowledge:\nExperience:\nSr. Business Systems Analyst\nNXP Semiconductors \u2013 Austin, TX\nDecember 2016 \u2013 Present\nDescription: This project succeeded the migration of the client\u2019s legacy distributed storage to a cloud-based platform designed for asset management with real time data provision and global connectivity. The mission was to provide our clients with a unified platform experience to manage hundreds of thousands of devices with the help of our cutting-edge sensors and platform solutions. The values we delivered were operational intelligence, effective compliance adherence, predictability of maintenance needs, improved equipment availability and overall supply chain life cycle.\nRoles and Responsibilities:\nConduct meetings with Subject Matter Experts to identify business needs and set up walkthrough meetings with Business Users and Directors\nWorking with MS Word, MS Excel, MS Project, MS Visio for documentation and analysis reports among other project management and reporting tools\nDriver behind the elimination of the supplier Code of Conduct Compliance (CoC) issues as well as the Universal Device Identification (UDI) regulation adherence issues\nDocument analysis and represent them in Requirements Traceability Matrix (RTM) using Requisite Pro\nMaintain project documents and share important links and tutorials, etc. with the team using SharePoint\nCreate User Requirement Specifications (URS) and Functional Requirement Specifications (FRS) documents after consultation with the stakeholders involved using elicitation techniques like interviews, and focus groups\nDesign Use Case Diagrams and wrote their specifications in the Use Case Specification documents using UML/Visio based on the requirements gathered\nDirect several mission critical updates like Db2 and SQL Server upgrade, file system upgrades, DLL installations, disaster recovery testing etc.\nProvided detailed information about business data, such as standard definitions of data elements, their meanings and allowable values using Data Dictionary\nPerformed data mapping using Infomatica PowerCenter 8.6, validated connection objects and target writing path of the concerned warehouse with the help of proper naming conventions\nProficient in design and development of various dashboards, reports utilizing Tableau Visualizations like bar graphs, line diagrams, pareto charts, funnel charts, scatter plots, pie-charts, donut charts, bubble charts, funnel charts, heat maps, tree maps according to the end user requirements\nDiscuss table design, data flow, data migration, ETL processes, reporting via Tableau, & consequently discuss with development team requirement details\nInvolved in UAT and post-production support by creating a checklist to ensure close to 100% coverage\nAssisted QA Team in executing the Test Plans, Test Cases, Test Scenarios and collected the Test and Process Metrics. Also, assisted with writing manual testing scripts when needed\nUtilized Manual testing methodologies using HP QC ALM to check validity of existing data within the databases and also to protect the system from SQL Injection Attacks. Assisted in writing Test Scripts to check load capacity and spikes using Load Runner\nDeveloped numerous Reports / Dashboards with different Analytics Views (Drill-Down / Dynamic, Cascading Style Sheets, Pivot Table, Column Selector, Tabular with global and local Filters) using Presentation Services\nSupported the Ariba Implementation of Ariba Contract Management, Contract Compliance, Sourcing, SLP and Guided Buying modules\nInvolved in create, read, update and delete operations within SAP \u2013 ERP 6.0 related to purchase orders, sales orders, work orders, transfer orders\nCreated a mapping document that maps all the fields in the HTML pages to XML tags which are in turn mapped with the tables and column names in the database and involved in performing client-side validation\nEnvironment: Waterfall-Agile, MS Office, MS Project, MS Visio, MySQL, IBM Db2 10.5, Postman 4.8, Informatica PowerCenter 9.6.x, Tableau 8.3.11, IBM Websphere, Apache Tomcat, IBM Maximo 7.6, Rational Requisite Pro, HP ALM, HP UFT 12.0, Cucumber, Load Runner, SAP \u2013 ERP 6.0, SAP \u2013 Ariba, HTML, XML, Jira 7.2.4, Twilio 2.0.0\nSr. Business Analyst\nKohl\u2019s \u2013 Milwaukee, WI\nOctober 2015 \u2013 November 2016\nDescription: The client is a value-oriented organization that specializes in offering retail services. Manage the development, implementation and administration of Vendor Scorecards. Work collaboratively with internal business groups and Kohl's vendor group to monitor and improve vendor compliance to established business rules and guidelines. Drive consistent excellent vendor performance while reducing expense and waste from the total supply chain.\nRoles and Responsibilities:\nAssisted Product Owner in release level, sprint level and product backlog economics with respect to supply chain\nImproved the product backlog incrementally and reviewed it with Product Owner during sprints\nMulti project experience on Ariba Spend Management Suite - Ariba Buyer, Ariba Sourcing, Analysis and Contracts Workbench\nHelped visualize business goals in the form of success indicators & KPI\u2019s which included order fulfillment rates, on-time delivery, PO item fill rate, PO quantity fill rate, GM return on investment etc.\nHelped create wireframes for UI / UX requirements with direction from Product Owner and Presentation Layer from JAD sessions\nEnhanced the flow of communication between stakeholders and acted as the point of contact for all incident tickets and test environment issues. Coordinate issue escalation and/or resolution, manage code installs, and manage security for respective environments\nResponsible for writing effective user stories using INVEST criteria and splitting into tasks using the SMART criteria and defined the Acceptance criteria. Helped the team in estimating stories using techniques Planning Poker and proficient in prioritization techniques like MoSCoW and KANO based on the business needs prevalent during that season. Expert at performing various retrospective techniques as well\nDeveloped & maintained Requirements Traceability Matrix (RTM) tracking end-to-end mapping of system specifications and test cases ensuring 100% coverage\nCreated, assigned, and maintained tickets in Jira for issues, bugs and defects so that these could be monitored systematically to its closeout. Also performed Issue Tracking in Jira to effectively track impediments surrounding user stories and other dependencies\nUsed Jira to create Sprint Burnup and Burndown Chart reports to present to relevant high-level stakeholders to identify the next courses of action regarding the release, supporting our supply chain\nAssisted and communicated requirements with the team in Test Driven Development (TDD), Behavior Driven Development (BDD) which were used in test-centric development and daily standup meetings, ensuring quality standards were met as per the build-release schedule\nInvolved in writing Feature Files in the Gherkins language for Cucumber testing and have experience and collaborated in the Cucumber \u2013 Selenium integrated automated testing environment for BDD\nAssisted the team in writing user stories by clearly articulating each Acceptance Criteria written in the Gherkins language. Constantly re-iterated how each user story relates to the final Definition of Done (DoD)\nCreated Master Test Plan, Test Scenarios, Test Cases, and Test Scripts and performed facilitated User Acceptance Testing (UAT) collaborating intensively with the quality assurance team\nInvolved in API documentation and testing using Swagger, provided guidance and review for SOA implementation its deployment and management. Integration based on XML / JSON data representation using SOAP / REST Services or orchestrated services\nUtilized SQL to probe data dependency issues and inconsistencies to safeguard against SQL Injection\nSet up the reporting tools and monitored the reporting functions for the clients using MS Excel and Power BI\nEnvironment: Agile, Apache Hadoop, HDFS, Apache Hive, Apache Kafka, Jira 6.2, SQL, MS Office, MS Project, Selenium, Cucumber, Power BI, Jira 7, SAP \u2013 Ariba Spend Management Suite\nSr. Business Analyst\nState of Massachusetts \u2013 Boston, MA\nFebruary 2014 \u2013 September 2015\nDescription: The client was the Department of Transportation and is a provider of transportation solutions used by public to manage and mitigate risk. I was a part of a team involved in one of the bridge improvements project and was one of the five largest projects of the $3 billion effort to reduce the number of structurally deficient bridges in Massachusetts. \nRoles and Responsibilities:\nAccomplished experience through whole SDLC from requirement analysis till deployment\nWrote SLA and negotiated terms of contract that ensure quality of service and cost savings \nCreated and tracked schedules to ensure deliverables are met throughout the SDLC\nDeveloped non-functional and functional requirements documents like BRD, FRD, and SRS\nCreated need analysis documents of the requirements gathered through JAD, Brainstorming and interviews sessions using Rational Requisite Pro\nCoordinated and managed business analyst resources work queue to ensure project timelines are met to produce builds as per the requirements using MS Project\nActively involved in Advanced Planning and Scheduling (APS) including production scheduling, distribution planning and transportation planning\nWorked on Accounts Receivables, Accounts Payables, cash management, asset management, supplier management using the SAP ERP - Ariba integrated modules \nProvided Systems Consultant functions such as managing change requests across the systems development life-cycle (SDLC), building and managing client business user relationships in a highly dynamic work environment\nParticipated in the Change Control Board to identify the impact of the change, collaborate and come to a consensus as to its feasibility and risks involved chartered documents on the appropriate next steps \nDid root cause analysis to identify data quality problems and obtain historical trends\nCreated Physical architecture diagrams from Conceptual architecture diagrams using the Erwin tool\nInvolved in the maintenance and timely updates of Data Dictionary\nExtensive experience in UAT testing and post \u2013 production support by creating a checklist to ensure close to 100% requirements coverage\nWorked proactively with the DevOps team in performing Impact analysis, Cost Benefit analysis, and capturing Risks in Risk Registers and mitigating using Risk Mitigation Strategies\nBuild rich and dynamic dashboards using out-of-the-box features, customizations, visualizations and advance functionalities like Groups in calculations, Cluster analysis, Cross Database joins etc. in MicroStrategy\nEnvironment: Waterfall, Erwin 9.5, Rational Requisite Pro, Oracle 11g, MS Office, MicroStrategy 9.5x, Oracle NetSuite, SAP \u2013 ERP, SAP - Ariba\nBusiness Systems Analyst\nVF Corporation \u2013 Greensboro, NC\nMarch 2013 \u2013 December 2013\nDescription: The company has a diverse e-commerce portfolio of brands that meet consumer needs across a broad spectrum of activities and lifestyles. The objective of the project was to develop a system that can process large volumes of data structures to analyze sales and operational raw data by simulating different scenarios and provide the business users with predictive analysis that would help in accurate and efficient strategical planning and marketing. The project involved implementing Bigdata, developing a web application and enhancing the mobile application designed for Chinese and Taiwanese markets.\nRoles and Responsibilities:\nOrganized, set up and facilitated JAD sessions, and functional design review meetings\nProvided assistance with creating process flow and data flow diagrams. Assisted architects in the creation of Physical and Logical architecture diagrams by clearly communicating requirements\nUpdated business specification documentation based on updates to functionality of the systems\nCollaborated with Program manager, Product Management to revise User Acceptance Criteria and change management plan by integrating the insights from focus groups and impact analysis. Facilitated UAT sessions to demo features and functionalities with the end users and obtained feedback\nReviewed the Data Models and updated Data Mapping Documents as needed\nPerformed SQL data validation for the data integrity in the back-end systems\nWorked closely with Product management and Program manager in designing the change management plan using Rational Clear Quest and analyzed impact, incorporated feedback from business focus groups \nDesigned and created business dashboards by using Tableau to extract key insights such as resource utilization and changes in inventory levels\nPerformed GAP analysis to derive requirements for existing system enhancements\nInvolved in writing Feature Files in the Gherkins language for Cucumber testing and have experience and collaborated in the Cucumber \u2013 Selenium integrated automated testing environment for BDD\nRequested updates and was actively involved with Data Dictionary maintenance \nDesigned the Use Case Diagrams and coordinated with multiple resources in creation of Activity Diagrams and Sequence Diagrams using MS Visio\nWorked with QA team to prepare master test plan, test strategy, test cases, and test scripts to perform functional testing, black-box testing, GUI testing, system testing, performance testing, stress testing, sanity testing, regression testing, and supported user acceptance testing (UAT)\nEnvironment: Agile (SAFe), MySQL, Hadoop, Apache Flume and Scoop, Apache Spark, Apache Tomcat, Cucumber, Selenium, XML, HTML, JSON, HTTP, REST API, MS Visio, Load Runner, Tableau 7.0.12, MS Project\nBusiness Analyst\nZomato \u2013 Bangalore, India\nAugust 2012 \u2013 March 2013\nDescription: Zomato is an Indian restaurant search and discovery service founded in 2008. To ease data management and reporting, Zomato was looking for an end-to-end data warehouse solution that can help them improve performance of their multiple client databases and offload reporting from their production server. Our team provided solutions to extract the data from the database for predictive analysis.\nRoles and Responsibilities:\nIdentifying customer inputs, activities and release calendar\nEstimating cost/effort, forecasting defects, and managing uncertainty. Streamlined project management practices, eliminated role confusion and ensured tasks are not overlooked by creating RACI matrices\nDocumented the Traceability Matrix for tracing the Test Cases and requirements related to them\nLed and innovated in the expansion of the group's use of issue tracking system Jira 5.0\nUsed SharePoint for uploading project deliverables and maintaining the documents\nWas responsible for handing requirement churn, by successfully transitioning from longer release cycles to shorter ones, reducing risk and increasing flexibility to remain competitive\nInvolved in technical writing, API documentation and end user manuals and documentation using MS Office Suite (Word, MS Access, Excel, Power point)\nInvolved in Regression Testing every new build in UAT, QA and Pre-Production Environments\nAssisted QA team in Unit, Integration, and Regression testing to validate report and mapping functionality\nExtensively worked in Data Requirement Analysis and Data Mapping for ETL process using the Informatica PowerCenter Suite including modules like Analyzer, Workflow Monitoring, and Workflow Designer\nEnvironment: Agile, Jira 5.0, Informatica PowerCenter 9.0.x, HP ALM, HP QTP 11.0, Oracle 10g, Crystal\nBusiness/Data Analyst\nAlibaba.com - Singapore\nJune 2011 \u2013 July 2012\nDescription: The organization is a multi-national e-commerce consumer goods corporation whose portfolio includes a wide range of household goods. When the company first registered in Singapore, we were one of the first offshore teams to provide services that included various projects/programs building/maintaining portal/web applications; enhancing, maintaining and supporting Alibaba web sites, development of e-commerce Applications for the Singapore market\nRoles and Responsibilities:\nConducted requirement gathering with end users and converted them into business specs and developed the specs into application design and development\nConducted Gap Analysis to check the compatibility of the existing infrastructure with the new Business Requirements that helped provide key insights\nPerformed data analysis, data cleansing and presented results using SQL, SSIS, Excel\nCreated Project Plan, Critical Path, and Gantt Chart using MS Project\nAssisted in the construction of Logical models and Entity Relationship Design using ERwin\nWorked as a unit BA lead with Data Warehouse team for transferring information between the business, developer, data modeler and testing team\nInteracted with third party vendors to help the company with mission critical applications\nPerformed basic DML and DDL Queries to probe and perform CRUD operations on the existing database under supervision of Lead Developer\nCreated SSIS packages to load data from Oracle to SQL Server using various transformations in SSIS\nCreated different report views like Table, Pivot, Graph, Narrative, Static, Filter for testing purpose\nEnvironment: Waterfall-SDLC, Oracle 10g, Microsoft SQL Server, SSIS, MS Office Suite, Erwin 8, MS Project\nSQL Developer\nFlipkart \u2013Bangalore, India\nJune 2009 \u2013 May 2011\nDescription: The organization is an e-commerce company, providing technology solutions and services for consumers, small-businesses and enterprises. Flipkart maintained several legacy product data repositories that would feed customer facing portals. These product data repositories formed the core of the client\u2019s ability to configure components and products based on customer preferences to form a complete product. Our team was involved in building the integration bridge between PIR\u2019s and EDR.\nRoles and Responsibilities:\nDeveloped Use Case diagrams, business flow diagrams, Activity/State diagrams and Sequence diagrams using Rational Requisite Pro so that developers and other stakeholders can understand the business process\nEstablished Traceability Matrix using Excel to trace completeness of requirements in different SDLC stages\nInvolved in requirements gathering of the work streams by meeting with the business user and baselining details\nEvaluated POS in regard to establishing stores across the country by 2015\nWrote basic SQL queries for retrieving data from Database using Microsoft SQL Server\nUsed SQL queries to modify the application with different data sets\nExperienced in Data ETL using various tools such as SQL Server Integration Services (SSIS)\nAssisted SQL Lead in using SQL Server Profiler that aided in profiling server resource usage and optimization\nEnvironment: Waterfall, Windows, Microsoft SQL Server, SQL Server Profiler, Visual Studio, SSIS, Crystal\n\t ", "years_experience": 8, "score": 0.32104204234525646, "path": "C:\\Users\\admin\\AI-Resume-Ranker\\data\\uploads\\Yohan_BSA.docx"}, {"filename": "Candidate105_Sr_BSA_Resume.docx", "text": "Candidate105                                \nSr. Business System Analyst\n\nPROFESSIONAL SUMMARY:\n    Proactive, confident, goal-oriented, pragmatic and attentive Sr. Business Analyst and Business system analyst with over 8 years of experience in Retail, Banking, E-commerce domains with a focus on designing, development, implementation of business processes and system solutions with the addition of an in-depth domain understanding of the entire retail business operations.\n   -Certified Scrum Master with a thorough understanding of Agile principles and Scrum ceremonies.\n   -Experienced and have detailed knowledge about Order Management System, Retail POS Applications, Customer Relationship Management (CRM i.e. Loyalty Program), E-commerce platforms.\n    -Extensive knowledge, expertise and involvement in various phases of Software Development Life Cycle (SDLC) in Waterfall, Scrum, Agile (Extreme Programming, Test-Driven Development) and Scrum-Waterfall Hybrid and SAFe software development lifecycle environments.\n   -Skilled in conducting and facilitating Requirement Gathering Activities to gather SMART (Specific, Measurable, Attainable, Realistic and Timely) requirements using diverse elicitation techniques including, but not limited to Interviews, Requirement Workshops, Prototyping, Survey/Questionnaire, JAD Sessions and Brainstorming.\n   -Skilled as a liaison between technical team and management, change agent and a servant leader with an ability to communicate technical information to non-technical stakeholders. Experience of mentoring the team and providing business value oriented conflict resolution.\n   -Excellent writing and documentation skills in creating, modifying and analyzing Business Requirement Document (BRD), Functional Requirement Document (FRD), System Requirements Specification (SRS), Use-Case Specification, High Level and Low-Level Documents (HLD/LLD), User Story. \n   -Detailed understanding of RACI Matrix (Responsible, Accountable, Consulted and Informed) and prepared Requirement Traceability Matrix (RTM) to map requirement with its origin throughout the process to verify adherence.\n   -Proficient in Requirement Management, Sprint Planning, Issue Tracking, Impediment Management, and Work Allocation using tools like JIRA, RALLY, and HP ALM.Helped PM in preparing various artifacts related to the Planning phase.\n   -Thorough understanding of the preparation and use of Unified Modeling Language (UML) diagrams including Process Diagram, Activity Chart, Use Case Diagram, Sequence Diagram, Data Flow Diagram and ER Diagram using MS Visio and Draw.io to support system design and helped in identifying primary actors and business process flow around entities.\n   -Experienced in creating Prototypes, Wireframes and Mock-Up Screens to visualize Graphical User Interface (GUI) as to understand the layout of the interface using tools like Balsamiq MockUps and MockUp Screens.\n   -Proven Track Record in conducting AS-IS and TO-BE analysis, GAP Analysis, Risk Analysis, Cost-Benefit Analysis and SWOT Analysis to assist PM in Release Planning as well as PO in setting better stakeholder expectation.\n   -Integrated Salesforce Force.com IDE platform with Amazon Web Service(AWS), Oracle Database 11g and integrated IBM Sterling Order Management System with IBM Websphere commerce using SOAP, RESTful APIs. Experienced in enhancing data warehouse with extensive knowledge of Data mapping, Dimensional Modeling, ETL jobs, OLAP operations.\n   -Experience in Service Oriented Architecture (SOAP, WSDL, REST, XML, JSON, HTTP).\n   -Well versed in writing SQL queries and SOQL queries in order to do data manipulation for analysis and reporting.\n   - Assisted Sr. Business analyst in Data Migration project where identified target driven priorities, part of data migration strategy and redefined data for migration (i.e. based on Relevance, Accuracy, Integrity, Completeness, Compliance etc).\n   -Participated in various Scrum Ceremonies including Daily Scrum, Sprint Planning, Sprint Review, Sprint Retrospective and Backlog Refinement meeting as part of the management framework. Helped PO in maintaining information radiators.\n   -Experienced in conducting Scrum Training Workshops and User Story Writing workshops to familiarize stakeholders with Scrum process skeleton and concept. Helped Scrum teams in User story point estimations using various techniques.\n   -Adept at handling Requirement Churns in product backlog by performing Impact Analysis as part of the change request.\n   -Well versed with backlog prioritization techniques (MoSCoW, Business Value, Kano, And Technical Risk) and estimation techniques (T-Shirt sizing, Planning Poker, Complexity buckets, value streaming and technical risk-based estimations) to help product owner in picking Product Backlog Items (PBIs) to commit to in the iterations (Sprints).\n   -Proven expertise in developing Test Cases, Test Plans, Procedures and Traceability Matrix and documenting defects using HP QC and HP ALM. Collaborated and monitored Black-Box Testing, Smoke Testing, User Acceptance Testing (UAT) and Regression Testing.Helped testing team in understanding test-scenarios regarding Interface testing & API testing.\n   -Created Training Script and conducted Training Sessions for the end users in the production environment once the product went live and was ready to use.\n   -Experienced in working with Three-Tier Architecture, and Client-Side Verification and Server-side Verification.\n   -Thorough understanding of Business Intelligence (BI) and experience in creating interactive, analytical dashboards using Tableau and IBM Cognos for decision making.\n\n\n\nACADEMICS: \n-Bachelors in Technology, Computer Science. \n\nCERTIFICATIONS:\n-Certified Scrum master \nPROFESSIONAL EXPERIENCE: \nCLIENT: KOHL\u2019s, MENOMONEE FALLS-WI                                                                           Jan2017-Present ROLE: Sr. Business System Analyst\nDescription: The scope of the project was the two-part integration of SALESFORCE with our existing e-commerce platform. Integration of Sales cloud was to capture range of information about customers using e-commerce platform. This insight helped marketers to develop targeted promotions and offers matching customer\u2019s personal taste. In the second part, we integrated SALESFORCE with our legacy CRM system for a complete view of customer data using different retail channels. Used Marketing cloud to plan, implement, execute, monitor and analyze campaigns across multiple channels.\nRESPONSIBILITIES:\n-SFDC requirement elicitation and coached junior business analyst in converting business requirements to technical specifications, designing the solution and work closely with Stakeholders, Architects, Developers and Deployment teams.\n-Implemented the requirements for users and customers on Salesforce\u2019s Force.com IDE Plug-in using Eclipse.\n-Managing users, Public Groups, Profiles, and Roles within the Salesforce CRM, this involved designating access to the applicable user within the user hierarchy.\n-Created Formula Fields, Validation Rules, Workflows and approvals, Process builders for the flexibility and functionality of force platform application and to help automate the different types of actions based on organization processes.\n-Worked with integrating Salesforce with 3rd party systems using web services and App exchange for greater usability.\n-Worked with integration platforms, API integrations SOAP, REST with other systems for extracting the data from the home-grown applications by using the home-grown web services.\n-Worked closely with Java development team in Agile Environment to build and support current dashboards and portals.\n-Supporting the Sales cloud implementation, Service cloud modules, Salesforce SF1, Client interfacing, Chatter when converting leads and dealing with cases for company-wide discussions.\n-Worked with development and deployment of code through changesets from sandbox to production environments.\n-Designed and created reports to determine most popular items by age range, geography and profession and other parameters to make informed decisions on inventory.\n-Used SOQL & SOSL Queries with consideration to governor limits for data manipulation needs of the application using platform database objects and to support various item-level analytics on database objects.\n-Hands on experience on Salesforce Lightning for Customizing Reports and Dashboards for business use.\n-Used Data Loader for insert, update and bulk import or export of data from Salesforce S-Objects Used it to read, extract, and load data from a comma-separated value (CSV) file.\n-Used Force.com developer toolkit including Apex Classes, Apex Triggers, Visualforce pages to help in the development of custom business logic.\n-Integrated the web services by generating the necessary stubs from the WSDL files for extracting the data from the home\n -grown applications by using the home-grown web services.\n-Designed the Test Cases in SOAPUI for API testing, Test Plans for all the processes using CONFLUENCE and tracked defects on JIRA, hence was part of Black Box Testing, and was part of Interface Testing, System Testing.\n-Used CUCUMBER for Behavior Driven Development and used MAVEN to build the project and connect it to JENKINS CI Tool.\nENVIRONMENT:Agile-Scrum,Eclipse,Force.com IDE,Workbench,SOQLxplorer,Jenkins,JIRA/Confluence,Oracle Database server11g,Amazon DynamoDB(AWS),.Net framework 4.5,Maven,Custom Import Wizard,Apex Data loader,MS Windows,LINUX, Salesforce integration tools, Connect offline,Apex Explorer,Cucumber, Apex Classes/Controllers, Test Classes, Apex Triggers, Visualforce (Pages, Component & Controllers),Standard objects, Workflow & Approvals, SOQL, SOSL, Custom objects, S-Controls, Analytic Snapshots,SOAPUI,Sandbox environments.\n\nCLIENT: LBRANDS, COLUMBUS-OH                                                                                        Nov2015- Jan2017     \nROLE: Sr. Business System Analyst\nDescription: Scope of the project was to integrate IBM Sterling order management system to our e-commerce platform which includes functions like accessing and updating customer orders, to provide the retailer with an operational foundation to towards seamless, integrated omni-channel solution.\nRESPONSIBILITIES: \n-Elicited requirements together with the Product Owner to integrate IBM Sterling order management system with LBrands\u2019s IBM WebSphere commerce platform used for a storefront for the complete view on order as well as inventory database.\n-Implemented DOM with role-based access for multiple supply chain participants, created a centralized database as single order repository to monitor complete order lifecycle (inventory availability, order routing, payment processing, shipment).\n-Integrated two systems by establishing WebSphere enterprise service bus which translated calls and services from one system to other in terms of API calls to support CRUD operations.\n-Collaborated with sales teams, independent stores, transportation partners, and customer warehouse to do Out-of-the-box configurations for seamless delivery execution and monitoring of services. \n-Used CONFLUENCE for drafting the User stories and managing the requirements and integrating with JAMA Project Management Tool ensuring that all the teams are in sync at all times.\n-Created acceptance criteria for stories in JIRA and Participated in all Scrum Ceremonies Leading Sprint Planning session.\n-Validated completed stories that passed all the acceptance criteria on behalf of or with the Product Owner in exceptional scenarios and Production validation on behalf of or with the Product Owner/Manager. \n-Reviewed the Processes, Tools, Techniques used by various Development & QA Teams along with Scrum Master to provide leadership and guidance for improvement with the goal to deliver high-quality Product.\n-Ensured that the Development & QA teams, based on the requirements, have completed the appropriate level of Test. \n-Planning along with Traceability back to the requirements to ensure complete coverage of the Program increment. Worked Extensively with the Dev Ops Team for the successful implementation of the Project.\n-Helped in designing Test Cases in SOAPUI for API testing, Test Plans for all the processes using Confluence, hence did the Black Box Testing and was part of System Testing.Tracked the identified defects using JIRA.\n-Used CUCUMBER for Behavior Driven Development and used MAVEN to build the project and connect it to JENKINS CI Tool.\n-Assisted Junior Business Analysts with the development of Use Case Diagrams, Activity Diagrams, Sequence Diagrams using MS-Visio by analyzing the business processes to describe the functional requirements of the system.\n-Created and updated training manuals and Provided advanced business knowledge and technical support for business requirements development.\n-Used BI tools like Tableau for reporting and analyzing most profitable channel partners, order replenishment etc. \nENVIRONMENT: Agile-Scrum, IBMWebSphereCommercev7, IBMSterlingordermanagementsystemv9.3, JIRA, CONFLUENCE, JAMA, MySQL-DB, MS-VISIO, APAR fixes, JMS API, XML, Tableau, SOAPUI, J2EE, Cucumber, Jenkins, Maven.\n\n CLIENT: STATE OF FLORIDA, TALLAHASSEE-FL                                                                    Jan2014-Oct2015\n ROLE: Sr. Business System Analyst \n Description: Scope of the project was to enhance existing enterprise data warehouse for agricultural department of State.\n Improved the data definitions and logical structures related to current data-warehouse in order to increase accuracy \n and precision of statistical calculations done agricultural data collected through surveys. \n RESPONSIBILITIES:\n -Conducted meetings and discussions sessions with stakeholders of the agriculture department of State government to elaborate on the business drivers, vision, and mission, high-level requirements and project objectives.\n -Facilitated Joint Application Development (JAD) sessions to design Data Flow Diagrams (DFD\u2019s) for the different phases of requirements. Acted as liaison between end users and technical team to address issues and recommend solutions.\n -Documented workflow for each of the batch process feeding to the master database.\n -Analyzed source database, identified cardinalities and Created conceptual, logical ER diagrams. Responsible for identifying different data sources and conducted data profiling using Informatica.\n -Involved in identifying facts and dimensions and created data mapping document. Coordinated with database developer to create star dimension modeling and enhanced the target Datawarehouse with respect to agricultural data parameters.\n -Analyzed ETL jobs and mappings to document the existing transformation logic and understand ETL strategies used.\n -Assisted in developing different dimensions such as slowly changing, rapidly changing, and conformed dimensions and fact tables with facts and measures as per business requirement.\n -Led BI and dashboards development and trained core executive users on usage of developed BI platform.\n -Maintained metadata of the data warehouse in the CA Platinum repository by identifying different types of metadata.\n -Assisted testing team and Responsible for creating and monitoring Requirements traceability matrix, test cases, test plans, defining User Acceptance criteria and tracking defects using HPALM.\n -Assisted data analyst in enhancing OLAP cubes by communicating new business rules and Performed Rollup, Drill down, Slice and Dice operation to provide secured, timely, accurate insights.\n -Helped to create operations document and knowledge transfer to the operations team.\n\n ENVIRONMENT: Waterfall-SDLC, MS-Access, MS-Excel, MySQL-DB, MS-Office, Informatica, HPALM, PowerBI, CA Platinum Repository, Oracle Database 10g\n \n CLIENT: OFFICE DEPOT, BOCA RATON-FL                                                                            Sept2012-Nov2013\n ROLE: Business System Analyst \nDescription: Office Depot is an American office supply retailing corporation. The project consisted of designing a new in-house order management system and in-store billing application. Respective store managers and customer sales staff order the products they need from the store's system, and in 3 days they receive the products they had ordered, the project was to replace with the new order management system and in-store billing application.\nRESPONSIBILITIES:\n-Conducted feasibility analysis for new requirements, cost-benefit analysis and assisted the project manager in risk analysis. \n-Analysed data flow and business requirements to create business requirement document(BRD), functional requirement document (FRD) etc. \n-Created Activity Diagrams, Sequence Diagrams using UML methodologies for easy understanding using MS-VISIO.\n-Created User Stories from documented FRD to be estimated in Sprint Planning and Backlog Grooming meetings. Helped Product Owner to define acceptance criteria for user stories and prioritize those user stories based on business value.\n-Responsible for implementing, coordinating and documenting all business processes for (web-based) Internet billing customers (e.g. creating invoices on a recurring basis based on pricing plan, discounts applied specifically to SKUs etc).\n-Helped the team in estimating the complexity of the user story by using Planning Poker and other estimation techniques.\n-Used JIRA for deploying task and Issue Tracking, User Story Management.Also linked the sprint burndown charts and release burnup chart to the sprint in execution to track the progress of the sprint as well as a release for future planning.\n-Assisted Business Intelligence (BI) team in creating dashboards using Tableau as required by the end users.\n-The effort involved developing extensive SQL Queries and Procedures, utilized to perform database integrity testing, ensuring various data table entries, data validity, and data integrity w.r.t transactional data.\n-Performed logical data modeling and created ER diagrams and used SQL queries for manipulating data.\n-Prepared Requirement Traceability Matrix (RTM) report to ensure that every Business requirement is adhered to achieve the expected performance and have visibility on complete test coverage w.r.t requirements elicited.\n-Helped in development of Test Strategies and creation of Test Scenarios, Test Environment, and Test case.\n-Conducted User Acceptance Test(UAT) by setting up UAT environment utilizing HPALM.\n ENVIRONMENT: Scrum-Waterfall(Hybrid), JIRA, Tableau, SQL Server, .Net framework3.5, Mock-Up Screens, UML, MSOffice Suite, MS Visio, HP-ALM.\n\n CLIENT: Scrum Master BASKET, June2011-Aug2012\n ROLE: Business Analyst \n Description: Scrum Master Basket is an Online supermarket for food and grocery. The scope of a project is to integrate and enhance the existing Web Based E-commerce platform using Microsoft Commerce server to provide Product Catalog, Targeting, and Profiling. The enhanced application provides utilities for selecting products, shopping cart, checkout, payment modes, and membership login for the discounts.\n RESPONSIBILITIES:\n -Elicitated requirements by interview, surveys, and prototyping with the stakeholders.\n -Conducted GAP Analysis for the AS-IS and TO-BE Processes to identify and validate the requirements and then developed the strategy to close the gap as to have a better understanding of server requirements.\n -Worked with various e-commerce modules for supply chain management, Internet marketing, online transaction processing, electronic data interchange (EDI), inventory management systems, and automated data collection systems.\n -Conducted technology reviews of various modules for the team. Built web application prototype using iRise.\n -Actively Participated and assisted project manager in writing Project scope statement, Work Break Down structure etc.\n -Documented the server-based and security-related requirements within Share Point and using Windows Active Directory.\n -Created Mock-ups and Wireframes to help facilitate the conversion of business needs into technical requirements.\n -Prepared Functional specification document(FRD) based on the understanding of the required functionalities-entirely re-engineering the existing process to make it more efficient and accurate to deliver planned business value.\n -Conducted Requirement Validation through requirement reviews and use of prototyping and planned how requirements should be verified by establishing Acceptance tests and ensuring all requirements supports delivery of value to the business.\n -Designed and developed Use-case diagrams, Sequence Diagram and Activity Diagram using MS-Visio.\n -Created Physical Inventory process (like creating barcode labels, create purchase orders, create a sales order and pick/issue along with cycle count) for e-commerce consisted of data integration and inventory processes with the third-party vendor.\n -Planned and documented procedures for data processing and prepared data flow diagrams for the application.\n -Designed workflows and allocated permissions within SharePoint.\n -Assisted in Unit Testing and performed User Acceptance testing and documented detailed results.\nENVIRONMENT: Waterfall-SDLC, MS-SharePoint 2007, Microsoft commerce server 2009, .Net framework 3.5, MS Office, SQL Server, MS Project, MS Visio, Mock-up screens, iRise, MS Access.\n\nCLIENT: Barclays BANK                                                       Nov2010-May2011\nROLE: Business Analyst \nDescription: Scope of the project was migrating the bank\u2019s core legacy system (Core Banking System i.e. CBS) onto a consolidated Finacle Platform for retail banking. Legacy system was increasingly expensive to maintain and had several performance issues. Also, it lacked various vital functionalities such as NEFT/RTGS payments, FLEXI deposits, SWEEPs etc. New System brought 24x7 banking to back office, enhanced straight-through processing, reduced manual intervention which eventually reduced process time and error.\nRESPONSIBILITIES: \n-Assisted Senior Business Analyst in Client Meetings and scribed JAD sessions to allow users/stakeholders from multiple teams to communicate and understand how many systems Finacle had to interface with and to avoid Scope creep.\n-Acted as the liaison between the Business Users and the Technical teams towards providing feedback on the implication of requests and escalating/resolving issues and conflicts. Worked with different commercial card team in knowing what exactly bank's client wants.\n-Created project artifacts like Business Requirements Document(BRD), Business Rules Spreadsheet BRS, Use Case Document, Functional Requirements Specification(FRS), Data Mapping Document.\n-Employed UML methodology and thus facilitated the creation of Use Case, Activity, Sequence and Communication Diagrams as to identify the actors (primary stakeholders) and to do behavioral modeling using MS Visio.\n-Assisted the Project Managers in the development of standard project documentation like project scope statement etc\n-Created documents like the Business Glossary, Actors Catalog and Data Dictionary to facilitate the other teams in understanding client business.\n-Involved in the overall requirements management and to build the Requirements Traceability Matrix (RTM) in order to trace the origins of requirement with respect to written test-cases using HP Application Lifecycle Management.\n-Facilitated Requirements Review Sessions with the Development and Testing teams.\n-The Facilitated decision on proposed requirements changes after baseline as part of the Change Management process, thus managing Volatility and Integrity of the requirements. Also helped teams in baselining changes w.r.t business users.\n-Created UAT test case scenarios by covering all business process lifecycle functionalities of bank\u2019s offerings together with business stakeholders. Helped QA teams in setting up UAT environment by providing data definitions for test data.\n-Reviewed Test cases, Participated in Defect-Review meetings with software developers, QA engineers, managers, suggested enhancements to the existing application from business perspectives, provided solutions to existing bugs.\n-Was part of Data Migration strategy from legacy system to Finacle, assisted Sr. business analyst and participated in Deployment Planning meeting.\nENVIRONMENT: Waterfall SDLC, MS-Visio, MS-Office, MS-Project, HP-ALM, Linux, Finacle, iCEDQ, SOA test, MS-SQL Server.\n\nCLIENT: OLA                                                               Aug2009-Oct2010                             \nROLE: Jr. Business Analyst    \nDescription: OLA is one of the largest companies. The scope of the project was to develop Payment Verification and Processing System which leads to secured payments for existing and new customers using Credit or Debit cards.\nRESPONSIBILITIES:\n-Interacted with the customers and internal stakeholders to elicit user requirement to make the existing system safer for a transaction by making it PCI DSS compliant as increase control over cardholder data and reduce credit card frauds.\n-Documented requirements using Functional Requirement Document, Business Requirement Document templates.\n-Formed a bridge between the Project Manager, Stakeholders and IT teams with effective presentations.\n-Planned and organized team engagement activities to encourage self-organization among the team.\n-Assessed business priorities and advised business units by performing Risk Analysis along with Sr. Business Analyst.\n-Created UML Diagrams like Activity diagram, Sequence diagram, Use case using MS-Visio.\n-Tracked and reported timely performance against plans and performed GAP analysis as to recommend corrective measures to improve productivity. Did Impact Analysis for change management along with project manager, stakeholders.\n-Identified the logical relationship between entities and primary keys to making every payment per transaction unique in the database. Helped data architect in database designing and other technical teams by participating in E-R modeling.\n-Participated in the development and testing of Service Oriented Architecture, REST, SOAP APIs.\n-Helped in preparation of the Test Plan with the testing team as well as ensuring testability, reliability, usability, maintainability, the performance of the application through User Acceptance Testing(UAT).\n-Involved in incident reporting and change management procedures using Use Case.\n-Maintained and shared documents using MS SharePoint. \n-Managed test cases and logged defects using HPQC tool.\nEnvironment: Waterfall SDLC, HP QC, Payment Processing, Payment gateways, MS Project, MS Visio, MySQL Server, SharePoint, MS Office (MS Excel, MS Word, MS PowerPoint)10, HPQC, SOAtest, Linux, MS-SQL server.", "years_experience": 8, "score": 0.29395109495103283, "path": "C:\\Users\\admin\\AI-Resume-Ranker\\data\\uploads\\Candidate105_Sr_BSA_Resume.docx"}, {"filename": "Candidate199.docx", "text": "Candidate199\nSr. Java/ Hadoop Developer\n@:candidate19909@gmail.com                                                                                   \u00a9: (646) 801-5340\n\nResponsibility:\n9+ years of experience in SDLC with key emphasis on the trending Big Data and Java Technologies - Spark, Scala, Spark Mlib, Hadoop, Tableau, Cassandra, Java, J2EE.\nArchitect, design & develop Big Data Solutions practice including set up Big Data roadmap, build supporting infrastructure and team to provide Big Data.\nArchitecting, Solutioning and Modeling DI (Data Integrity) Platforms using sqoop, flume, kafka, Spark Streaming, Spark Mllib, Cassandra.\nStrong experience in migrating data warehouses and databases into Hadoop/NoSQL platforms.\nStrong expertise on Amazon AWS EC2, Dynamo DB, S3, Kinesis and other services\u00a0\nExpertise in data analysis, design and modeling using tools like ErWin.\nExpertise in Big Data architecture like hadoop (Azure, Hortonworks, Cloudera) distributed system, MongoDB, NoSQL.\nExpertise in Service Oriented Architectures (SOA- Web Services) using Apache Axis, WebLogic, JBoss and\u00a0EJB\u00a0Web service framework.\u00a0\nUsed Mule ESB in designing the application as a middleware between the third-party system and the customer side system.\nHands on experience on Hadoop /Big Data related technology experience in Storage, Querying, Processing and analysis of data. \nExpertise in archtiecting Big data solutions using Data ingestion, Data Storage\nStrong Experience in Front End Technologies like JSP, HTML5, JQuery, JavaScript, CSS3. \nWorked on windows server AD configuration and Kerberos protocol.\u00a0\nExperienced with Perl, Shell scripting and test automation tools like Selenium RC, Web Driver and Selenium Grid.\u00a0 \nDeveloped Python Mapper and Reducer scripts and implemented them using Hadoop streaming. \nExperienced in customizing Selenium API to suit in testing environment.\u00a0 \nIntegration of Mule ESB system while utilizing MQ Series, Http, File system and SFTP transports.\nSolid Knowledge of My SQL and Oracle databases and writing SQL Queries.\u00a0\nProficient in developing the application using JSF, Hibernate, Core\u00a0Java, JDBC and Groovy and Grails presentation layer components using JSPs, Java script, XML and HTML Cassandra , Cucumber, OLE and Continuous deployment,  API , Angular JS along with Web service , REST , GemFire , Rabbit MQ , Spring Boot..\u00a0\nExperience in Back End Development including Web services, Data service layers\u00a0with service desk experience.\nDesigned and coded Hibernate, struts for mapping, configurations and HQL for enhancement and new module development of Transport Optimization, Planning and Scheduling Web app.\u00a0\nUsed Groovy and Grails with spring, Java, J2EE for user interface.\nInitiated the Automation framework using\u00a0Selenium Web Driver to run test cases in multiple browsers and platforms.\nHighly motivated software engineer and experience in developing in web applications using Java script, Backbone.js and Coffee script technologies.\u00a0\u00a0 \nUtilized integration Patterns, integration tools, EAI, Transformations, XML Schemas, and XSLT. \nUsed Quartz connector to schedule the batch jobs. \nArchitected Integrations using Mule Soft ESB environments for both on premise and Cloud hub environments. \n\u2022 Experience in developing interfaces between Sales force and Oracle ERP using Informatica Cloud/Mule ESB technologies. \n\u2022 Implemented flows for sales force outbound / inbound calls and business process. \n\u2022 Experience in Mulesoft Any point API platform on designing and implementing Mule APIs.\nGood knowledge on Soap UI tool to unit testing SOA based applications.\u00a0\nAbility to understand and use design patterns in application development. \nVery good knowledge in different development methodologies like SDLC and Agile.\u00a0\nExperienced in developing applications using HIBERNATE (Object/Relational mapping framework) and involves in working on service desk client.\nExperienced in developing Web Services using JAX-RPC, JAXP, SOAP and WSDL. Also knowledgeable in using WSIF (Web Services Invocation Framework) API. \nExperience in writing database objects like Stored Procedures, Triggers, SQL, PL/SQL packages and Cursors for Oracle, SQL Server, DB2 and Sybase. \nJava 8, J2EE, spring (MVC, Data-JPA, Security),Hibernate, Jenkins or Bamboo, HTML 5, JSP, JavaScript, JQuery, Ajax, Angular JS.\nTechnical Skills\n\nBig Data: Hive, Hadoop, oozie, sqoop, Storm, Kafka, Elastic Search, HDFS, Zoo Keeper, Map Reduce, hive, pig, spark, flume.\nJ2EE Technologies: Servlets, JSP, JDBC, JNDI, OSGI, EJB, RMI, ASP.\nProgramming Languages:  Java 8, C, C++, Pig Latin, HQL, R, Python, XPath, Spark.\nFrameworks: Jakarta Struts, Spring, Spring MVC, JSF (Java Server Faces), Hibernate, Tiles, I Batis, Validator, Cucumber, OLE and Continuous deployment, micro services, Groovy.\nWeb Technologies: HTML, DHTML, Cassandra , API , Angular JS along with Web service, REST, Gem Fire, Rabbit MQ , Java script with J query, Python, Ext JS, AJAX, CSS,CMS, Yahoo UI, ice faces API\u00a0, Angular, Node.js, Backbone.js.\nXML Technologies: XML 1.0, XSLT, XSL, HTML5, DHTML, J query,, XSL / XSLT /XSL-FO, JNDI, LDAP, SOAP, AXIS 2\u00a0\nApplication/Web Servers: IBM Web Sphere 5.X/6.0/7.0/8.0, IBM HTTP server 8.x, Web Logic 7.x/8.x/9.0, Web Logic Portal 5.x, J Boss 4.0, j BPM, Apache Tomcat, OC4J, Docker.\nNO SQL Data Base: Cassandra, mongo DB\nDatabases: Oracle 12c /10g/11g, SQL Server, My SQL, DB2. \nMessaging Systems: JMS, IBM MQ-Series\u00a0\nIDE Tools:\u00a0IBM Web Sphere Studio Application Developer (WSAD) RSA, RAD, Eclipse /RCP, J developer, Net Beans .\n\nProfessional Experience\n\nCigna, Hartford, CT\t\t\t                                                                              Aug\u201915 \u2013 Till Date\nSr. Java/Hadoop Developer\n      Roles & Responsibilities:\nImplementation of Big Data ecosystem (Hive, Impala, Sqoop, Flume, Spark, Lambda) with Cloud Architecture\u00a0\nUsed Talend for Big data Integration using Spark and Hadoop\nUsed Microsoft Windows server and authenticated client server relationship via Kerbros protocol.\nExperience on BI reporting with At Scale OLAP for Big Data.\nImplemented solutions for ingesting data from various sources and processing the Data-at-Rest utilizing Big Data technologies such as\u00a0Hadoop, Map Reduce Frameworks, HBase, Hive\nLoaded and transformed large sets of structured, semi structured and unstructured data using Hadoop/Big Data concepts.\u00a0\nI have Working experience in Middleware Integration product Mulesoft \nDesigned and Developed Real time Stream processing Application using Spark, Kafka, Scala and Hive to perform Streaming ETL and apply Machine Learning.\nIdentify query duplication, complexity and dependency to minimize migration efforts\u00a0\nTechnology stack: Oracle, Hortonworks HDP cluster, Attunity Visibility, Cloudera Navigator Optimizer, AWS Cloud and Dynamo DB.\nExperience in AWS, implementing solutions using services like (EC2, S3, RDS, Redshift, VPC)\nWorked on Talend Magic Quadrant for performing fast integration tasks.\nWorked as a Hadoop consultant on (Map Reduce/Pig/HIVE/Sqoop).\nWorked with Spark and Python.\nWorked using Apache Hadoop ecosystem components like HDFS, Hive, Sqoop, Pig, and Map Reduce.\nLead architecture and design of data processing, warehousing and analytics initiatives.\nWorked with AWS to implement the client-side encryption as Dynamo DB does not support at rest encryption at this time.\u00a0\nExploring with the Spark for improving the performance and optimization of the existing algorithms in Hadoop using Spark Context, Spark-SQL, Data Frame, Pair RDD's, Spark YARN. \nUsed Data Frame API in Scala for converting the distributed collection of data organized into named columns. \nPerformed data profiling and transformation on the raw data using Pig, Python, and Java.\nExperienced with batch processing of data sources using Apache Spark. \nDeveloping predictive analytic using Apache Spark Scala APIs. \nInvolved in working of big data analysis using Pig and User defined functions (UDF).\nCreated Hive External tables and loaded the data into tables and query data using HQL.\nUsed Sqoop to efficiently transfer data between databases and HDFS and used Flume to stream the log data from servers.\u00a0\nInvolved in developing and configuration of enterprise components using Mulesoft ESB. \nImplement enterprise grade platform (mark logic) for ETL from mainframe to NO SQL (cassandra).\nExperience on BI reporting with At Scale OLAP for Big Data.\u00a0\nResponsible for importing log files from various sources into HDFS using Flume\u00a0\nWorked on tools Flume, Storm and Spark.\u00a0\nExpert in performing business analytical scripts using Hive SQL.\u00a0\nBest practices for designing integration modules using ESB& Data Integrator modules    \nImplemented continuous integration & deployment (CICD) through Jenkins for Hadoop jobs.\u00a0\nWorked in writing Hadoop Jobs for analyzing data using Hive, Pig accessing Text format files, sequence files, Parquet files.\nExperience in different Hadoop distributions like Cloudera (CDH3 & CDH4) and Horton Works Distributions (HDP) and MapR.\nExperience in integrating oozie logs to kibana dashboard.\u00a0\nExtracted the data from MySQL, AWS RedShift into HDFS using Sqoop.\u00a0\nDeveloped Spark code using Scala and Spark-SQL for faster testing and data processing. \nImported millions of structured data from relational databases using Sqoop import to process using Spark and stored the data into HDFS in CSV format. \nDeveloped Spark streaming application to pull data from cloud to Hive table.\nUsed Spark SQL to process the huge amount of structured data.\nAssigned name to each of the columns using case class option in Scala. \nImplemented Spark GraphX application to analyze guest behavior for data science segments.\nEnhancements to traditional data warehouse based on STAR schema, update data models, perform Data Analytics and Reporting using Tableau.\n\nEnvironment: Big Data, SparkSpark, YARN, HIVE, Pig, Scala, Python, Hadoop, AWS, Dynamo DB, Kibana, Cloudera, EMR, JDBC, Redshift, NOSQL, Sqoop, MYSQL.\n\nBNFS,Fort Worth, TX\t\t\t\t\t\t\t                                 Apr\u201913 \u2013 Jul\u201915\nSr. Java/ Hadoop Developer\n\nRoles & Responsibilities:\n\nInvolved in Big\u00a0Data\u00a0Project Implementation and Support.\u00a0\nInvolved in the coding and integration of several business critical modules of CARE application using spring, Hibernate and REST web services on Web Sphere application server.\nImplemented Installation and configuration of multi-node cluster on Cloud using AWS on EC2.\nDesigned and developed Enterprise Eligibility business objects and domain objects with Object Relational Mapping framework such as Hibernate.\nUsed\u00a0Hive\u00a0to analyze data ingested into\u00a0HBase\u00a0by using\u00a0Hive-HBase\u00a0integration and compute various metrics for reporting on the dashboard\nDeveloped the Web Based Rich Internet Application (RIA) using JAVA/J2EE (spring framework).\nUsed the light weight container of the Spring Frame work to provide architectural flexibility for inversion of controller (IOC).\nUtilized Oozie workflow to run Pig and Hive Jobs Extracted files from Mongo DB through Sqoop and placed in HDFS and processed.\nUsed Flume to collect, aggregate, and store the web log data from different sources like web servers, mobile and network devices and pushed to HDFS.\nInvolved in end to end implementation of\u00a0Big\u00a0data\u00a0design.\nDeveloped and Implemented new UI's using Angular JS and HTML.\nDeveloped Spring Configuration for dependency injection by using Spring IOC, Spring Controllers.\nAll the data was loaded from our relational DBs to HIVE using Sqoop. We were getting four flat files from different vendors. These were all in different formats e.g. text, EDI and XML formats\nObjective of this project is to build a data lake as a cloud based solution in AWS using Apache Spark and provide visualization of the ETL orchestration using CDAP tool.\nProof-of-concept to determine feasibility and product evaluation of Big Data products \nWriting Hive join query to fetch info from multiple tables, writing multiple Map Reduce jobs to collect output from Hive\nUsed Hive to analyze the partitioned and bucketed data and compute various metrics for reporting on the dashboard.\nAWS Cloud and On-Premise environments with Infrastructure Provisioning / Configuration.\nWorked on writing Perl scripts covering data feed handling, implementingmark logic, communicating with web-services through SOAP Lite module and WSDL.\u00a0\nInvolved in developing Map-reduce framework, writing queries scheduling map-reduce\nDeveloped the code for Importing and exporting data into HDFS and Hive using Sqoop\nInstalled and configured Hadoop and responsible for maintaining cluster and managing and reviewing Hadoop log files.\nDeveloped Shell, Perl and Python scripts to automate and provide Control flow to Pig scripts.\nDesign of Redshift Data model, Redshift Performance improvements/analysis\nContinuous monitoring and managing the Hadoop cluster through Cloudera Manager.\nWorked on configuring and managing disaster recovery and backup on Cassandra Data.\nPerformed File system management and monitoring on Hadoop log files.\nImplemented partitioning, dynamic partitions and buckets in HIVE.\nDeveloped customized classes for serialization and Deserialization in Hadoop\nAnalyzed large amounts of data sets to determine optimal way to aggregate and report on it.\nImplemented a proof of concept deploying this product in Amazon Web Services AWS.\u00a0\nInvolved in migration of data from existing RDBMS (oracle and SQL server) to Hadoop using Sqoop for processing data.\nEnvironment: Pig, Sqoop, Kafka, Apache Cassandra, Oozie, Impala, Cloudera, AWS, AWS EMR, Redshift, Flume, Apache Hadoop, HDFS, Hive, Map Reduce, Cassandra, Zookeeper, MySQL, Eclipse, Dynamo DB, PL/SQL and Python.\n\nWW Norton, NYC, NY                                                                                                                Nov\u201911 \u2013 Mar\u201913\nJAVA/J2EE DEVELOPER\n\nRoles & Responsibilities:\n\nUsed Web Logic to build and deploy the application.\u00a0\nCreated stubs to consume Web services.\nAutomated tests were coded in Java Script with Frog logic\u2019s Squish or Smart Bear\u2019s Test Complete for client applications and coded in Java with Selenium for web application testing.\nDeveloped automation testing process using Selenium\u00a0and QTP which involves study of client testing requirements, analyzing the feasible testing strategies and development of automated test scripts which also includes testing and finally deployment of the test scripts.\u00a0\nUsed spring framework to achieve loose coupling between the layers thus moving towards Service Oriented Architecture (SOA) exposed through\u00a0RESTful.\u00a0\u00a0\nInvolved in performing Unit and Integration testing (Junit)\u00a0\nInvolved in building EJB Session/Entity beans to maintain Transaction Management across the application.\nBuilt Web pages that is more user-interactive using Java script and Angular JS.\nGroovy allows using the primitive\u2019s types as a short form for the variable declaration and the compiler translates this into the object.\nExtensively used Spring\u00a0JDBC\u00a0in data access layer to access and update information in the database.\nDeveloped Web Services to create reports module and send it to different agencies and premium calculation for manual classes using\u00a0SOAP\u00a0and Restful web services and rich faces components.\nInvolved in writing Spring\u00a0MVC\u00a0controllers and writing custom validations.\u00a0\nWorking on Struts Framework for developing the front-end application and extensively. Spring as middle tier for entire application.\u00a0\nUsed JAX-WS (SOAP) and JAX-RS (REST) to produce web services and involved in writing programs to consume the web services.\nJava 8, J2EE, spring (MVC, Data-JPA, Security),Hibernate, Jenkins or Bamboo, HTML 5, JSP, JavaScript, JQuery, Ajax, Angular JS\nInvolved in working with Struts Tiles for the common look and feel for a web application.\u00a0\nWorking on Web Services using\u00a0Java\u00a0API for XML Services (JAX-WS) and supporting, building, deploying Web APIs Services.\u00a0\nWorking as a part of team from business transfer, development, testing, code review, build implementation and support.\u00a0\nWrote PL/SQL statements according to the need using Oracle 10g database.\u00a0\nWorking on an internal web-based client server application built with Struts 2 Framework using Oracle backend Database, working on establishing the relation for the different beans using the Hibernate 3.1.\u00a0\nInvolved in writing various components using Spring AOP and IOC framework.\nInvolved in writing JSP and JSF components. Used JSTL Tag library (Core, Logic, Nested, Beans and Html tag lib's) to create standard dynamic web pages.\nDeveloped connection to the backend using\u00a0JDBC\u00a0after building the Entity Beans as Bean Managed Persistence Entity Beans.\nDesigned and Developed the UI Framework using Spring\u00a0MVC\u00a0and AngularJS.\nCreation of REST Web Services for the management of data using Apache CXF and Docker.\nImplementation of EJB as entry point for web services. Effectively prepared for and organized technical inspections to review code and design models with peers and software architects.\u00a0\nIdentified the defects through Selenium and ensured that business processes deliver the expected results and remain reliable throughout the production release.\nSpring 3.x is used as framework to write the application code and\u00a0RESTful\u00a0web services for external clients. \nDesigned and developed backend application servers using\u00a0Python.\nManaged application deployment using Python.\nUpgraded Python 2.3 to Python 2.5, this required recompiling mode Python to use Python 2.5.\nEnhanced user experience by designing new web features using MVC Framework like Backbone.js, and node.js.\nUsed\u00a0JDBC\u00a0connectivity for connecting to the Oracle 8.0 database.\nDeveloped major websites and services by including\u00a0Mongo\u00a0DB\u00a0as backend software.\nGood experience in creating and consuming Restful and SOAP Web Services.\u00a0\u00a0\nDeveloping ability to move and consolidate critical information for the businesses and financial account data Using EJB 2.1 and Hibernate for performing the Database Transactions.\u00a0\nEnvironment:\u00a0Java\u00a0and\u00a0, Struts Framework ,J query, Oracle , HTML, Mark logic, micro services, Python, Groovy,  PL/SQL, JDBC, Mark logic, Talend, Hibernate, Ant, WSDL, EJB .\n\nInfinite Computer Solutions                                                                              Jun\u201908 \u2013 Oct\u201911\t                                                                                         \nJAVA DEVELOPER\n\nRoles & Responsibilities:\n\nInvolved in various phases of Software Development Life Cycle (SDLC) of the application like Requirement gathering, Design, Analysis and Code development.\nDeveloped\u00a0hibernate\u00a0mapping using db model.\u00a0\nInvolved in designing and developing Customized tags using JSP tag lib\u00a0\nImplemented Model View Control (MVC) architecture using Struts Framework and Spring framework\u00a0\nDeveloped browser-based\u00a0Java\u00a0Server Faces front-end to an AS/400 system\u00a0\nUsed Ajax to provide dynamic features where applicable.\nImplemented\u00a0RESTful\u00a0web services to communicate with components of other Sourcing systems within the firm and to provide data to the reporting team.\u00a0\nUsed MVC pattern for GUI development in JSF and worked closely with JSF lifecycle, Servlets and JSPs are used for real-time reporting which is too complex to be handled by the Business Objects\u00a0\nUsed Jira for bug tracking and project management.\nPrepared user documentation with screenshots for UAT (User Acceptance testing).\u00a0\nImplemented Struts Validation Framework for Server side validation.\u00a0\nDeveloped JSP's with Custom Tag Libraries for control of the business processes in the middle-tier and was involved in their integration.\u00a0\nDeveloped Web services (SOAP) through WSDL in Apache Axis to interact with other components.\u00a0\nImplemented EJBs Session beans for business logic.\u00a0\nUsed parsers like SAX and DOM for parsing xml documents and used XML transformations using XSLT.\u00a0\nWrote stored procedures, triggers, and cursors using Oracle PL/SQL.\u00a0\nUsed Rational Clear Case as Version control.\u00a0\nImplemented\u00a0Java/J2EE Design patterns like Business Delegate and Data Transfer Object (DTO), Data Access Object and Service Locator.\u00a0\nInteract with clients to understand their needs and propose design to the team to implement the requirement.\nBuilt an online system using XML, Java script, AJAX, Strut 2.0, JDBC\u00a0\nInvolved in technical Documentation for the module\u00a0\nDesigned and created SQL Server Database, Stored Procedures\u00a0\n\u00a0\nEnvironment: Java, JSP, JDBC, Cassandra , API , Python, J query, Angular JS along with Web service , REST , Spring Core, Struts, Hibernate, Design Patterns, XML, Oracle, Apache Axis, ANT, Junit, UML, Web services, SOAP, XSLT, Jira.", "years_experience": 9, "score": 0.27344314893644284, "path": "C:\\Users\\admin\\AI-Resume-Ranker\\data\\uploads\\Candidate199.docx"}, {"filename": "LeafGuard-Transforming_Big_Data_with_Data_Science_SRS.pdf", "text": " \n \n \n \n \n \n \nSoftware Requirements Specification \nVersion 1.0 \n \n \nLeafGuard  \n \n \n \nTheme: Green Environment \nProject Name: LeafGuard \nCategory: Transforming Big Data with Data Science \n \n \n \n \n\u00a9 Aptech Limited \n \n \n \nContents \n1.1 Background and Necessity for the Application ............................... 2 \n1.2 Proposed Solution ......................................................................... 3 \n1.3 Purpose of the Document ............................................................. 6 \n1.4 Scope of Project ............................................................................ 6 \n1.5 Constraints .................................................................................... 7 \n1.6 Functional Requirements ............................................................... 8 \n1.7 Non-Functional Requirements ..................................................... 10 \n1.8 Interface Requirements ............................................................... 11 \n1.8.1 Hardware .................................................................................... 11 \n1.8.2 Software ...................................................................................... 11 \n1.9 Project Deliverables .................................................................... 12 \n \n \n \n \n \n\u00a9 Aptech Limited \n1.1  Background and Necessity for the Application \n \nLeaf diseases are a significant concern for agricultural productivity and \nsustainability worldwide. They can cause substantial yield losses and affect the \nquality of produce, thereby impacting food security and economic stability for \nfarmers. Traditional methods of leaf disease detection involve manual inspection \nby experts, which is time-consuming, labor-intensive, and impractical for large-\nscale farming operations. With the advent of technology, there has been a \ngrowing interest in leveraging Data Science and Big Data processing to address \nthis challenge. By analyzing data from sources such as images, weather \npatterns, and soil conditions, more efficient methods for detecting leaf diseases \ncan be developed. This approach also enhances the accuracy of disease \ndetection. \n \nThe application of Big Data processing and Data Science in leaf disease detection \nis not just a technological advancement, but a necessity for modern agriculture. \nThe increasing global population demands higher agricultural productivity, while \nresources such as land and water remain limited. Early and accurate detection \nof leaf diseases can help in timely intervention, reducing the spread of diseases \nand minimizing crop losses.  \n \n \n \n \n \n \n\u00a9 Aptech Limited \n1.2  Proposed Solution \n \nThe \u2018LeafGuard\u2019 application has the potential to completely transform the way \nplant health is monitored and diseases are managed. From modest gardens to \nenormous industrial farms, it can be applied in a variety of environments. The \nadoption of this technology will help promote environmentally responsible \nfarming practices. To stop the spread of plant diseases and reduce their impacts, \nthere is a necessity for reliable methods to identify plant illnesses. The \nLeafGuard application aims to address this issue by using Big Data processing \nand Data Science algorithms to diagnose plant diseases. It offers farmers and \ngardeners a quick, dependable, and cost-effective way to monitor the health of \ntheir plants and crops. \n \n\u2022 Data Collection: Initially, the process begins with data collection and \ningestion where high-quality images of leaves, potentially affected by \nvarious diseases, are sourced from repositories such as Kaggle. These \nimages are then ingested into a distributed storage system such as HDFS \nor cloud-based storage which provides scalable and reliable data storage \ncapabilities. The dataset is provided to you. Alternatively, you can \ndownload the dataset from:  \n \nhttps://www.kaggle.com/datasets/vipoooool/new-plant-diseases-dataset. \n \nHint: Dataset of images for plant disease detection downloaded from Kaggle for \nimplementation purpose is as follows: \n \n \n \n \nThe sample architecture for LeafGuard application can be as follows: \n \n \n\u00a9 Aptech Limited \n \nSample Architecture of the Application \n \nLeafGuard can be effectively accomplished using Big Data processing and Data \nScience techniques. Here is how these techniques can be applied: \n \n\u2022 Data Pre-Processing and Feature Extraction: Following ingestion, the \ndata undergoes a preprocessing phase that includes cleaning and image \nenhancement to remove noise and standardize the dataset. Feature \nextraction is performed to isolate key characteristics such as color, \ntexture, and shape which are crucial for accurate disease identification. \nThis step is critical as it prepares the data for effective analysis by machine \nlearning models. \n \n\u2022 Data Management and Version Control: Data management and version \ncontrol are integrated into the application to track and manage the \ndatasets and preprocessing workflows, ensuring consistency and \nreproducibility of results. Techniques such as MapReduce are employed \nfor efficient processing and management of large datasets, allowing for the \nhandling of complex data transformations and analysis tasks. \n \n \n \n\u2022 Model Training: Model training involves using the preprocessed data to \ntrain machine learning models, employing techniques such as cross-\n \n \n\u00a9 Aptech Limited \nvalidation to fine-tune the model's performance, and ensure robustness. \nThe evaluation phase focuses on assessing the models' accuracy, \nprecision, and other metrics ensuring that they meet the desired \nperformance standards. \n \n\u2022 Model Evaluation: The final stage involves deploying these models to \npredict leaf diseases, with the results visualized on an interactive \ndashboard. This dashboard provides a user-friendly interface for users, \nenabling them to interpret the results and make informed decisions based \non the data insights. The entire methodology is designed to be scalable, \nsecure, and efficient capable of handling large volumes of data and \ndelivering precise disease diagnostics. \n \nIn conclusion, the suggested methodology entails gathering plant picture data, \npreprocessing the data to eliminate any inconsistencies, and choosing suitable \nmethods. It also involves training the model, assessing its performance, and \ndeploying the model for usage in a Web-based application. This methodology \naims to create a machine learning-based model for plant disease identification \nthat is both precise and effective. This model can lead to early diagnosis and \ntreatment, improving agriculture and reducing environmental degradability. \n \n \n \n \n \n \n \n \n \n\u00a9 Aptech Limited \n1.3  Purpose of the Document \n \nThe purpose of this document is to present a trained and tested interactive model \ntitled \u2018LeafGuard\u2019.  \nThis document explains the purpose and features of Big Data Processing Using \nData Science and the constraints under which it must operate. This document \nis intended for both stakeholders and developers of the system. \n1.4  Scope of Project \n \nThe scope of the project is to develop a comprehensive application that can \naccurately and efficiently identify various leaf diseases in crops. By leveraging \nlarge datasets comprising high-resolution images, weather data, and soil \nconditions the project will utilize Machine Learning algorithms to analyze and \nclassify different disease patterns. \n \nThe scope includes creating a scalable data pipeline for continuous data \ncollection and implementing robust preprocessing techniques. Additionally, it \ninvolves developing a user-friendly interface for farmers to easily upload images \nand receive diagnostic results. \n \n \n \n \n \n\u00a9 Aptech Limited \n1.5  Constraints \n \nImplementing an effective \u2018LeafGuard\u2019 application faces various non-technical \nconstraints that can impact its performance.  \n\u2022 Regulatory compliance and ethical considerations around data collection, \nparticularly concerning agricultural practices and environmental impact, \ncan influence the application\u2019s design and deployment.  \n\u2022 Operational constraints, such as limited access to consistent Internet \nconnectivity in remote agricultural areas, may hinder data transmission \nand application\u2019s updates.  \n\u2022 Furthermore, integrating the application with existing agricultural \nworkflows and practices poses compatibility challenges that require \ncareful planning and stakeholder engagement.  \n\u2022 Additionally, seasonal variations in leaf appearance and disease patterns \nintroduce complexity necessitating adaptive algorithms capable of \nhandling dynamic environmental conditions. \n \n \n \n \n \n \n \n \n\u00a9 Aptech Limited \n1.6  Functional Requirements \n \nFunctional requirements for a leaf disease detection system are crucial to ensure \nits effectiveness in agricultural applications. These requirements define the \nspecific capabilities and behaviors the system must exhibit to accurately identify \nand manage plant diseases. Key functional requirements of the application are \nas follows: \n \ni. \nData Collection and Ingestion: Data Collection and Ingestion involve \nimporting and storing leaf images from multiple sources. It also includes \nefficiently ingesting this data into a distributed storage system such as \nHDFS or the cloud for easy access and processing. \n \nii. \nData Storage: Data Storage involves using distributed storage systems to \nsecurely and reliably manage large volumes of data. This approach \nensures that data is accessible and protected across various locations and \nsystems. \n \niii. \nData Management and Version Control: Data Management and Version \nControl involve implementing systems to track and manage different data \nversions, \nensuring \nconsistency \nand \nreproducibility. \nAdditionally, \nMapReduce is used for processing large datasets and handling \ncomputational tasks efficiently. \n \niv. \nData pre-processing: The application shall preprocess the collected data \nby cleaning, normalizing, and augmenting the images to enhance their \nquality. The system shall handle missing data, filter out noise, and \nstandardize input formats to ensure consistency. \n \nv. \nFeature Extraction: The application shall extract relevant features from \nthe images using techniques such as edge detection, color analysis, and \ntexture analysis. The system shall also extract features from contextual \ndata (for example, weather and soil conditions) to aid in disease detection. \n \nvi. \nModel Training and Evaluation: The application should train machine \nlearning models using labeled datasets that include various leaf diseases. \nThe application should support different types of models, such as \nConvolutional Neural Networks (CNNs), for image analysis and decision \n \n \n \n \n \n\u00a9 Aptech Limited \ntrees for contextual data. After training the models, they are evaluated to \nensure accuracy and reliability in disease detection. \n \nvii. \nDisease Detection: The application should analyze new leaf images and \ncontextual data to detect and classify diseases accurately. The application \nshould provide probability scores or confidence levels for the detected \ndiseases. It should be able to predict disease labels based on model \noutputs. \n \nviii. \nUser Interface: The application should offer a user-friendly interface for \nfarmers to upload leaf images and input additional data. The application \nshould display the results of the disease detection including the type of \ndisease, confidence level, and recommended actions. \n \nix. \nReporting and Visualization: The application should generate reports \non disease occurrences, trends, and patterns over time. The application \nshould also provide visualizations such as heatmaps and charts to help \nusers understand the spread and impact of diseases. \n \n \n \n \n \n \n \n \n \n \n\u00a9 Aptech Limited \n1.7  Non-Functional Requirements \n \nThere are several non-functional requirements that should be fulfilled by the \napplication. The application should be: \n \n1. Usable: The application should be designed with a clear and intuitive \ninterface ensuring ease of use for all users. \n \n2. Scalable: The leaf disease detection application should be scalable, \ncapable of efficiently processing large volumes of image data, and \naccommodating a growing number of concurrent users without \ncompromising performance or accuracy. \n \n3. Accuracy: The application must achieve a high level of accuracy in \nidentifying and classifying diseases, based on benchmark datasets and \nstandard evaluation metrics. This requirement ensures reliable disease \ndiagnosis and effective management strategies for farmers, supporting \nsustainable agricultural practices and minimizing crop losses. \n \n4. Robustness: \nThe \napplication \nmust \ndemonstrate \nrobustness \nby \nmaintaining consistent performance across varying environmental \nconditions such as different lighting conditions, diverse plant species, and \nvarying leaf orientations. \n \n5. Reliable: The application should consistently provide accurate results and \nbe reliable in various situations and environments. \n \n \nThese are the bare minimum expectations from the project. \nIt is a must to implement the FUNCTIONAL and NON-\nFUNCTIONAL requirements given in this SRS. \n \nOnce they are complete, you can use your own creativity and \nimagination to add more features if required. \n \n \n \n \n \n  \n \n \n \n \n \n\u00a9 Aptech Limited \n1.8  Interface Requirements \n \n1.8.1 Hardware  \n \nIntel Core i5/i7 Processor or higher \n8 GB RAM or higher \nColor SVGA \n500 GB Hard Disk space  \nMouse \nKeyboard \n \n1.8.2 Software  \n \nTechnologies to be used: \n \n1. Data Store: HDFS, Apache HBase, MongoDB, or CSV \n2. Backend: Apache Spark or Apache Hive \n3. Programming/IDE: R programming/ Python 3.11.4 or higher , Jupyter \nNotebook, Anaconda 23.1.0 or higher, or Google Colab \n4. Libraries: OpenCV, TensorFlow, scikit-learn, Pandas, NumPy, PyTorch, \nMatplotlib, and Seaborn \n5. Visualization: Tableau Desktop \n \n \n \n \n \n\u00a9 Aptech Limited \n \n1.9  Project Deliverables \n \nYou will design and build the project and submit it along with a complete project \nreport that includes: \n \n\u25cf Problem Definition \n\u25cf Design Specifications \n\u25cf Diagrams such as User Flow Diagram/User Journey Map \n\u25cf Test Data Used in the Project \n\u25cf Project Installation Instructions  \n\u25cf Link of GitHub for accessing the uploaded project code (Link should have \npublic access) \n\u25cf Detailed Steps to Execute the Project \n\u25cf Link of Published Blog \n \nThe source code, including .ipynb files for Jupyter Notebook and Google Colab, \nshould be shared via GitHub. Appropriate access permissions should be granted \nto users to allow testing for Jupyter Notebook and Google Colab. The \nconsolidated project must be submitted on GitHub with a ReadMe.doc file listing \nassumptions (if any) made at your end.  \nProvide the GitHub URL where the project has been uploaded for sharing. The \nrepository on GitHub should have public access. Documentation is a very \nimportant part of the project; hence, all crucial aspects of the project must be \ndocumented \nproperly. \nEnsure \nthat \ndocumentation \nis \ncomplete \nand \ncomprehensive. \nYou should publish a blog of minimum 2000 words on any free blogging Website \nsuch as Blogger, Tumblr, Ghost or any other blogging Website. The link of the \npublished blog should be submitted along with the project documentation. \n \nSubmit a video (.mp4 file) demonstrating the working \nof the application, including all the functionalities of \nthe project. This is MANDATORY. \n \n~~~ End of Document ~~~ \n", "years_experience": 0, "score": 0.2666074634166087, "path": "C:\\Users\\admin\\AI-Resume-Ranker\\data\\uploads\\LeafGuard-Transforming_Big_Data_with_Data_Science_SRS.pdf"}, {"filename": "StockPulse-Data_Science_Dominion_SRS_Final.pdf", "text": " \n \n \n \n \n \n \nSoftware Requirements Specification \nVersion 1.0 \n \n \nStockPulse  \n \n \nTheme: Stock Price Anomaly Tracker \nProject Name: StockPulse \nCategory: Data Science Dominion \n \n \n \n \n \n\u00a9 Aptech Limited  \nTable of Contents \n1.1 Background and Necessity for the Application ............................... 3 \n1.2 Proposed Solution ......................................................................... 4 \n1.3 Purpose of the Document ............................................................. 7 \n1.4 Scope of Project ............................................................................ 7 \n1.5 Constraints .................................................................................... 8 \n1.6 Functional Requirements ............................................................... 9 \n1.7 Non-Functional Requirements ....................................................... 9 \n1.8 Interface Requirements ............................................................... 10 \n1.8.1 Hardware .................................................................................... 11 \n1.8.2 Software ...................................................................................... 11 \n1.9 Project Deliverables .................................................................... 12 \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n\u00a9 Aptech Limited  \n1.1  Background and Necessity for the Application \n \nFinancial time series data, such as stock prices and returns, are vital indicators \nof a company\u2019s performance and broader market dynamics. These datasets are \noften characterized by high volatility, noise, and non-linear trends making them \ncomplex to interpret using traditional methods. Amid these regular patterns lie \nanomalies such as sudden spikes, dips, or irregular fluctuations that deviate \nfrom expected market behavior. These anomalies could be early indicators of \ncritical events such as insider trading, economic crises, cyber-attacks, or system \nmalfunctions. For example, a sudden plunge in stock price not aligned with \ncompany news could suggest market manipulation or erroneous trading. \nDetecting such anomalies is crucial for financial analysts, regulatory bodies, and \ninstitutional investors to investigate root causes, mitigate risks, and maintain \nmarket integrity. \n \nAs financial markets become more digitized and data-intensive, the manual \ninspection of anomalies is neither practical nor scalable. Traditional statistical \ntechniques often assume stationarity and may fail to capture dynamic changes \nor hidden patterns within large datasets. This necessitates the adoption of \nadvanced anomaly detection and Data Science driven approaches that can learn \ncomplex temporal patterns and flag irregularities in near real-time. An effective \nanomaly detection system in stock price series can aid in early warning for \ninvestment risk, fraud detection, algorithmic trading adjustments, and \nregulatory compliance. Additionally, such systems can empower investors to \nmake smarter and data-driven decisions. In a world where milliseconds can \nresult in massive financial loss or gain, building an intelligent anomaly detection \nframework is not just beneficial, it is essential for modern financial \ninfrastructure. \n \n \n \n \n\u00a9 Aptech Limited  \n1.2  Proposed Solution \n \nThe significance of stock market anomalies resides in their ability to reveal \npotential opportunities or hazards. Positive news about the firm or its industry, \nfor instance, may cause a rapid jump in the price of stock, indicating a possible \ninvestment opportunity. On the other hand, an unanticipated decline in price \ncan indicate underlying problems or shifts in market sentiment, indicating a risk \nthat investors would have to control. In the proposed solution \u2018StockPulse\u2019, the \naim is to detect anomalies in financial stock market data by identifying unusual \npatterns, spikes, or dips in stock prices that deviate from expected behavior.  \n \n\u2022 Data Collection: The process begins with collecting real-time stock \nmarket data. Choose any active company such as Apple, Google, or others \nand fetch stock data using reliable sources such as Yahoo Finance APIs, \nAlpha Vantage, or other real-time financial data providers. The dataset \ntypically includes features such as Date, Open, High, Low, Close, Adjusted \nClose, and Volume which are essential for understanding market trends \nand detecting anomalies. Working with real-time data will provide more \npractical insights and make the analysis relevant to current market \nbehavior. \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n\u00a9 Aptech Limited  \nThe sample architecture for StockPulse application can be as follows: \n \n \nSample Architecture of the Application \n \n \n\u00a9 Aptech Limited  \n \nTo achieve robust anomaly detection, a data science-driven approach is followed, \nwith unsupervised anomaly detection algorithms for better interpretability and \ncomputational efficiency. Steps are outlined based on the proposed architecture \nas follows: \n \n\u2022 Exploratory Data Analysis (EDA) - Begin with initial data exploration to \nunderstand trends, seasonality, and volatility in stock prices over time. \nUtilize tools such as line plots, moving averages, and boxplots to visualize \nvariations and detect early signs of anomalies. \n \n\u2022 Data Pre-processing - The dataset should be cleaned by handling missing \nvalues, parsing date formats, sorting records chronologically, and \nremoving inconsistencies. Data should be normalized or standardized to \nensure uniform scaling, which is especially helpful for distance-based \nanomaly detection methods. \n \n\u2022 Feature Extraction - Derive new features such as returns, moving \naverages, and momentum indicators to enrich the dataset and provide \ndeeper insights useful for identifying anomalies. \n \n\u2022 Feature Subset Selection - Use statistical techniques such as variance \nthresholding, correlation analysis, or feature importance (for example, \nfrom tree-based models), select the most relevant and non-redundant \nfeatures to reduce dimensionality and noise. \n \n\u2022 Data Splitting - The processed dataset should be split into training and \ntesting sets. The training set can be used to learn normal behavior \npatterns, while the testing set may contain potential anomalies for model \nevaluation. \n \n\u2022 Anomaly Detection Algorithm - Anomaly detection techniques such as \nIsolation Forest, or Support Vector Machine (SVM) should be employed. \nThese algorithms are well-suited for identifying outliers in time series \nwithout requiring labeled anomalies. \n \n\u2022 Parameter Tuning - Parameters of the chosen should be fine-tuned using \ninternal cross-validation and evaluation on the validation set to optimize \nthe detection capability. \n \n\u2022 Model Evaluation - The final model should be tested on unseen data to \nevaluate its performance in correctly classifying normal and anomalous \npatterns. Evaluation metrics such as precision, recall, F1-score, and visual \ninspection of flagged anomalies should be used to assess accuracy. \n \n \n \n\u00a9 Aptech Limited  \nThis proposed solution offers a complete, scalable, and interpretable workflow \nfor financial anomaly detection, suitable for real-time fraud monitoring, risk \nassessment, or early warning systems in the stock market domain. \n \n \n \n \n1.3  Purpose of the Document \n \nThe purpose of this document is to present a trained and tested interactive model \ntitled \u2018StockPulse\u2019.  \nThis document explains the purpose and features of Data Science and the \nconstraints under which it must operate. This document is intended for both \nstakeholders and developers of the system. \n1.4  Scope of Project \n \nThe scope of this project is to develop a data science-driven solution for detecting \nanomalies in financial time series data by analyzing stock price movements over \ntime. It includes key stages such as data exploration, preprocessing, feature \nengineering, and the application of anomaly detection algorithms to identify \nunusual patterns, spikes, or dips that deviate from expected trends.  \n \nThe project also involves designing a simple user interface to display the results \nand visualizing the findings through interactive dashboards and charts, enabling \nusers to easily interpret detected anomalies.  \n \n \n \n\u00a9 Aptech Limited  \n \n \n1.5  Constraints \n \nAlgorithmic assumptions like data stability may not be held in financial time \nseries. The project also focuses on offline analysis, with a basic dashboard for \nvisualization, and does not include real-time detection or interactive trading \nfeatures. \n \n \n \n\u00a9 Aptech Limited  \n1.6  Functional Requirements \n \nFunctional requirements for a financial time series anomaly detection system are \nessential to ensure accurate identification of unusual stock price patterns and \ntrends. These requirements define the capabilities and behaviors the system \nmust exhibit to detect anomalies effectively and present findings in an intuitive, \nvisual manner.  \n \nKey functional requirements of the application are as follows: \ni. \nData Ingestion and Management: The application shall ingest historical \nfinancial stock market data, including price and volume information, and \norganize it chronologically. It should allow importing data from structured \nCSV or other standard formats into the system for further analysis. \n \nii. \nData Pre-processing: The application shall preprocess the data by \ncleaning missing values, parsing and formatting date fields, handling \ninconsistencies, and performing normalization or standardization to \nprepare the dataset for further analysis. \n \niii. \nFeature Engineering: The system shall compute derived metrics such as \nreturns, moving averages, and volatility indicators to enhance the dataset. \nThese features will provide rich input for anomaly detection algorithms. \n \niv. \nFeature Selection: The application shall include functionality to identify \nand retain relevant features using statistical techniques such as \ncorrelation analysis or variance thresholds to reduce noise and improve \nmodel efficiency. \n \nv. \nData Partitioning: The dataset shall be split into training and testing \nsets. The training set will represent typical behavior, while the testing set \nwill be used to validate anomaly detection results. \n \nvi. \nAnomaly Detection Module: The application shall implement data \nscience-driven anomaly detection algorithms such as Isolation Forest or \nSVM to identify unusual stock behavior. It should allow configuration of \nalgorithm parameters for optimal detection performance. \n \nvii. \nModel Evaluation: The system shall evaluate the detection results using \nmetrics such as precision, recall, and F1-score. It should also enable \nvisual confirmation of detected anomalies to validate accuracy. \n \n \n \n \n \n\u00a9 Aptech Limited  \n \nviii. \nUser Interface (UI) and Interaction: The application should provide a \nsimple UI to allow users to upload datasets, initiate analysis, and view \nresults. Users should be able to navigate through outputs and interact \nwith visual elements easily. \n \nix. \nDashboard Visualization: The system shall generate interactive \ndashboards using Tableau features or other visualization techniques. \nThese dashboards will display key trends, time series plots, and highlight \ndetected anomalies, providing users with clear, actionable insights. \n \nx. \nExport and Reporting: The application shall offer functionality to export \nanomaly reports and visual summaries for documentation or further \ndecision-making support. \n \n \n \n \n \n1.7  Non-Functional Requirements \n \n \n \n \n \n \n \n\u00a9 Aptech Limited  \nThere are several non-functional requirements that should be fulfilled by the \napplication. The application should be: \n \n1. Usable: The application should be designed with a clear and intuitive \ninterface ensuring ease of use for all users. \n \n2. Scalable: The system shall process large volumes of financial time series \ndata efficiently, ensuring timely detection of anomalies. It should be \nscalable to handle additional data without significant degradation in \nperformance. \n \n3. Accuracy: The application shall provide consistent results with a high \ndegree of accuracy in anomaly detection. It must minimize false positives \nand false negatives while maintaining dependable output across different \ndata segments. \n \n4. Compatible: The application should be compatible with common \noperating systems and support standard data formats. It should be \nintegrated smoothly with external visualization tools such as Tableau for \ndashboard rendering. \n \n \nThese are the bare minimum expectations from the project. \nIt is a must to implement the FUNCTIONAL and NON-\nFUNCTIONAL requirements given in this SRS. \n \nOnce they are complete, you can use your own creativity and \nimagination to add more features if required. \n \n \n \n \n \n \n \n  \n1.8  Interface Requirements \n \n \n \n \n \n \n\u00a9 Aptech Limited  \n1.8.1 Hardware  \n \nIntel Core i5/i7 Processor or higher \n8 GB RAM or higher \nColor SVGA monitor \n500 GB Hard Disk space  \nMouse and Keyboard \n \n1.8.2 Software  \n \nTechnologies to be used: \n \n1. Data Store: CSV \n2. Backend: Apache Spark or Apache Hive, Flask/Django \n3. Database: MongoDB/MySQL \n4. Programming/IDE: R programming/Python 3.11.4 or higher, Jupyter \nNotebook, Anaconda 23.1.0 or higher, or Google Colab \n5. Libraries: OpenCV, TensorFlow, scikit-learn, Pandas, NumPy, PyTorch, \nMatplotlib, and Seaborn \n6. Visualization: Tableau Desktop features/other visualization techniques \n \n \n \n \n \n \n \n\u00a9 Aptech Limited  \n1.9  Project Deliverables \n \nYou will design and build the project and submit it along with a complete project \nreport that includes: \n \n\u25cf Problem Definition \n\u25cf Design Specifications \n\u25cf Diagrams such as User Flow Diagram/User Journey Map \n\u25cf Test Data Used in the Project \n\u25cf Project Installation Instructions  \n\u25cf Link of GitHub for accessing the uploaded project code (Link should have \npublic access) \n\u25cf Detailed Steps to Execute the Project \n\u25cf Link of Published Blog \n \nThe source code, including .ipynb files for Jupyter Notebook and Google Colab, \nshould be shared via GitHub. Appropriate access permissions should be granted \nto users to allow testing for Jupyter Notebook and Google Colab. The \nconsolidated project must be submitted on GitHub with a ReadMe.doc file listing \nassumptions (if any) made at your end.  \nProvide the GitHub URL where the project has been uploaded for sharing. The \nrepository on GitHub should have public access. Documentation is a very \nimportant part of the project; hence, all crucial aspects of the project must be \ndocumented \nproperly. \nEnsure \nthat \ndocumentation \nis \ncomplete \nand \ncomprehensive. \nYou should publish a blog of minimum 2000 words on any free blogging Website \nsuch as Blogger, Tumblr, Ghost, or any other blogging Website. The link of the \npublished blog should be submitted along with the project documentation. \nDo NOT copy content or code from GPTs or other AI tools, although you are \npermitted to use images generated by AI tools for any visual representation \npurposes. It is mandatory to mention such tools used in case you add any AI \ngenerated images. \n \nSubmit a video (.mp4 file) demonstrating the working of the \napplication, including all the functionalities of the project. This is \nMANDATORY. \n~~~ End of Document ~~~ \n", "years_experience": 0, "score": 0.22975607515463853, "path": "C:\\Users\\admin\\AI-Resume-Ranker\\data\\uploads\\StockPulse-Data_Science_Dominion_SRS_Final.pdf"}, {"filename": "InsightBot-Data_Science_Dominion_SRS_Final.pdf", "text": " \n \n \n \n \n \nSoftware Requirements Specification \nVersion 1.0 \n \n \nInsightBot \n \n \nTheme: Daily News Simplified \nProject Name: InsightBot \nCategory: Data Science Dominion \n \n \n \n \n\u00a9 Aptech Limited  \nTable of Contents \n1.1 \nBackground and Necessity for the Application ............................................ 2 \n1.2 \nProposed Solution ...................................................................................... 3 \n1.3 \nPurpose of the Document ........................................................................... 6 \n1.4 \nScope of Project ......................................................................................... 6 \n1.5 \nConstraints ................................................................................................. 7 \n1.6 \nFunctional Requirements ............................................................................ 7 \n1.7 \nNon-Functional Requirements .................................................................. 10 \n1.8 \nInterface Requirements ............................................................................. 11 \n1.8.1 Hardware ................................................................................................. 11 \n1.8.2 Software ................................................................................................... 11 \n1.9 \nProject Deliverables .................................................................................. 12 \n \n \n \n \n \n\u00a9 Aptech Limited  \n1.1  Background and Necessity for the Application \n \nIn an age of information overload, people are bombarded with hundreds of news \narticles, blog posts, and updates daily across multiple platforms and in various \nformats. Navigating this sea of content to find clear, trustworthy, and relevant \ninformation can be time-consuming and overwhelming. To address this, \nInsightBot emerges as a smart, Data Science-driven solution that automatically \nextracts, filters, and simplifies news content into digestible summaries. \n \nThe necessity for such an application lies in the growing demand for \npersonalized, real-time information delivery. By leveraging pattern mining \ntechniques in Web scraping. InsightBot identifies structured elements such as \ntitles and article bodies from diverse news Websites across multiple languages. \nThis not only enables multilingual content processing, but also supports cross-\nregional awareness and media literacy. The integration of intelligent \nsummarization and classification transforms raw Web data into user-friendly \ninsights empowering readers, researchers, and professionals to stay informed \nwith minimal effort. \n \n \n \n \n \n \n \n \n \n\u00a9 Aptech Limited  \n1.2  Proposed Solution \n \nTo build a robust multilingual news simplification system, InsightBot applies a \nData Science-driven approach focused on automated content extraction and \npattern-based generalization. The methodology is designed to simplify daily news \nfrom multiple sources using lightweight and scalable techniques. The approach \navoids complex browser automation and instead relies on pattern mining and \nstructured HTML parsing to retrieve relevant information such as headlines and \narticle content across various news Websites. \n \nThe sample architecture of the application can be as follows: \n \n \n \n \nSample Architecture of the Application \n \n \n\u00a9 Aptech Limited  \nDataset and Features \nThe dataset for this project is provided to you. The dataset consists of HTML page \nsources extracted from 40 multilingual news and blog Websites, covering \nEnglish, Arabic, and Russian languages. These Websites were selected to \nrepresent diverse layouts and content styles commonly found in real-world news \nplatforms. The dataset includes a mix of global and regional news outlets and \nblogs, each providing articles with varying structure, tone, and formatting. \n \nEach record in the dataset contains unstructured HTML content parsed from \nWeb pages, including key components such as article title, main body, \npublication date, language, and source Website URL.  \n \nAdditionally, a separate set of 10 unseen Websites is used exclusively for testing \npurposes to evaluate the generalization ability of the rule-based scraping \napproach. These test sites help validate whether the system can effectively \nextract relevant content from websites it has not been trained on. \n \nThis dataset forms the basis for training a rule-driven content extraction system \nand visualizing key news insights through dashboards using Data Science \ntechniques and Tableau. \n \nThe project begins with data ingestion where a Web browser connects to news \nWebsites and blog sources to collect HTML content. This raw data is stored in \nstructured formats such as JSON or CSV, which serve as the storage foundation \nfor the next stages. \n \nThe ingested content undergoes initial pre-processing, which includes cleaning, \nformatting, and isolating useful HTML tags. This step helps remove noise and \nprepare the data for further processing. Following this, a pattern matching \nmodule is employed to identify key visual and structural indicators on a Web \npage. These include the largest text blocks for headlines or paragraph blocks for \narticle bodies, based on observations from 40 Websites across three languages \n(English, Arabic, and Russian). \n \nAfter pattern matching, the data goes through another round of pre-processing \nto refine and normalize the extracted features. This ensures consistency before \nthe Web scraper application runs. The scraper, guided by the earlier pattern \nrules, extracts article titles, publication dates, article bodies, and formats them \ninto clean outputs. \n \nThe next phase is model training and testing, where the system uses these \nextracted samples from 40 Websites for training and evaluates generalization \nusing data from 10 additional Websites.  \n \nOnce the model is trained, it integrates with the User Interface (UI). Users can \neither log in (if already registered) or register via admin approval. The extractor\u2019s \n \n \n\u00a9 Aptech Limited  \noutput is displayed on the UI in real-time, showing cleaned news content in a \nuser-friendly layout. \n \nFinally, dashboard visualization on the UI using tools such as Tableau allows \nusers to view aggregated news trends, topic frequency, or language distribution. \nThis completes the pipeline from data collection to meaningful insight \npresentation in an interactive graphical form. \n \nSteps to build the model for InsightBot are as follows: \n1. Dataset Collection and Upload: Backend ingestion or manual URL input. \n2. Preprocessing + Pattern Mining: Identify large div text blocks and largest \n<h1>/<h2> titles. \n3. Training Model: Use 40 Websites to train Document Object Model (DOM) \npattern matching or rule-based logic. \n4. Testing Model: Apply model to 10 Websites to verify extraction accuracy. \n5. Display on UI: Extracted articles shown in clean template. \n6. Dashboard Visualization: Use Tableau or embed charts (article count by \ndomain, language, and date) via APIs. \n \n \n \n \n \n \n \n \n\u00a9 Aptech Limited  \n1.3  Purpose of the Document \n \nThe purpose of this document is to present a trained and tested interactive model \ntitled InsightBot.  \nThis document explains the purpose and features of Data Science and the \nconstraints under which it must operate. This document is intended for both \nstakeholders and developers of the application. \n \n1.4  Scope of Project \n \nThe scope of this project is to implement a simplified news extraction system, \nInsightBot, that scrapes content from multiple news and blog Websites across \ndifferent languages. The project includes designing and implementing data \ningestion, preprocessing, and pattern-based content extraction. It also involves \ntraining and testing the extraction logic using multilingual Websites and \ndisplaying the extracted news titles and bodies on a UI. \n \nHowever, advanced features such as sentiment analysis, fake news detection, \nmultilingual translation, or deploying the solution as a live Web service fall \noutside the scope of this implementation. \n \n \n \n \n \n\u00a9 Aptech Limited  \n1.5  Constraints \nThe constraints of this project include handling the structural variability of \ndifferent news and blog Websites across multiple languages, that may affect the \naccuracy of pattern-based extraction.  \n \nAdditionally, the project relies on publicly accessible Websites, meaning changes \nin HTML structure or access restrictions (such as CAPTCHA or anti-scraping \nmechanisms) can disrupt data collection. Limited language-specific NLP \ncapabilities and the absence of labeled training data also restrict the precision \nof automated content extraction. \n \n \n \n \n1.6  Functional Requirements \n \nThe InsightBot project focuses on building an intelligent content \nextraction and visualization system that scrapes news and blog data from \nmultilingual Websites (English, Arabic, and Russian). It identifies and \nextracts key textual content such as headlines and article bodies and \npresents this information through a user-friendly interface with \ninteractive dashboards. The solution emphasizes automated data \n \n \n\u00a9 Aptech Limited  \nhandling and pattern-based content extraction, allowing users to access \nstructured and simplified news content. \n \nKey functional requirements of this system are as follows: \n \ni. \nData Ingestion from Websites - The system shall collect Web pages from \n40 training Websites and 10 testing Websites in English, Arabic, and \nRussian using Python-based Web scraping tools/libraries such as \nBeautifulSoup or Scrapy. It must handle multilingual encoding and store \nraw HTML responses. \n \nii. \nContent Parsing and Preprocessing \u2013 The application shall parse raw \nHTML to extract clean and usable text. This involves removing \nunnecessary HTML tags, scripts, ads, and styles, and normalizing the text \nstructure for pattern analysis. \n \niii. \nPattern-Based Content Extraction - The system shall identify article \ntitles and bodies using predefined structural patterns. These patterns \ninclude detecting the largest text blocks for headlines and the longest \nparagraph elements for article bodies, based on analysis from training \nWebsites. \n \niv. \nContent Storage \u2013 The extracted and structured data (titles and article \nbodies) shall be stored in JSON and CSV formats for further use in \nvisualization and analysis. \n \nv. \nUI Development \u2013 The system shall provide a simple user interface to \nallow users to browse the extracted news content in a clean and \ncategorized format. The UI must support language filters and article \nbrowsing. \n \nvi. \nDashboard Visualization: The system shall integrate with Tableau \nDesktop to create dashboards displaying aggregated insights such as \ntrending topics, frequency of keywords, language distribution, and article \nvolume over time. \n \n \n \n \n \n \n \n \n \n\u00a9 Aptech Limited  \nvii. \nAutomatic Scheduling- The project shall include a mechanism to \nautomate scraping at regular intervals (example, daily), ensuring content \nfreshness and minimal manual intervention. \n \nviii. \nExtraction logic Evaluation- The application shall validate the pattern-\nmatching logic using a test set of 10 Websites and report performance \nbased on accuracy in identifying correct titles and bodies. \n \nThe InsightBot application should also be able to implement following \nfunctionalities: \n\u2022  Multilingual Content Support - The application shall support extraction \nfrom Websites in three languages. It must handle Unicode characters and \napply language-agnostic extraction rules. \n\u2022 Language Toggle and Search Functionality- If implemented, the UI \nshould support switching between supported languages and allow \nkeyword-based search within the extracted content. \n \n \n \n \n \n \n \n \n \n\u00a9 Aptech Limited  \n1.7  Non-Functional Requirements \n \nThere are several non-functional requirements that should be fulfilled by the application \nwhich are as follows.  \n1. Performance: The system should process and extract data from a Website \nwithin a reasonable time (example, under five seconds per page), ensuring \na smooth and responsive experience for the user. \n \n2. Scalable: The application should be scalable to handle the addition of \nmore Websites and multiple languages in the future without significant \nchanges to the architecture. \n \n3. Usable: The user interface should be simple, intuitive, and accessible to \nusers with minimal technical knowledge, allowing them to view extracted \nnews articles and dashboard insights effortlessly. \n \n4. Reliable: The system should maintain consistent performance over time, \nsuccessfully scraping and displaying data even if minor structural changes \noccur on the source Websites. \n \n5. Maintainable: The codebase should be modular and well-documented so \nthat future updates to scraping logic, UI components, or language support \ncan be implemented with ease. \n \n  \n \n \n \n \n\u00a9 Aptech Limited  \n \n1.8  Interface Requirements \n \n1.8.1 Hardware  \n \nIntel Core i5/i7 Processor or higher \n8 GB RAM or higher \nColor SVGA monitor \n500 GB Hard Disk space  \nMouse and Keyboard \n \n1.8.2 Software  \n \nTechnologies to be used: \n \n1. Data Store: JSON/TXT/CSV \n2. Backend: Apache Spark or Apache Hive, Flask, Django \n3. Database: MongoDB, MySQL \n4. Programming/IDE: Python 3.11.4 or higher, Jupyter Notebook, Anaconda \n23.1.0 or higher, Google Collab \n5. Libraries: BeautifulSoup, Scrapy, requests, regex, NLTK/BERT, Pandas, \nMatplotLib, seaborn, NumPy \n6. Visualization: Tableau Desktop \n \n \n \nOnce they are complete, you can use your own creativity and\nimagination to add more features if required. \nThese are the bare minimum expectations from the project. It is\na must to implement the FUNCTIONAL and NON-FUNCTIONAL\nrequirements given in this SRS. \n \n \n\u00a9 Aptech Limited  \n \n \n1.9  Project Deliverables \n \nYou will design and build the project and submit it along with a complete project \nreport that includes: \n \n\u25cf Problem Definition \n\u25cf Design Specifications \n\u25cf Diagrams such as User Flow Diagram/User Journey Map \n\u25cf Test Data Used in the Project \n\u25cf Project Installation Instructions  \n\u25cf Link of GitHub for accessing the uploaded project code (Link should have \npublic access) \n\u25cf Detailed Steps to Execute the Project \n\u25cf Link of Published Blog \n \nThe source code, including .ipynb files for Jupyter Notebook and Google Colab, \nshould be shared via GitHub. Appropriate access permissions should be granted \nto users to allow testing for Jupyter Notebook and Google Colab. The \nconsolidated project must be submitted on GitHub with a ReadMe.doc file listing \nassumptions (if any) made at your end.  \n \n \n\u00a9 Aptech Limited  \nProvide the GitHub URL where the project has been uploaded for sharing. The \nrepository on GitHub should have public access. Documentation is a very \nimportant part of the project; hence, all crucial aspects of the project must be \ndocumented \nproperly. \nEnsure \nthat \ndocumentation \nis \ncomplete \nand \ncomprehensive. \nYou should publish a blog of minimum 2000 words on any free blogging Website \nsuch as Blogger, Tumblr, Ghost, or any other blogging Website. The link of the \npublished blog should be submitted along with the project documentation. \n \nDo NOT copy content or code from GPTs or other AI tools, \nalthough you are permitted to use images generated by AI \ntools for any visual representation purposes. It is mandatory \nto mention such tools used in case you add any AI generated \nimages. \n \nSubmit a video (.mp4 file) demonstrating the working \nof the application, including all the functionalities of \nthe project. This is MANDATORY. \n~~~ End of Document ~~~ \n", "years_experience": 0, "score": 0.15510041781722056, "path": "C:\\Users\\admin\\AI-Resume-Ranker\\data\\uploads\\InsightBot-Data_Science_Dominion_SRS_Final.pdf"}, {"filename": "AI_Resume_Ranker-AI_and_Machine_Learning_Mania-SRS_final.pdf", "text": " \n \n \n \n \n \n \nSoftware Requirements Specification \nVersion 1.0 \n \nAI Resume \nRanker \n \nTheme: Smart Resume Ranker for Recruiters \nProject Name: AI Resume Ranker \nCategory: AI and Machine Learning Mania  \n \n \n\u00a9 Aptech Limited \n \nTable of Contents \n1.1 \n2 \n1.2 \n3 \n1.3 \n6 \n1.4 \n6 \n1.5 \n7 \n1.6 \n7 \n1.7 \n8 \n1.8 \nError! Bookmark not defined. \n1.8.1 10 \n1.8.2 10 \n1.9 \n11 \n \n \n \n \n \n\u00a9 Aptech Limited \n1.1  Background and Necessity for the Application \n \nIn the current highly competitive employment landscape, hiring managers are \noften inundated with numerous resumes for a single position, each featuring \ndistinct formatting and content. Traditional manual resume screening \napproaches are time-consuming, labor-intensive, and susceptible to human \nerror, potentially impeding the efficient selection of suitable candidates. \nHowever, with progress in Natural Language Processing (NLP) and Machine \nLearning technologies, automated systems have emerged as practical solutions \nfor resume parsing, information extraction, and matching applicants to job \nrequirements. \n \nAI Resume Ranker employs a blend of NLP techniques and machine learning \nalgorithms to recognize crucial sections within resumes, including applicant's \nname, contact details, skills, educational history, and professional experience. \nBy combining rule-based methods with machine learning, the tool ensures high \nprecision in information extraction, even when dealing with diverse resume \nlayouts. Furthermore, it filters resumes according to specific recruiter criteria, \nfacilitating more efficient candidate selection. \n \n \n \n \n \n \n \n \n \n \n\u00a9 Aptech Limited \n1.2  Proposed Solution  \n \nThe architecture of the proposed AI-powered resume ranking system is designed \nto automate the candidate shortlisting process by leveraging NLP and intelligent \nranking algorithms. The goal of the system is to accurately match resumes with \njob descriptions provided by recruiters, thereby reducing manual effort and \nensuring efficient candidate selection. \n \nThe process begins when an applicant uploads a resume, which is initially \ntreated as unstructured data. This unstructured resume is passed into the NLP \npipeline where several preprocessing techniques are applied, including \ntokenization, stopword removal, stemming, and lemmatization. These steps help \nto normalize and prepare the resume text for further analysis. \nFollowing pre-processing, the system performs skill set extraction to identify and \nextract technical and domain-specific skills from the resume content. These \nextracted skills are then, organized into a set of selected skills. At the same time, \nrecruiter requirements or job descriptions are processed using Named Entity \nRecognition (NER) to identify key expectations such as required skills, experience \nlevels, and job roles. \nThe selected skill sets from the resumes and the extracted information from job \ndescriptions are then, passed to a classification module. This component \nclassifies resumes based on their relevance and alignment with job posts. In \nparallel, a category-based matching technique is used to compare resumes and \njob categories to ensure only the most relevant resumes are ranked higher. \nFinally, the outputs of the classification and category-matching modules are \nintegrated into a ranking system, which generates a list of the most suitable \ncandidates. This ranked list assists recruiters in making informed decisions \nefficiently by presenting the best-fit applicants based on both skill relevance and \ncategory alignment. \n \n \n \n \n \n \n \n \n \n \n \n \n\u00a9 Aptech Limited \nThe sample architecture for AI Resume Ranker application is as follows: \n \n \n \nSample Architecture of the Application \n \nDataset and Features \nThe dataset for this project is provided to you. The dataset used in this project \ncomprises 228 Word documents (.docx), each representing an individual resume. \nThese resumes vary in size and content, ranging from 24 KB to 90 KB. It includes \ncandidates\u2019 details with diverse professional backgrounds such as Full Stack \nDevelopers, Business Analysts, Project Managers, and Software Engineers. \n \nEach document contains unstructured textual data, including personal details, \ntechnical and soft skills, educational background, certifications, and \nprofessional experience. This dataset serves as the foundation for building an \nAI-powered system that analyzes, ranks, and matches resumes against specific \njob roles or criteria using NLP and Machine Learning techniques. \n \nSteps to build the model for AI Resume Ranker are as follows: \n1. Resume Dataset Ingestion: The system begins with the ingestion of \nresumes in JSON format. These structured documents contain key \napplicant information, including education, work experience, skills, and \ncertifications. The JSON format facilitates easy parsing and information \nextraction during subsequent stages. \n \n \n\u00a9 Aptech Limited \n \n2. Text Pre-processing: The pre-processing stage is critical for cleaning and \nnormalizing the input text data. It involves several sub-steps: \n\u2022 Tokenization: Each resume is broken down into tokens, which are \nthe smallest units of text (example, words or symbols). \n\u2022 Stopwords Removal: Common words that do not contribute to the \nsemantic meaning (such as \u2018is\u2019, \u2018the\u2019, or \u2018at\u2019) are removed to reduce \nnoise and dimensionality. \n\u2022 Stemming and Lemmatization: Words are reduced to their base or \nroot form. Stemming applies rule-based reduction (example, \n\u2018running\u2019 to \u2018run\u2019), while lemmatization uses vocabulary and \ngrammar (example, \u2018better\u2019 to \u2018good\u2019).  \nThis step enhances the accuracy of matching and recognition tasks. \n3. Named Entity Recognition (NER): The cleansed and normalized tokens \nare passed through a NER module. NER is an NLP technique that identifies \nkey entities in text, such as skills (example, Python), roles (example, Data \nAnalyst), education, and company names. \n \n4. Skillset-Based Resume Extraction: After entities have been recognized, \nthe system focuses on skillset extraction. This phase filters resumes by \nidentifying and isolating relevant skills and technical competencies \nrequired for the job. This significantly narrows down the candidate pool to \nonly those who meet the core technical criteria specified by the employer. \n \n5. Matching Engine: Once both resumes and job descriptions have been \ntransformed into structured representations (based on extracted skills and \nentities), a matching algorithm compares two. The engine calculates a \nmatching score based on keyword overlap, semantic similarity, and \ncontextual alignment between job responsibilities and experience. This \nscore indicates the level of relevance of each candidate to the job profile. \n \n6. Scoring, Ranking, and Shortlisting: The scores generated by the \nmatching engine are used to score and rank. Select the top candidates who \nmeet or exceed a predefined threshold or simply pick the highest-ranked \napplicants for final review by recruiters. \n \n7. Output to Recruiters The system outputs a ranked and filtered list of \ncandidates who most closely match the job requirements. This allows \n \n \n\u00a9 Aptech Limited \nrecruiters to focus only on high-potential candidates, drastically reducing \ntime-to-hire and ensuring merit-based selection. \n \n \n1.3  Purpose of the Document \n \nThis document outlines the development plan for the AI Resume Ranker using \nAI and ML techniques. It ensures stakeholders have a common understanding, \npromotes effective communication, and equips the team to deliver a high-quality \napplication.  \nKey areas include recruiters focusing only on high-potential candidates, \ndrastically reducing time-to-hire and ensuring merit-based selection. This \ndocument is intended for both stakeholders and developers. \n1.4  Scope of Project \n \nThe scope of the project includes developing a complete Machine Learning \npipeline for AI Resume Ranker. The project aims to automate and streamline the \ncandidate shortlisting process by analyzing resumes and matching them to job \ndescriptions using NLP techniques. It extracts key information such as skills, \nroles, and qualifications, calculates a relevance score, and presents a ranked list \n \n \n\u00a9 Aptech Limited \nof top candidates through a user-friendly interface. The system also supports \nrecruiter feedback to improve accuracy over time. \n \n \n \n1.5  Constraints \n \nThe project may face several constraints, such as limited support for non-English \nresumes and strict reliance on well-structured PDF or DOCX formats. \nAdditionally, system accuracy depends heavily on the quality of job descriptions \nand available computational resources. Ensuring data privacy and avoiding bias \nare also critical limitations to consider. \n \n1.6  Functional Requirements \n \nThis comprehensive project aims to provide an effective tool for predicting earth \ntemperature, leveraging advanced ML techniques to ensure accessibility and \nscalability.  \n \n \nFunctional requirements are explained as follows: \n \ni. \nResume Upload - Users (recruiters or admins) should be able to upload \nsingle or multiple .docx resume files through the web interface or \napplication. \n \nii. \nJob Description Input \u2013 Users should be able to input or upload a Job \nDescription (JD) against which the resumes will be evaluated. \n \n \n \n \n\u00a9 Aptech Limited \niii. \nCandidate Scoring, Ranking, and Shortlisting \u2013 The system should use \nthe matching engine scores to rank candidates and automatically \nshortlist those who meet or exceed a predefined threshold, enabling \nrecruiters to review only the highest-ranked applicants. \n \niv. \nSearch and Filter \u2013 Users should be able to search candidates by \nkeywords, filter by ranking score, years of experience, specific skills, or \nqualifications. \n \nv. \nResume Parsing \u2013 The system should isolate resumes that contain job-\nrelevant skills and competencies. It must extract key technical skillsets \nfrom resumes to align with the employer\u2019s requirements. Only candidates \nmeeting core technical criteria should be retained for further analysis. \n \nvi. \nRanked Resume Display \u2013 The system should output a ranked and \nfiltered list of top-matching candidates, helping recruiters focus on high-\npotential profiles, reduce time-to-hire, and support merit-based selection. \n \nvii. \nResume Download or View \u2013 Provide options to view or download the \noriginal resume documents of shortlisted candidates. \n \nviii. \nExport Results \u2013 Enable export of ranked candidate lists into Excel or \nPDF format for offline review or reporting. \n \nix. \nUser Interface \u2013 The system should offer an intuitive interface for \nuploading multiple resumes and a job description, then display a ranked \nlist of top-matching candidates with highlighted skills, experience, and \nrelevance scores. \n \nIn addition, the system should implement a feedback loop that captures \nrecruiter\u2019s decisions (example, hired or rejected) to continuously retrain and \nenhance the model\u2019s accuracy over time. \n \n \n \n \n \n\u00a9 Aptech Limited \n \n \n1.7  Non-Functional Requirements \n \nThere are several non-functional requirements that should be fulfilled by the \napplication. \n \n1. Performance: The system should process and rank resumes in under five \nseconds per job description to ensure smooth recruiter workflow. \n2. Scalable: The system should be capable of handling bulk uploads (example, \n500+ resumes) without significant performance degradation. \n3. Usable: The interface should be intuitive and user-friendly for non-technical HR \nusers, requiring minimal training. \n4. Secure: All uploaded resumes and job data should be securely stored and \nprocessed with access control and encryption to protect sensitive personal \ninformation. \n5. Compatible: The tool should support commonly used file formats such as PDF \nand DOCX and be accessible across major browsers and devices. \n \nThese are the bare minimum expectations from the project. It is a \nmust to implement the FUNCTIONAL and NON-FUNCTIONAL \nrequirements given in this SRS. \n \nOnce they are complete, you can use your own creativity and \nimagination to add more features if required. \n \n \n\u00a9 Aptech Limited \n \n1.8  Interface Requirements \n \n1.8.1 \nHardware  \n \nIntel Core i5/i7 Processor or higher \n8 GB RAM or higher \nColor SVGA monitor \n500 GB Hard Disk space  \nMouse and Keyboard \n \n1.8.2 \nSoftware  \n \nTechnologies to be used: \n1. Frontend: HTML5/Streamlit or any other frontend programming languages \n2. Backend: Flask/Django \n3. Data Store: JSON/TXT/CSV/Word/PDF \n4. Programming/IDE: R/Python, Jupyter Notebook, Anaconda, Google \nColab \n5. Libraries: python-docx, PyPDF2, NER, NLTK, regex, scikitlearn, NumPy, \nPandas, Matplotlib \n \n \n \n \n\u00a9 Aptech Limited \n1.9  Project Deliverables \n \nYou will design and build the project and submit it along with a complete project \nreport that includes: \n \n\u25cf Problem Definition \n\u25cf Design specifications \n\u25cf Diagrams such as Dialog Flow \n\u25cf Test Data Used in the Project \n\u25cf Project Installation Instructions  \n\u25cf Proper Steps to execute the project \n\u25cf Link of GitHub for accessing the uploaded project code (Link should have \npublic access) \n\u25cf Link of published blog \n \nThe source code, including .ipynb files for Jupyter Notebook and Google Colab, \nshould be shared via GitHub. Appropriate access permissions should be granted \nto users to allow testing for Jupyter Notebook and Google Colab. The \nconsolidated project must be submitted on GitHub with a ReadMe.doc file listing \nassumptions (if any) made at your end.  \nPlease provide the GitHub URL where the project has been uploaded for sharing. \nThe repository on GitHub should have public access. Documentation is a very \nimportant part of the project; hence, all crucial aspects of the project must be \ndocumented \nproperly. \nEnsure \nthat \ndocumentation \nis \ncomplete \nand \ncomprehensive. \nYou should publish a blog of minimum 2000 words on any free blogging Website \nsuch as Blogger, Tumblr, Ghost, or any other blogging Website. The link of the \npublished blog should be submitted along with the project documentation. \n \nDo NOT copy content or code from GPTs or other AI tools, although you are \npermitted to use images generated by AI tools for any visual representation \npurposes. It is mandatory to mention such tools used in case you add any AI \ngenerated images. \n \nSubmit a video (.mp4 file) demonstrating the working of the application, \nincluding all the functionalities of the project. This is MANDATORY. \n~~~ End of Document ~~~ \n", "years_experience": 0, "score": 0.1428363568133533, "path": "C:\\Users\\admin\\AI-Resume-Ranker\\data\\uploads\\AI_Resume_Ranker-AI_and_Machine_Learning_Mania-SRS_final.pdf"}, {"filename": "TalkToText_Pro-Generative_AI_Odyssey-SRS_final.pdf", "text": " \n \n \n \n \n \nSoftware Requirements Specification \nVersion 1.0 \n \n \nTalkToText Pro \n \nTheme: AI-Powered Meeting Notes Rewriter \nProject Name: TalkToText Pro \nCategory: Generative AI Odyssey \n \n \n \n\u00a9 Aptech Limited \nContents \n1.1 \nBackground and Necessity for the Application ............................................ 2 \n1.2 \nProposed Solution ...................................................................................... 3 \n1.3 \nPurpose of the Document ........................................................................... 5 \n1.4 \nScope of Project ......................................................................................... 6 \n1.5 \nConstraints ................................................................................................. 7 \n1.6 \nFunctional Requirements ............................................................................ 7 \n1.7 \nNon-Functional Requirements .................................................................... 9 \n1.8 \nInterface Requirements ............................................................................. 11 \n1.8.1 Hardware ................................................................................................. 11 \n1.8.2 Software ................................................................................................... 11 \n1.9 \nProject Deliverables .................................................................................. 12 \n \n \n \n \n \n\u00a9 Aptech Limited \n1.1  Background and Necessity for the Application \n \nWith the rise of remote work, online learning, and virtual collaboration, meetings \nhave become increasingly common. Platforms such as Microsoft Teams, Google \nMeet, and Zoom now play an integral role in professional and academic \nenvironments. Despite this shift, the process of documenting meeting \ndiscussions, action points, and decisions remains largely manual and error-\nprone. Participants often struggle to take accurate notes while actively engaging \nin the conversation, leading to missed information, inconsistencies, and a lack \nof reliable documentation. As a result, organizations and teams are increasingly \nlooking for automated solutions that can transcribe and summarize meeting \ncontent efficiently and accurately. \n \nTo address this requirement, a solution must be created that can convert speech \ninto meeting notes. With the emergence of AI tools and technologies, utilizing \nthem to build this solution is appropriate.   \n \n \n \n \n \n \n \n \n \n \n\u00a9 Aptech Limited \n1.2  Proposed Solution \n \n\u2018TalkToText Pro\u2019 emerges as a timely solution to this challenge by offering an \nAI-powered system that converts speech into well-structured and actionable \nmeeting notes. By combining speech-to-text technology, language translation, \nand advanced summarization using models such as OpenAI\u2019s ChatGPT, the \napplication ensures that key meeting insights are captured, organized, and \neasily accessible. This automation not only saves time and effort, but also \nimproves communication clarity, enhances follow-up efficiency, and supports \nmultilingual collaboration. As teams grow more distributed and meetings \nbecome more frequent, the necessity for such intelligent documentation tools \nbecomes essential for maintaining productivity, accountability, and \ninstitutional memory. \nTalkToText Pro offers an end-to-end AI-powered system designed to automate \nthe process of converting meeting audio into clean, structured, and actionable \nnotes. The application begins by accepting audio input from recorded or live \nonline meetings conducted via platforms such as Microsoft Teams, Google Meet, \nor Zoom. It uses advanced AI-based speech-to-text transcription to accurately \nconvert spoken content into written form. If the meeting is conducted in a \nlanguage other than English, the system automatically translates the transcript \ninto English using a reliable language translation engine. \nOnce the transcript is available, the text is cleaned and token-optimized to \nprepare it for summarization. The core of the solution lies in using OpenAI\u2019s \nChatGPT API, which processes the optimized transcript and generates \ncomprehensive meeting minutes. These include an executive summary, key \ndiscussion points, action items, decisions made, and even sentiment analysis, if \nrequired. The final output is formatted for clarity and can be downloaded, shared \nvia email, or integrated into productivity tools. All versions of the transcript and \nnotes are stored in a database for future retrieval and reference. This ensures \nthat no important information is lost, and all meetings are documented with \nconsistency and precision.  \n \n \n \n \n \n \n\u00a9 Aptech Limited \nThe architecture for building the application will be as follows: \n  \n \nSample Architecture of the Application \n \nThe steps to create the application are as follows: \n1. Online Meeting Input - The system receives a meeting recording or live \naudio stream from tools such as Google Meet or Microsoft Teams. \n2. AI Speech-to-Text Transcription - The audio is converted to text using \nAI-based speech recognition APIs (example, Whisper or Google Speech-to-\nText). OpenAI or other speech models can be plugged in here for accurate \ntranscription. \n3. AI Translation to English - If the transcript is in another language, use \nAI translation tools to convert it to English. AI services such as Google \n \n \n\u00a9 Aptech Limited \nTranslate API or OpenAI language translation models can be used for \ntranslation. \n4. Optimize Text and Tokens - The raw/translated text is cleaned (such as \nremoving noise, filtering pauses, repetitions, filler words). Token \noptimization ensures the content fits within OpenAI API token limits, \nwhich is important for longer transcripts. \n5. Generate Meeting Minutes (OpenAI API Call) - The optimized transcript \nis passed to ChatGPT via OpenAI API, prompting it to extract: \n\u2022 Summary \n\u2022 Key discussion points \n\u2022 Decisions \n\u2022 Action items \n\u2022 Sentiment (positive/negative tone) \n6. Store in Database - All versions which include raw, translated, optimized, \nfinal notes are stored in a relational or NoSQL database (example, \nMongoDB). \n7. Output Formatted Meeting Notes - Final notes can be exported as PDF \nor Word reports. \n1.3  Purpose of the Document \n \nThe purpose of this document is to present a detailed description of the AI-\npowered meeting transcription and summarization system titled \u2018TalkToText \nPro.\u2019 \nThis document explains the purpose and features of the application, which uses \nGenerative AI technologies such as speech-to-text, language translation, and \nLarge Language Models (LLMs). These technologies work together to transform \nmeeting audio into structured, actionable notes. It also outlines the constraints \nunder which the system should operate. This document is intended for both \nstakeholders and developers of the application. \n \n \n\u00a9 Aptech Limited \n \n1.4  Scope of Project \n \nThe \u2018TalkToText Pro\u2019 application should be able to assist users in processing \nmeeting audio recordings from various sources such as Microsoft Teams, Google \nMeet, Zoom, and other conferencing platforms. It will support multiple audio \nformats and languages to ensure broad usability across professional and \nacademic settings. \n \nKey features will include AI-powered speech-to-text transcription, automatic \nlanguage translation, text optimization, and the use of Generative AI models for \ngenerating structured meeting notes. These notes will include summaries, key \ndiscussion points, action items, and sentiment analysis. A user-friendly interface \nand secure database storage will be employed to enable users to upload, view, \ndownload, and retrieve meeting notes, ensuring an efficient, consistent, and \nintelligent meeting documentation experience. \n \n \n \n\u00a9 Aptech Limited \n \n \n1.5  Constraints \n \nConstraints for the \u2018TalkToText Pro\u2019 project could include the requirement for \naccurate transcription of audio files with varying quality, background noise, \nmultiple speakers, and accents. These factors may impact the effectiveness of \nspeech-to-text conversion. Ensuring consistent language translation and \nsummarization across diverse meeting contexts is also a potential challenge.  \nAdditionally, security and privacy concerns when handling sensitive meeting \ndata will be critical. This will require strict adherence to data protection \nregulations and the implementation of secure data storage and processing \npractices throughout the application\u2019s lifecycle. \n \n1.6  Functional Requirements \n \nThe project involves developing an AI-Powered Meeting Notes Rewriter \napplication utilizing Generative AI, LLMs, and technologies from OpenAI. The \napplication will transform raw meeting transcripts into clean, structured notes, \nhighlighting key decisions, action items, and takeaways.  \n \n \n \n \n \n \n\u00a9 Aptech Limited \nSome of the functional requirements are explained as follows: \n \ni. \nUser Interface (UI) for Audio Input - The application should provide a \nUI that allows users to upload meeting audio files in supported formats \nsuch as .mp3, .wav, or .mp4. It should also enable users to input links \nfrom online meeting platforms such as Microsoft Teams or Google Meet. \n \nii. \nLanguage Selection - The application should provide an option for users \nto select the original language of the uploaded audio. \n \niii. \nSpeech-to-text Transcription - The application should transcribe the \nuploaded audio into text using an AI-based speech recognition service. \niv. \nLanguage Translation - The application should automatically translate \nthe transcript to English if the original language is not English. \nv. \nText and Token Optimization - The application should clean and \noptimize the transcribed text by removing filler words, repeated content, \nand adjusting token count to comply with the token limits of the OpenAI \nAPI. \nvi. \nMeeting Notes Generation using OpenAI - The application should use \nthe OpenAI API to generate structured meeting notes that include a \nsummary, key points, decisions, action items, and sentiment analysis. \nvii. \nDisplay of Processed Output - The application should display the \ngenerated meeting notes in a readable, structured format on the UI. \nviii. \nDownload and Export - The application should provide options to \ndownload the meeting notes in PDF or Word format and share them via \nemail or other integrations. \nix. \nData Storage - The application should store the raw transcript, \ntranslated version, optimized text, and final meeting notes in a database. \nx. \nView History - The system shall allow users to view and retrieve \npreviously generated meeting notes from the database. \nxi. \nProgress Tracking UI - The system shall show real-time processing \nstatus updates for each step (transcription, translation, optimization, \ngeneration) through a progress tracker on the UI. \n \n \n \n \n \n \n\u00a9 Aptech Limited \nIn addition, the application should include a login system to enable users to \nmanage their own meeting records and access history securely.  \nFollowing is a sample of an expected output: \n \n1.7  Non-Functional Requirements \n \nThere are several non-functional requirements that should be fulfilled by the \napplication. These are listed as follows: \n  \n1. Performance: The application must process a 30-minute meeting \nrecording and generate notes within one to two minutes. \n \n2. Secure: Meeting data and summaries must be securely stored with access \nrestrictions. \n \n3. Accuracy: Transcription and summarization must achieve at least 85\u2013\n90% accuracy under good audio conditions. \n \n \n \n \n\u00a9 Aptech Limited \n4. Scalable: The application should be scalable to handle multiple meeting \nuploads simultaneously. \n \n5. Usable: The UI must be simple, intuitive, and mobile-responsive for both \ntechnical and non-technical users. \n \n \n \n \n \n \n \n \n \n \nThese are the bare minimum expectations from the project. It is a must \nto implement the FUNCTIONAL and NON-FUNCTIONAL \nrequirements given in this SRS. \n \nOnce they are complete, you can use your own creativity and \nimagination to add more features if required. \n \n \n\u00a9 Aptech Limited \n1.8  Interface Requirements \n \n1.8.1 Hardware  \n \nIntel Core i5/i7 Processor or higher \n8 GB RAM or higher \nColor SVGA \n500 GB Hard Disk space Mouse \nKeyboard \n \n1.8.2 Software  \n \nTechnologies to be used: \n1. Frontend: HTML5 or any other frontend programming languages \n2. Backend: Flask/Django \n3. Data Store: PDF, MongoDB \n4. Programming/IDE: Python, Jupyter Notebook, Anaconda, Google Colab \n5. Libraries: Tensorflow, NLTK, Keras, OpenAI API, Python libraries, and \ntransformers \n \n \n \n \n\u00a9 Aptech Limited \n1.9  Project Deliverables \n \nYou will design and build the project and submit it along with a complete project \nreport that includes: \n\u2022 Problem Definition \n\u2022 Design specifications \n\u2022 Diagrams such as User Flow Diagram/User Journey Map \n\u2022 Detailed steps to execute the project \n\u2022 Test Data Used in the Project \n\u2022 Project Installation Instructions \n\u2022 Link of GitHub for accessing the uploaded project code (Link should have \npublic access) \n\u2022 Link of published blog \n \nThe source code, including .ipynb files for Jupyter Notebook and Google Colab, \nshould be shared via GitHub. Appropriate access permissions should be granted \nto users to allow testing for Jupyter Notebook and Google Colab. The \nconsolidated project must be submitted on GitHub with a ReadMe.doc file listing \nassumptions (if any) made at your end.  \nKindly provide the GitHub URL where the project has been uploaded for sharing. \nThe repository on GitHub should have public access. Documentation is a very \nimportant part of the project; hence, all crucial aspects of the project must be \ndocumented \nproperly. \nEnsure \nthat \ndocumentation \nis \ncomplete \nand \ncomprehensive. \nDo NOT copy content or code from GPTs or other AI tools, \nalthough you are permitted to use images generated by AI \ntools for any visual representation purposes. It is mandatory \nto mention such tools used in case you add any AI generated \nimages. \nSubmit a video (.mp4 file) demonstrating the working of the \napplication, including all the functionalities of the project. \nThis is MANDATORY. \n \n~~~ End of Document ~~~ \n", "years_experience": 0, "score": 0.10652613195017446, "path": "C:\\Users\\admin\\AI-Resume-Ranker\\data\\uploads\\TalkToText_Pro-Generative_AI_Odyssey-SRS_final.pdf"}, {"filename": "GeoSpeak-GenAI_Smart_Solutions_SRS.pdf", "text": " \n \n \n \n \n \n \nSoftware Requirements Specification \nVersion 1.0 \n \n \nGeoSpeak \n \nTheme: Language Master \nProject Name: GeoSpeak \nCategory:  GenAI Smart Solutions \n \n \n \n \n \n \n \n \n\u00a9 Aptech Limited \nContents \n \n1.1 Background and Necessity for the Application .................................... 2 \n1.2 Proposed Solution ..................................................................................... 3 \n1.3 Purpose of the Document ....................................................................... 5 \n1.4 Scope of Project ......................................................................................... 5 \n1.5 Constraints .................................................................................................. 6 \n1.6 Functional Requirements ......................................................................... 7 \n1.7 Non-Functional Requirements ................................................................ 8 \n1.8 Interface Requirements ............................................................................ 9 \n1.8.1 Hardware ..................................................................................................... 9 \n1.8.2 Software ...................................................................................................... 9 \n1.9 Project Deliverables ................................................................................ 10 \n \n \n \n \n \n\u00a9 Aptech Limited \n1.1  Background and Necessity for the Application \n \nIn a world that is becoming more connected, effective communication across \ndifferent languages is crucial for increasing collaboration, cultural exchange, and \nbusiness operations. The demand for accurate and efficient language translation \nservices has emerged, driven by international trade, travel, online content \ncreation, and social media interactions. Traditional translation methods, \nincluding manual translation and simple rule-based systems, often fall short in \nterms of speed, accuracy, and contextual relevance. \n \nThe necessity of this application \u2018GeoSpeak\u2019 arises from the limitations of \ntraditional translation methods and the growing complexity of language usage in \nvarious domains. Using OpenAI's Large Language Models (LLMs) and Generative \nAI (Gen AI), the application can dynamically interpret and translate text \nconsidering context, idioms, and subtle nuances. This is particularly important \nfor applications in diplomacy, global business, education, healthcare, and \ncustomer service where precise communication can significantly impact \noutcomes. This application aims to bridge language barriers, enhance cross-\ncultural understanding, and improve access to information for users worldwide. \n \n \n \n \n \n \n \n \n\u00a9 Aptech Limited \n1.2  Proposed Solution \n \nThe proposed solution, GeoSpeak, is a comprehensive real-time Web application \ntranslation tool designed to bridge language barriers. At its core, the application \nwill leverage advanced AI technologies, specifically a cloud-based LLM, to provide \naccurate and contextually appropriate translations. This GeoSpeak solution \nmeets current requirements and evolves for future growth, becoming essential \nin multinational and multilingual work environments. OpenAI's models, known \nfor their exceptional language understanding and generation capabilities, enable \nreal-time and contextually aware translations that are far more accurate than \ntraditional methods. By addressing both, current translation challenges and \nexpecting future necessities, this solution has the potential to significantly \nimprove cross-language communication in diverse workplace settings. \n \n \nWorkflow of the GeoSpeak application is as follows: \n \n                      \n \n \n \n\u00a9 Aptech Limited \nSample Architecture of the Application is as follows: \n \n \n \n \nSample Architecture of the Application \n \nInitially at the User Input stage, the user provides text in the English language \nand selects the desired target language from a dropdown menu. This input text \nis then processed by the Embedding API, which converts the text into a high-\ndimensional \nembedding \nvector \nthat \ncaptures \nits \nsemantic \nmeaning. \nSubsequently, this embedding vector is used to query a Vector Database that \ncontains precomputed embeddings of parallel corpora or translation examples. \nThe system retrieves relevant documents that closely match the source text \nembedding. These documents consist of paired texts in both the source and \ntarget languages, which help to inform the translation process. \nThe application then creates a Context-aware Prompt, which integrates the \nsource text and the specified target language, ensuring the translation model \nunderstands the context and language requirements. For instance, the prompt \nmight be structured as \"Translate following text to French: 'Hello, how are you?'\". \nThis context-aware prompt is fed into a Language Model, a sophisticated AI \nmodel trained on vast amounts of multilingual text data. The language model \nprocesses the prompt and generates the corresponding translation in the target \nlanguage. \n \n \n \n\u00a9 Aptech Limited \nFinally, the translated text is presented to the user as output. This output can \nbe displayed in a user-friendly format allowing the user to see the translated text \ndirectly. Throughout this process, the application leverages OpenAI LLMs to \nensure accurate and contextually appropriate translations, making it a powerful \ntool for users requiring real-time language translation services. \n \n1.3  Purpose of the Document \n \nThe purpose of this document is to present a detailed description of the (Gen AI) \nsimulating text conversion by matching user prompts to scripted responses titled \nGeoSpeak. \nThis document explains the purpose and features of (Gen AI) and LLM and the \nconstraints under which it must operate. This document is intended for both \nstakeholders and developers of the system. \n1.4  Scope of Project \n \nThe scope of this project involves developing a language translation application \nthat allows users to input text, select a target language, and receive a translation. \nIt includes processing text through an Embedding API to generate a semantic \nvector, querying a Vector Database for relevant translation examples, and \ncreating a context-aware prompt for a Language Model. This model then \ngenerates the translation, which is presented to the user in a clear and user-\nfriendly format. The application leverages OpenAI\u2019s LLM to ensure accurate and \ncontextually appropriate translations. \n \n \n\u00a9 Aptech Limited \n \n \n \n1.5  Constraints \n \nGeoSpeak faces several constraints including the requirement for high-quality \nprecomputed embeddings in the Vector Database to ensure accurate retrieval of \nparallel corpa/translation examples. There is also a reliance on the performance \nof the Language Model to provide contextually accurate translations which \nrequires efficient integration and processing capabilities.  \n \nAdditionally, the application must handle diverse languages and text inputs \nrobustly and maintain user data security and privacy throughout the translation \nprocess. Performance and scalability are crucial, especially under high user \nloads. This application is not equipped to audio and speech translation.  \n \n \n \n \n\u00a9 Aptech Limited \n1.6  Functional Requirements \n \nThe project involves developing an intelligent translation application utilizing  \n(Gen AI), Large LLMs, and technologies from OpenAI or other relevant model. The \napplication will be designed to process user input provided in the form of text \ninput. Some of the functional requirements are as follows: \n \ni. \nText Input - The application must allow users to input text in a source \nlanguage and select a target language from a drop-down menu. \n \nii. \nText Processing - It must process the input text using an Embedding API \nto generate a high-dimensional semantic vector. \n \niii. \nDocument Retrieval - The application must query a Vector Database to \nretrieve relevant documents based on the generated embedding vector. \n \niv. \nContext-aware Prompt Creation - It must generate a context-aware  \nTranslation model. \n \nv. \nTranslation Generation - The application must use a Language Model to \ngenerate a translation of the source text based on the context-aware \nprompt. \n \nvi. \nResult Display - It must present the translated text to the user in a clear \nand user-friendly format. \n \nvii. \nError Handling - The system should handle errors in text processing, \ndocument retrieval, and translation generation gracefully. \n \nviii. \nUser Interface - It must provide an intuitive and responsive user \ninterface for input, language selection, and viewing translation results. \n \n \n \n \n \n\u00a9 Aptech Limited \n \n1.7  Non-Functional Requirements \n \nThere are several non-functional requirements that should be fulfilled by the \nsystem. \n  \nThe application should be: \n \n1. Secure: The application must ensure the secure handling of user data and \ninteractions with APIs and databases. \n2. Efficient: The application should process and return translations quickly, \nwith minimal latency, to ensure a smooth user experience. \n3. Scalable: The Application must handle varying loads efficiently, from \nindividual users to high traffic scenarios, without degradation in \nperformance. \n4. Reliable: The application should operate consistently and correctly with \nminimal downtime and robust error recovery mechanisms. \n5. Usable: The user interface should be intuitive and accessible, providing a \nseamless experience for users of varying technical skills. \nThese are the bare minimum expectations from the project. It is a must \nto implement the FUNCTIONAL and NON-FUNCTIONAL \nrequirements given in this SRS. \n \nOnce they are complete, you can use your own creativity and \nimagination to add more features if required. \n \n \n \n\u00a9 Aptech Limited \n1.8  Interface Requirements \n \n1.8.1 \nHardware  \n \nIntel Core i5/i7 Processor or higher \n8 GB RAM or higher \nColor SVGA \n500 GB Hard Disk space  \nMouse \nKeyboard \n \n1.8.2 \nSoftware  \n \nTechnologies to be used: \n1. Frontend: HTML5 or any other scripting languages \n2. Backend: Flask/Django \n3. Data Store: TXT \n4. Programming/IDE: Python, Jupyter Notebook, Anaconda, or Google \nColab \n5. Libraries: Tensorflow, Keras, OpenAI API, Python libraries, and  \npre-trained transformers \n \n \n \n \n \n \n \n \n \n\u00a9 Aptech Limited \n1.9  Project Deliverables \n \nYou will design and build the project and submit it along with a complete project \nreport that includes: \n \n\u25cf Problem Definition \n\u25cf Design Specifications \n\u25cf Diagrams such as User Flow Diagram/User Journey Map \n\u25cf Detailed steps to execute the project \n\u25cf Test Data Used in the Project \n\u25cf Project Installation Instructions  \n\u25cf Link of GitHub for accessing the uploaded project code (Link should have \npublic access) \n\u25cf Link of Published Blog \nThe source code, including .ipynb files for Jupyter Notebook and Google Colab, \nshould be shared via GitHub. Appropriate access permissions should be granted \nto users to allow testing for Jupyter Notebook and Google Colab. The \nconsolidated project must be submitted on GitHub with a ReadMe.doc file listing \nassumptions (if any) made at your end.  \nProvide the GitHub URL where the project has been uploaded for sharing. The \nrepository on GitHub should have public access. Documentation is a very \nimportant part of the project; hence, all crucial aspects of the project must be \ndocumented \nproperly. \nEnsure \nthat \ndocumentation \nis \ncomplete \nand \ncomprehensive. \nYou should publish a blog of minimum 2000 words on any free blogging Website \nsuch as Blogger, Tumblr, Ghost, or any other blogging Website. The link of the \npublished blog should be submitted along with the project documentation. \n \nSubmit a video (.mp4 file) demonstrating the working \nof the application, including all the functionalities of \nthe project. This is MANDATORY. \n \n \n~~~ End of Document ~~~ \n", "years_experience": 0, "score": 0.10286886007464752, "path": "C:\\Users\\admin\\AI-Resume-Ranker\\data\\uploads\\GeoSpeak-GenAI_Smart_Solutions_SRS.pdf"}, {"filename": "SYED_MOHSIN_KAZMI_---_CV.docx", "text": "CV \nSYED MOHSIN KAZMI \nHouse No 1296, Street No 9, Mehmoodabad No 6, LAC-II, Karachi, Pakistan. | 0344-4896978| \nOBJECTIVE \nMy object is to join a reputable organization where my services and capabilities shall be fully utilized forthe benefit of both the organization and myself for a long period of time. \nPERSONAL INFORMATION \n\u2219 Father Name : Syed Zulfiqar-ul-Hassan \n\u2219 Date of Birth : 31-08-2006 \n\u2219 Nationality : Pakistani \n\u2219 Religion : Islam \n\u2219 Gender : Male \n\u2219 Marital Status : Single \nEXPERIENCE \n\ud83d\uddb3 FRESH \nEDUCATION \n\ud83d\udd6e BOARD OF INTERMEDIATE EDUCATION KARACHI \nIntermediate \u2013\u2013 (Pre-Eng) \n\ud83d\udd6e BOARD OF SECONDARY EDUCATION KARACHI \nMatriculation \u2013\u2013 (Science) \nSKILLS \n\u25aa Teamwork \n\u25aa Problem solving abilities \n\u25aa SEO \n\u25aa SMM \n\u25aa SEM \n\u25aa Email Marketing \nLANGUAGE \n\u25aa Urdu \n\u25aa English \nREFERENCE  \n\u2022 Aptech Shahrah e Faisal ", "years_experience": 0, "score": 0.04334230480106764, "path": "C:\\Users\\admin\\AI-Resume-Ranker\\data\\uploads\\SYED_MOHSIN_KAZMI_---_CV.docx"}, {"filename": "CV.pdf", "text": "Software Engineering\n(Diploma)\nAptech\nMeritorious High School\nO\u2019Levels\nOngoing\n2019 - 2022\nReact JS\nFLUTTER\nPHP\nTailwind CSS\nEducation\nContact\nExpertise\nEnglish\nLanguage\nProfile\nProjects\nABDULREHMAN\nWebsite Developer\n2025\nPulsetech\nFull Stack Developer React JS\nWorking with the wider development team.\nManage website design, content, and backend\n2024\nEdu Man\nWeb Designer and developer\nWorking on UI/UX figma and webflow.\nManage website design, content, backend of\nwebsites.\n2023\nS4S\nWordPress Developer\nWorking with the wider development team.\nManage website design, content\nI'm Abdul Rehman, a backend-focused web developer with 3\nyears of freelancing and office experience. Passionate about\nbuilding efficient, reliable solutions and currently looking for new\nopportunities to grow and contribute.\nCompany Site\nwww.zainasohail.org\nEducation System Website\nwww.edu-man.com\nWork Experience\n+92 327 8445498\nlogicwork560@gmail.com\nShah Faisal Town, Karachi,\nPakistan\nwww.reallygreatsite.com\nAptech\nInter\nOngoing\nWORDPRESS\nEbook Website\nhttps://brinkwriter.com/\n", "years_experience": 3, "score": 0.04211605996347786, "path": "C:\\Users\\admin\\AI-Resume-Ranker\\data\\uploads\\CV.pdf"}, {"filename": "Kids_7.docx", "text": "Kids Water Bottles\nStainless Steel:\nSingle-Wall: Lightweight and typically not insulated. Best for basic hydration needs.\nDouble-Wall Insulated: Features a vacuum insulation layer between the walls that helps maintain the temperature of beverages, keeping drinks cold for up to 24 hours and hot for 12 hours.\nElectro-polished Stainless Steel:\nThis finish helps to prevent corrosion, resists stains, and is non-reactive with the liquid inside.\nBenefits of Steel Water Bottles:\nDurability: Stainless steel is tough and resistant to rust, corrosion, and damage from drops or impacts.\nEco-Friendly: Using a steel bottle helps reduce reliance on disposable plastic bottles, which are harmful to the environment.\nNon-Toxic: Stainless steel is a safer option compared to plastic, as it doesn't contain harmful chemicals like BPA (Bisphenol A), which can leach into liquids from plastic.\nTemperature Control: Insulated bottles keep your drinks at your preferred temperature for longer. This is great for hot coffee, tea, or cold water.\nTaste Preservation: Unlike plastic, steel doesn't absorb odors or flavors, keeping the taste of your drinks pure. \nMaintenance:\nCleaning: Most steel water bottles are easy to clean and dishwasher-safe, but it\u2019s often recommended to clean them by hand to preserve the insulation properties.\nAvoiding Scratches: While steel is durable, it\u2019s best to avoid abrasive scrubbing on the inside to maintain the bottle\u2019s integrity and to keep the inside smooth\n\n\n", "years_experience": 0, "score": 0.01398552852741328, "path": "C:\\Users\\admin\\AI-Resume-Ranker\\data\\uploads\\Kids_7.docx"}, {"filename": "Untitled_design_1.pdf", "text": "", "years_experience": 0, "score": 0.0, "path": "C:\\Users\\admin\\AI-Resume-Ranker\\data\\uploads\\Untitled_design_1.pdf"}, {"filename": "Untitled_design.pdf", "text": "", "years_experience": 0, "score": 0.0, "path": "C:\\Users\\admin\\AI-Resume-Ranker\\data\\uploads\\Untitled_design.pdf"}]